Project Structure (files included):
├── .cursorrules
├── .gitattributes
├── .gitignore
├── LICENSE
├── README.md
├── Resources
│   ├── ArcHydro_Includes.md
│   ├── Customize tool behavior in a Python toolbox—ArcGIS Pro - Documentation.url
│   ├── ESRI Developer Links.txt
│   ├── Esri Developer.url
│   ├── RAS Commander Arc Hydro Tools - ESRI 2025 User Conference Launch Slides.txt
│   ├── SELF_CONTAINED_TOOLBOX.md
│   ├── install_toolbox.ps1
│   └── uninstall_toolbox.ps1
├── Scripts
│   └── archydro
│       ├── rc_load_hecras_1d_geometry.py
│       ├── rc_load_hecras_2d_geometry.py
│       ├── rc_load_hecras_2d_results.py
│       ├── rc_load_ras_terrain.py
│       ├── rc_organize_ras_project.py
│       └── rc_utils.py
├── TRADEMARKS.md
└── toolboxes
    └── RAS-Commander.pyt

File: c:\GH\ras-commander-hydro\.cursorrules
==================================================
# RAS Commander Arc Hydro Tools - Development Rules

## Project Overview
This project implements Arc Hydro Tools for HEC-RAS data extraction and organization within ArcGIS Pro.

## Code Style and Conventions

### Python Style
- Follow PEP 8 conventions
- Use descriptive variable names that clearly indicate purpose
- Prefer f-strings for string formatting
- Use type hints where appropriate
- Keep line length under 120 characters for better readability in ArcGIS Pro

### ArcPy Specific Conventions
- Always use the `messages` parameter passed to methods, never `arcpy.AddMessage` directly
- Format multi-line messages as single strings before passing to messages object:
  ```python
  # Good
  msg = f"Processing {count} features in {format_time(elapsed)}"
  messages.addMessage(msg)
  
  # Bad - can cause AttributeError
  messages.addMessage(
      f"Processing {count} features "
      f"in {format_time(elapsed)}"
  )
  ```
- Use `arcpy.ExecuteError()` for tool execution failures
- Check for feature existence before operations

### Performance Profiling Pattern
When adding performance profiling to tools:

1. Import required utilities:
   ```python
   from rc_utils import (
       PerformanceTimer,
       timed_operation,
       format_time,
       get_feature_count_estimate
   )
   ```

2. Wrap execute method with timer:
   ```python
   def execute(self, parameters, messages):
       """Execute the tool with performance profiling."""
       tool_timer = PerformanceTimer().start()
       
       messages.addMessage(f"\n{'='*60}")
       messages.addMessage(f"Starting {self.label}")
       messages.addMessage(f"{'='*60}\n")
       
       try:
           # Tool logic here
           
           # Performance summary
           total_time = tool_timer.total()
           messages.addMessage(f"\n{'='*60}")
           messages.addMessage(f"PERFORMANCE SUMMARY - {self.label}")
           messages.addMessage(f"{'='*60}")
           messages.addMessage(f"  Total execution time: {format_time(total_time)}")
           messages.addMessage(f"{'='*60}\n")
           
       except Exception as e:
           elapsed = tool_timer.total()
           messages.addErrorMessage(f"\nTool failed after {format_time(elapsed)}: {str(e)}")
           raise
   ```

3. Use timed_operation for major steps:
   ```python
   with timed_operation(messages, "Loading HDF metadata"):
       # Operation code
   ```

4. Add progress reporting for long operations:
   ```python
   if batch_elapsed > 1.0:  # Report if batch took > 1 second
       rate = batch_created / batch_elapsed
       messages.addMessage(
           f"Processed {batch_end}/{total} ({progress*100:.1f}%) - "
           f"Rate: {rate:.0f}/sec - ETA: {format_time(eta)}"
       )
   ```

### HDF5 File Handling
- Always use context managers for HDF5 files
- Cache metadata when accessed multiple times
- Process data in batches for large datasets
- Use numpy operations for performance where possible

### Error Handling
- Provide specific, actionable error messages
- Continue processing other elements if one fails
- Always close HDF5 files properly
- Log performance metrics even on failure

### Feature Class Creation
- Check if output already exists before creation
- Use batch inserts for better performance (batch_size = 10000)
- Report progress for operations over 50,000 features
- Handle numpy data types properly (convert to Python types)

### PowerShell Installer Scripts
- Cast path variables to strings explicitly:
  ```powershell
  $scriptDir = [string](Split-Path -Parent $MyInvocation.MyCommand.Definition)
  ```
- Use explicit Join-Path parameters:
  ```powershell
  Join-Path -Path $repoRoot -ChildPath "Scripts\archydro"
  ```
- Validate paths are not arrays before use
- Build file lists separately from path construction

## Tool Organization

### File Structure
```
Scripts/archydro/
├── rc_utils.py                    # Shared utilities and performance helpers
├── rc_load_hecras_1d_geometry.py  # 1D geometry extraction
├── rc_load_hecras_2d_geometry.py  # 2D geometry extraction
├── rc_load_hecras_2d_results.py   # Results data extraction
├── rc_load_ras_terrain.py         # Terrain data extraction
└── rc_organize_ras_project.py     # Master organization tool
```

### Method Patterns
- `_get_*_direct()` methods: Direct HDF5 data extraction
- `execute()`: Main tool execution with parameters
- `getParameterInfo()`: Define tool parameters
- `updateParameters()`: Dynamic parameter updates
- `updateMessages()`: Parameter validation messages

## Performance Guidelines

### Expected Processing Rates
- 1D Geometry: ~1,000 features/second
- 2D Points: ~10,000 features/second
- 2D Polygons: ~1,000 cells/second
- Results: ~5,000 points/second

### Optimization Strategies
1. Pre-compute shared data (e.g., mesh faces for polygons)
2. Use vectorized numpy operations
3. Batch database operations
4. Report progress to keep users informed
5. Warn users about large dataset processing times

### Memory Management
- Process large datasets in chunks
- Clear large arrays when no longer needed
- Use generators where possible
- Monitor memory usage for operations over 1GB

## Common Pitfalls to Avoid

1. **Don't use `arcpy.AddMessage` in methods** - always use the passed `messages` parameter
2. **Don't pass multi-line f-strings to messages** - format first, then pass
3. **Don't assume paths are strings** - PowerShell can create arrays unexpectedly
4. **Don't process entire large datasets at once** - use batching
5. **Don't ignore numpy type conversions** - can cause database write failures
6. **Don't create files without user request** - no proactive documentation generation

## Testing Checklist

Before committing changes:
- [ ] All tools run without errors on sample data
- [ ] Performance metrics display correctly
- [ ] Progress reporting works for large datasets
- [ ] Error messages are clear and actionable
- [ ] PowerShell installer runs without array conversion errors
- [ ] Memory usage is reasonable for large datasets
- [ ] All numpy types convert properly to Python types

## Documentation Standards

- Include docstrings for all classes and methods
- Document expected performance characteristics
- Provide usage examples in tool help
- Note any limitations or known issues
- Keep README updated with new features
==================================================

File: c:\GH\ras-commander-hydro\.gitattributes
==================================================
# Auto detect text files and perform LF normalization
* text=auto

==================================================

File: c:\GH\ras-commander-hydro\.gitignore
==================================================


# Python cache
__pycache__/
**/__pycache__/
*.pyc
*.pyo
*.pyd
.Python
==================================================

File: c:\GH\ras-commander-hydro\LICENSE
==================================================
The MIT License (MIT)
Copyright © CLB Engineering Corporation

Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.


NOTICE
* **HEC‑RAS**™ is a trademark of the U.S. Army Corps of Engineers (USACE) Hydrologic Engineering Center (HEC).
* **ARC HYDRO** is a trademark of Environmental Systems Research Institute (ESRI)

"RAS Commander" and "RAS Commander Arc Hydro Tools" are independent open source projects and are **not** affiliated with, endorsed by, or sponsored by USACE or HEC.
==================================================

File: c:\GH\ras-commander-hydro\README.md
==================================================
# RAS Commander Arc Hydro Tools

<div align="center">
  <img src="Images/ras-commander-archydro.svg" alt="RAS Commander Arc Hydro Tools" width="600">

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
[![ArcGIS Pro](https://img.shields.io/badge/ArcGIS%20Pro-2.8%2B-blue)](https://www.esri.com/en-us/arcgis/products/arcgis-pro/overview)
[![HEC-RAS](https://img.shields.io/badge/HEC--RAS-6.x-green)](https://www.hec.usace.army.mil/software/hec-ras/)

**Bringing HEC-RAS 6.x Direct Data Access to ArcGIS**

### 🎉 Launching at Esri User Conference 2025 🎉

[CLB Engineering](https://clbengineering.com/) | [RAS Commander Library](https://github.com/gpt-cmdr/ras-commander) | [Arc Hydro](https://www.esri.com/en-us/industries/water-resources/arc-hydro)

</div>

---

## Partnership Announcement

**ESRI has partnered with CLB Engineering Corporation's William "Bill" Katzenmeyer**, Vice President and creator of the RAS Commander Open Source Python Library, to bring powerful HEC-RAS 6.x HDF5 data extraction capabilities directly into ArcGIS Pro through the Arc Hydro Tools framework.

This collaboration represents a groundbreaking application of **LLM Forward engineering**—using Large Language Models to rapidly develop sophisticated geospatial tools that bridge hydraulic modeling and GIS workflows.

### 🚀 From Concept to Conference in One Month

Following an ASFPM brainstorming session in late May 2025, this toolbox was developed using CLB's innovative LLM Forward approach, achieving production‑ready functionality in just over a month—demonstrating the transformative potential of AI‑assisted development in the water‑resources sector.

---

## Overview

**RAS Commander Arc Hydro Tools** enables hydraulic engineers and GIS professionals to seamlessly extract and visualize HEC‑RAS 1D and 2D geometry, terrain, and results data without manual conversion steps. This free and open‑source toolbox brings the power of the [RAS Commander library](https://github.com/gpt-cmdr/ras-commander) directly into ArcGIS Pro.

### Key Features

* 📊 **Direct HDF5 Import** – Load HEC‑RAS data directly from geometry (`g*.hdf`) and plan (`p*.hdf`) files
* 🗺️ **1D Geometry Support** – Extract cross sections, river centerlines, bank lines, and structures
* 🌊 **2D Geometry Support** – Import mesh elements, breaklines, boundary conditions, and cell polygons
* 🏗️ **Pipe Networks** – Full support for storm/sewer pipe networks including SWMM imports
* 📈 **Results Visualization** – Display maximum WSE and velocity results with time of occurrence
* ⛰️ **Terrain Loading** – Import HEC‑RAS terrain layers from RAS Mapper VRT files
* 🗂️ **Project Organization** – Batch process entire HEC‑RAS projects into organized geodatabases

<div align="center">
  <a href="https://clbengineering.com/">
    <img src="Images/CLBEngineeringMainLogo.png" alt="CLB Engineering" width="300">
  </a>

*Developed by [CLB Engineering](https://clbengineering.com/) in partnership with ESRI*

</div>

---

## 📸 Interface Walk‑Through

Below are high‑resolution screenshots of each Arc Hydro tool in action to help you get up to speed quickly.

### Load HEC‑RAS 1D Geometry Layers

<p align="center">
  <img src="Images/docs/Load1DGeometry.png" alt="Load 1D Geometry" width="35%">
</p>

---

### Load HEC‑RAS 2D Geometry Layers

<p align="center">
  <img src="Images/docs/Load2DGeometry.png" alt="Load 2D Geometry" width="35%">
</p>

---

### Load HEC‑RAS 2D Results Summary Layers

<p align="center">
  <img src="Images/docs/Load2DSummaryResults.png" alt="Load 2D Results Summary" width="35%">
</p>

---

### Load HEC‑RAS Terrain

<p align="center">
  <img src="Images/docs/LoadRASTerrain.png" alt="Load RAS Terrain" width="35%">
</p>

---

### Organize HEC‑RAS Project

<p align="center">
  <img src="Images/docs/OrganizeRASProject.png" alt="Organize HEC‑RAS Project" width="35%">
</p>

---

### Full 2D Model Example — New Orleans HEC Example Project

<p align="center">
  <img src="Images/docs/rc_neworleanspipes.png" alt="New Orleans Imported Pipe Networks" width="50%">
</p>

<p align="center">
  <img src="Images/docs/rc_neworleanspipes_results.png" alt="New Orleans Maximum WSEL" width="50%">
</p>

These images showcase a 2D HEC‑RAS model of the New Orleans metro storm‑water system, complete with pipe networks, mesh polygons, and a maximum WSEL raster generated directly inside ArcGIS Pro.

---

## Community‑Driven Development

This is a **community‑driven effort**, and we're actively seeking your feedback!

### Are you a...

* 🏛️ **Municipality** looking to integrate HEC‑RAS data into dashboards?
* 👷 **Engineer** communicating multi‑hazard flood risk?
* 🗺️ **GIS Professional** preparing 2D model data?
* 🔬 **Researcher** analyzing model results?

**We want to hear from you!** [Share your ideas and use cases](https://github.com/gpt-cmdr/ras-commander-hydro/issues)

---

## The LLM Forward Revolution

This project showcases CLB Engineering's [LLM Forward approach](https://clbengineering.com/)—a revolutionary methodology that leverages Large Language Models to accelerate software development while maintaining professional‑grade quality. Learn more about this innovative approach and the author's work at [Engineering with LLMs](https://engineeringwithllms.info/).

### What Makes This Special?

1. **Rapid Development**: From concept to production in just over a month
2. **Cross‑Domain Expertise**: Seamlessly bridges hydraulic engineering and GIS
3. **Community Focused**: Built with user feedback at its core
4. **Open Source**: Transparent development process you can contribute to

---

## Installation

### Primary Method: Arc Hydro Tools Installation

The RAS Commander toolbox will be included as part of the Arc Hydro Tools distribution. This is the recommended installation method for most users.

1. Install Arc Hydro Tools following the standard installation process
2. The RAS Commander toolbox will be available under:

   ```
   Toolboxes → Arc Hydro Tools → RAS Commander
   ```

### Development Installation

For developers and users who want to extend or customize the tools:

1. **Clone the Repository**

   ```bash
   git clone https://github.com/gpt-cmdr/ras-commander-hydro.git
   cd ras-commander-hydro
   ```

2. **Option A: Add Toolbox in ArcGIS Pro**

   * Open ArcGIS Pro
   * In the Catalog pane, right‑click on **Toolboxes**
   * Select **Add Toolbox**
   * Navigate to `toolboxes/RAS-Commander.pyt`

3. **Option B: Install for Development (Requires Admin)**

   ```powershell
   # Run PowerShell as Administrator
   cd Resources
   .\install_toolbox.ps1
   ```

   To uninstall:

   ```powershell
   # Run PowerShell as Administrator
   cd Resources
   .\uninstall_toolbox.ps1
   ```

---

## Tools Overview

### 🔧 Load HEC‑RAS 1D Geometry Layers

Extract comprehensive 1D hydraulic model elements for report figures and analysis.

### 🌐 Load HEC‑RAS 2D Geometry Layers

Import complete 2D model components including mesh cells as polygons for advanced spatial analysis.

### 📊 Load HEC‑RAS 2D Results Summary Layers

Visualize maximum water‑surface elevation and velocity results with temporal data.

### ⛰️ Load HEC‑RAS Terrain

Import terrain layers from RAS Mapper with proper georeferencing.

### 🗂️ Organize HEC‑RAS Project

Comprehensive batch‑processing tool for entire HEC‑RAS projects.

---

## Current Capabilities & Roadmap

### Initial Release Features

* ✅ 1D and 2D Geometry Extraction (Including Pipe Networks!)
* ✅ Max WSE and Velocity as 2D Mesh Results
* ✅ Terrain Import for Inundation Mapping
* ✅ Support for HEC‑RAS 2D Models
* ✅ Organize Entire Projects as Geodatabases

### Coming Soon

* 📊 **Improved Schemas & Layer Styling**
* 📈 **1D Results & Full Time Series**
* 🌊 **Fluvial/Pluvial Delineation**
* 📍 **Land Use Layer Integration**
* 🔄 **Sync Changes Back to HEC‑RAS**
* 🎯 **Community‑Requested Features**

[View Full Roadmap & Vote on Features](https://github.com/gpt-cmdr/ras-commander-hydro/issues)

---

## Get Involved

### Webinar This Fall!

Join us for an in‑depth webinar showcasing advanced workflows and new features.

### Contributors Welcome!

Visit our [GitHub Repository](https://github.com/gpt-cmdr/ras-commander-hydro) to:

* Report issues
* Suggest features
* Submit pull requests
* Share your use cases

---

## Resources & Links

* **RAS Commander Arc Hydro Tools**: [https://github.com/gpt-cmdr/ras-commander-hydro](https://github.com/gpt-cmdr/ras-commander-hydro)
* **RAS Commander Library**: [https://github.com/gpt-cmdr/ras-commander](https://github.com/gpt-cmdr/ras-commander)
* **CLB Engineering Corporation**: [https://clbengineering.com/](https://clbengineering.com/)
* **Engineering with LLMs**: [https://engineeringwithllms.info/](https://engineeringwithllms.info/)

---

## License

This project is licensed under the MIT License – see the [LICENSE](LICENSE) file for details.

## Acknowledgments

* **[ESRI](https://www.esri.com/)** – Partnership and Arc Hydro Tools integration
* **[CLB Engineering](https://clbengineering.com/)** – Project sponsor and LLM Forward methodology
* **[USACE HEC](https://www.hec.usace.army.mil/)** – HEC‑RAS software and Example Projects
* **Water Resources Community** – For invaluable feedback and use cases

---

<div align="center">
  <img src="Images/ras-commander_logo.svg" alt="RAS Commander" width="150">

**Transform Your HEC‑RAS Workflow Today**

### 🎉 See us at Esri User Conference 2025! 🎉

[Get Started](https://github.com/gpt-cmdr/ras-commander-hydro) | [Documentation](Doc/RASCommander_Help.html) | [Report Issues](https://github.com/gpt-cmdr/ras-commander-hydro/issues)

</div>

==================================================

File: c:\GH\ras-commander-hydro\TRADEMARKS.md
==================================================
# TRADEMARKS.md

## Project Trademark

“RAS Commander”™ is an unregistered trademark of CLB Engineering Corporation. The mark is used solely to identify this open‑source software project and related materials.

## Third‑Party Trademarks

* **HEC‑RAS**™ is a trademark of the U.S. Army Corps of Engineers (USACE) Hydrologic Engineering Center (HEC).
* **ARC HYDRO** is a trademark of Environmental Systems Research Institute (ESRI)

* "RAS Commander" and "RAS Commander Arc Hydro Tools" are independent open source projects and are **not** affiliated with, endorsed by, or sponsored by USACE or HEC.

All other product names, logos, and brands mentioned in this repository are property of their respective owners and are used for identification purposes only.

## Naming & Compliance Policy

We respect the trademark rights and license terms of USACE and all other rights holders. If USACE, ESRI—or any other rightful owner—objects to our use of “RAS” or "Arc Hydro" in the project name, we will promptly rename the project and update all references within **30 days** of receiving written notice.

## Contact

To raise any trademark or licensing concerns, please open an issue in this repository or e‑mail **[info@engineeringwithllms.info](mailto:info@engineeringwithllms.info)**.
 
==================================================

File: c:\GH\ras-commander-hydro\Resources\ArcHydro_Includes.md
==================================================
To assist with packaging of this toolbox with Arc Hydro Tools, the scripts will be located in /Scripts/archydro

All .py scripts will be prefixed by "rc_" (for ras commander)

Due to the fact that __init__.py already exists in the Arc Hydro folder, we will need to manage our toolbox without using __init__.py, and be aware that in production a different __init__.py exists that we cannot control.  

The Arc Hydro Team prefers that the RAS Commander Arc Hydro Tools conform to the existing flat file structure under /Scripts/archydro.  No other files exist in this folder with an rc_ prefix.  use lower case file names 




Folder Mapping for ArcHydro Production: 

/Images/ras-commander-archydro-revised.png
This is used in the Help file and documentation

/Scripts/archydro 
All supporting .py files, organized to fit into the existing flat archydro file structure.  All .py files should start with rc_ prefix, use lower case file names and should not use __init__.py, as one already exists in this folder.  All imports should be relative, as the repository mirrors the final production folder structure.  

/Templates/Layers/archydro
Any layer lyrx files in this folder should be named lower case, with an "rc_" prefix.  I need a full guide on how to incorporate the use the .lyrx files into my toolbox's operations.  I understand I may need to calculate the max/min of a dataset to set the color ramp limits before inserting.  I need to figure out how to do this by looking up the ArcGIS Toolbox documentation and confirming how it is done elsewhere.  

/toolboxes
RAS Commander.pyt and .pyt.xml files for each tool box are located in this folder. 
==================================================

File: c:\GH\ras-commander-hydro\Resources\Customize tool behavior in a Python toolbox—ArcGIS Pro - Documentation.url
==================================================
[InternetShortcut]
URL=https://pro.arcgis.com/en/pro-app/latest/arcpy/geoprocessing_and_python/customizing-tool-behavior-in-a-python-toolbox.htm

==================================================

File: c:\GH\ras-commander-hydro\Resources\ESRI Developer Links.txt
==================================================
https://developers.arcgis.com/



https://pro.arcgis.com/en/pro-app/latest/arcpy/geoprocessing_and_python/customizing-tool-behavior-in-a-python-toolbox.htm



https://pro.arcgis.com/en/pro-app/latest/arcpy/geoprocessing_and_python/updating-schema-in-a-python-toolbox.htm


https://pro.arcgis.com/en/pro-app/latest/arcpy/classes/schema.htm


https://pro.arcgis.com/en/pro-app/latest/arcpy/geoprocessing_and_python/accessing-parameters-within-a-python-toolbox.htm


https://pro.arcgis.com/en/pro-app/latest/arcpy/geoprocessing_and_python/accessing-parameters-within-a-python-toolbox.htm


https://pro.arcgis.com/en/pro-app/latest/arcpy/geoprocessing_and_python/writing-messages-in-a-python-toolbox.htm
==================================================

File: c:\GH\ras-commander-hydro\Resources\Esri Developer.url
==================================================
[InternetShortcut]
URL=https://developers.arcgis.com/

==================================================

File: c:\GH\ras-commander-hydro\Resources\install_toolbox.ps1
==================================================
# install_toolbox.ps1
#
# Installs the Arc Hydro RAS Commander toolbox for local development.
# This script copies all toolbox files from the repository to the ArcGIS Pro installation directories.
#
# Usage:
#   Right-click on this file and select "Run with PowerShell" 
#   OR
#   Open PowerShell as Administrator and run: .\install_toolbox.ps1
#
# Note: This script requires administrator privileges to write to Program Files.

# Check if running as administrator
if (-NOT ([Security.Principal.WindowsPrincipal] [Security.Principal.WindowsIdentity]::GetCurrent()).IsInRole([Security.Principal.WindowsBuiltInRole] "Administrator")) {
    Write-Host "This script requires Administrator privileges." -ForegroundColor Red
    Write-Host "Please run PowerShell as Administrator and try again." -ForegroundColor Yellow
    Write-Host ""
    Write-Host "Press any key to exit..."
    $null = $Host.UI.RawUI.ReadKey("NoEcho,IncludeKeyDown")
    exit 1
}

Write-Host "Arc Hydro RAS Commander Toolbox Installer" -ForegroundColor Cyan
Write-Host ("=" * 50) -ForegroundColor Cyan
Write-Host ""

# Get the script directory (Resources folder) and then get parent (repository root)
$scriptDir = Split-Path -Parent $MyInvocation.MyCommand.Definition
$repoRoot = Split-Path -Parent $scriptDir

Write-Host "Script location: $scriptDir" -ForegroundColor Gray
Write-Host "Repository root: $repoRoot" -ForegroundColor Gray
Write-Host ""

# Function to find ArcGIS Pro installation
function Find-ArcGISPro {
    $potentialPaths = @(
        "C:\Program Files\ArcGIS\Pro",
        "C:\Program Files (x86)\ArcGIS\Pro",
        "$env:ProgramFiles\ArcGIS\Pro",
        "${env:ProgramFiles(x86)}\ArcGIS\Pro"
    )
    
    # Check each potential path
    foreach ($path in $potentialPaths) {
        if (Test-Path $path) {
            $toolboxPath = Join-Path $path "Resources\ArcToolBox"
            if (Test-Path $toolboxPath) {
                return $path
            }
        }
    }
    
    # Check registry
    try {
        $regPath = "HKLM:\SOFTWARE\ESRI\ArcGISPro"
        if (Test-Path $regPath) {
            $installDir = (Get-ItemProperty -Path $regPath -Name InstallDir -ErrorAction SilentlyContinue).InstallDir
            if ($installDir -and (Test-Path $installDir)) {
                return $installDir
            }
        }
    } catch {
        # Registry check failed, continue
    }
    
    return $null
}

# Find ArcGIS Pro installation
$arcgisProPath = Find-ArcGISPro
if (-not $arcgisProPath) {
    Write-Host "ERROR: Could not find ArcGIS Pro installation." -ForegroundColor Red
    Write-Host "Please ensure ArcGIS Pro is installed." -ForegroundColor Yellow
    Write-Host ""
    Write-Host "Press any key to exit..."
    $null = $Host.UI.RawUI.ReadKey("NoEcho,IncludeKeyDown")
    exit 1
}

Write-Host "Found ArcGIS Pro at: $arcgisProPath" -ForegroundColor Green
Write-Host ""

# Define source and destination mappings
$mappings = @(
    @{
        Name = "Python Scripts"
        Source = Join-Path $repoRoot "Scripts\archydro"
        Destination = Join-Path $arcgisProPath "Resources\ArcToolBox\Scripts\archydro"
        Type = "Directory"
        Required = $true
        Filter = "rc_*.py"
    },
    @{
        Name = "Python Toolbox"
        Source = Join-Path $repoRoot "toolboxes\RAS-Commander.pyt"
        Destination = Join-Path $arcgisProPath "Resources\ArcToolBox\toolboxes\RAS-Commander.pyt"
        Type = "File"
        Required = $true
    },
    @{
        Name = "Layer Templates"
        Source = Join-Path $repoRoot "Templates\Layers\archydro"
        Destination = Join-Path $arcgisProPath "Resources\ArcToolBox\Templates\Layers\archydro"
        Type = "Directory"
        Required = $false
    },
    @{
        Name = "Images"
        Source = Join-Path $repoRoot "Images"
        Destination = Join-Path $arcgisProPath "Resources\ArcToolBox\Images"
        Type = "Directory"
        Required = $false
    },
    @{
        Name = "Geodatabase Template"
        Source = Join-Path $repoRoot "Data\archydro\Ras2DTemplate.gdb"
        Destination = Join-Path $arcgisProPath "Resources\ArcToolBox\Data\archydro\Ras2DTemplate.gdb"
        Type = "Directory"
        Required = $false
    }
)

Write-Host "Installing Arc Hydro RAS Commander components..." -ForegroundColor Yellow
Write-Host ""

$successCount = 0
$errorCount = 0
$skippedCount = 0

foreach ($mapping in $mappings) {
    Write-Host "Installing: $($mapping.Name)" -ForegroundColor White
    
    if (-not (Test-Path $mapping.Source)) {
        if ($mapping.Required) {
            Write-Host "  ERROR: Source not found: $($mapping.Source)" -ForegroundColor Red
            $errorCount++
        } else {
            Write-Host "  Skipping (optional component not found)" -ForegroundColor Gray
            $skippedCount++
        }
        continue
    }
    
    try {
        # Create parent directory if it doesn't exist
        $parentDir = Split-Path -Parent $mapping.Destination
        if (-not (Test-Path $parentDir)) {
            New-Item -ItemType Directory -Path $parentDir -Force | Out-Null
        }
        
        # Remove existing destination if it exists
        if ($mapping.Name -eq "Python Toolbox") {
            # Remove existing RAS Commander toolbox if present
            if (Test-Path $mapping.Destination) {
                Remove-Item -Path $mapping.Destination -Force
                Write-Host "  Removed existing: RAS-Commander.pyt" -ForegroundColor Gray
            }
            # Also remove any old XML files (both old and new naming)
            $destDir = Split-Path -Parent $mapping.Destination
            $xmlFiles = @(
                Join-Path $destDir "RAS Commander.pyt",
                Join-Path $destDir "RAS Commander.pyt.xml",
                Join-Path $destDir "RAS Commander.LoadHECRAS1DGeometry.pyt.xml",
                Join-Path $destDir "RAS Commander.LoadHECRAS2DGeometry.pyt.xml",
                Join-Path $destDir "RAS Commander.LoadHECRAS2DResults.pyt.xml",
                Join-Path $destDir "RAS Commander.LoadRASTerrain.pyt.xml",
                Join-Path $destDir "RAS Commander.OrganizeRASProject.pyt.xml",
                Join-Path $destDir "RAS-Commander.pyt.xml",
                Join-Path $destDir "RAS-Commander.LoadHECRAS1DGeometry.pyt.xml",
                Join-Path $destDir "RAS-Commander.LoadHECRAS2DGeometry.pyt.xml",
                Join-Path $destDir "RAS-Commander.LoadHECRAS2DResults.pyt.xml",
                Join-Path $destDir "RAS-Commander.LoadRASTerrain.pyt.xml",
                Join-Path $destDir "RAS-Commander.OrganizeRASProject.pyt.xml"
            )
            foreach ($xmlFile in $xmlFiles) {
                if (Test-Path $xmlFile) {
                    Remove-Item -Path $xmlFile -Force
                    Write-Host "  Removed old XML: $(Split-Path $xmlFile -Leaf)" -ForegroundColor Gray
                }
            }
        } elseif ($mapping.Type -eq "Directory" -and $mapping.Filter) {
            # Remove only matching files in the destination directory
            if (Test-Path $mapping.Destination) {
                Get-ChildItem -Path $mapping.Destination -Filter $mapping.Filter -File | Remove-Item -Force
            }
        } elseif (Test-Path $mapping.Destination) {
            Remove-Item -Path $mapping.Destination -Recurse -Force
        }
        
        # Copy the content
        if ($mapping.Type -eq "Directory" -and $mapping.Filter) {
            if (-not (Test-Path $mapping.Destination)) {
                New-Item -ItemType Directory -Path $mapping.Destination -Force | Out-Null
            }
            Get-ChildItem -Path $mapping.Source -Filter $mapping.Filter | ForEach-Object {
                Copy-Item -Path $_.FullName -Destination $mapping.Destination -Force
            }
        } elseif ($mapping.Type -eq "Directory") {
            Copy-Item -Path $mapping.Source -Destination $mapping.Destination -Recurse -Force
        } else {
            # For files, ensure the destination directory exists
            $destDir = Split-Path -Parent $mapping.Destination
            if (-not (Test-Path $destDir)) {
                New-Item -ItemType Directory -Path $destDir -Force | Out-Null
            }
            Copy-Item -Path $mapping.Source -Destination $mapping.Destination -Force
        }
        
        Write-Host "  Success: Copied to $($mapping.Destination)" -ForegroundColor Green
        $successCount++
        
    } catch {
        Write-Host "  ERROR: Failed to copy - $_" -ForegroundColor Red
        $errorCount++
    }
}

Write-Host ""
Write-Host ("=" * 50) -ForegroundColor Cyan
Write-Host "Installation Summary:" -ForegroundColor Cyan
Write-Host "  Successful: $successCount components" -ForegroundColor Green
Write-Host "  Failed: $errorCount components" -ForegroundColor $(if ($errorCount -gt 0) { "Red" } else { "Gray" })
Write-Host "  Skipped: $skippedCount optional components" -ForegroundColor Gray
Write-Host ""

if ($errorCount -gt 0) {
    Write-Host "Some components failed to install. Please check the errors above." -ForegroundColor Yellow
} else {
    Write-Host "All components installed successfully!" -ForegroundColor Green
    Write-Host ""
    Write-Host "To use the toolbox in ArcGIS Pro:" -ForegroundColor Yellow
    Write-Host "  1. Open ArcGIS Pro"
    Write-Host "  2. Go to the Catalog pane"
    Write-Host "  3. Navigate to Toolboxes > RAS-Commander"
    Write-Host "  4. The tools will be available there"
}

Write-Host ""
Write-Host ("-" * 50) -ForegroundColor Gray

# Ask about development mode (symlinks)
Write-Host ""
$response = Read-Host "Would you like to create development symlinks instead of copying? (y/N)"

if ($response -eq 'y' -or $response -eq 'Y') {
    Write-Host ""
    Write-Host "Creating symlinks for development mode..." -ForegroundColor Yellow
    Write-Host "(This allows changes in the repo to be reflected immediately)" -ForegroundColor Gray
    Write-Host ""
    
    $symlinkSuccess = 0
    $symlinkError = 0
    
    foreach ($mapping in $mappings) {
        if (-not (Test-Path $mapping.Source)) {
            continue
        }
        
        Write-Host "Creating symlink: $($mapping.Name)" -ForegroundColor White
        
        try {
            # Remove existing destination if it exists
            if ($mapping.Name -eq "Python Toolbox") {
                # Remove existing RAS Commander toolbox if present
                if (Test-Path $mapping.Destination) {
                    Remove-Item -Path $mapping.Destination -Force
                }
            } elseif ($mapping.Type -eq "Directory" -and $mapping.Filter) {
                # Remove only matching files in the destination directory
                if (Test-Path $mapping.Destination) {
                    Get-ChildItem -Path $mapping.Destination -Filter $mapping.Filter -File | Remove-Item -Force
                }
            } elseif (Test-Path $mapping.Destination) {
                Remove-Item -Path $mapping.Destination -Recurse -Force
            }
            
            # Create parent directory if needed
            $parentDir = Split-Path -Parent $mapping.Destination
            if (-not (Test-Path $parentDir)) {
                New-Item -ItemType Directory -Path $parentDir -Force | Out-Null
            }
            
            # Create symlink
            if ($mapping.Type -eq "Directory" -and $mapping.Filter) {
                Get-ChildItem -Path $mapping.Source -Filter $mapping.Filter | ForEach-Object {
                    $destFile = Join-Path $mapping.Destination $_.Name
                    if (Test-Path $destFile) {
                        Remove-Item -Path $destFile -Force
                    }
                    New-Item -ItemType SymbolicLink -Path $destFile -Target $_.FullName | Out-Null
                }
                Write-Host "  Success: Symlinked filtered files to $($mapping.Destination)" -ForegroundColor Green
                $symlinkSuccess++
            } else {
                New-Item -ItemType SymbolicLink -Path $mapping.Destination -Target $mapping.Source | Out-Null
                Write-Host "  Success: Symlinked to $($mapping.Destination)" -ForegroundColor Green
                $symlinkSuccess++
            }
            
        } catch {
            Write-Host "  ERROR: Failed to create symlink - $_" -ForegroundColor Red
            $symlinkError++
        }
    }
    
    Write-Host ""
    Write-Host "Symlink Summary:" -ForegroundColor Cyan
    Write-Host "  Successful: $symlinkSuccess symlinks" -ForegroundColor Green
    Write-Host "  Failed: $symlinkError symlinks" -ForegroundColor $(if ($symlinkError -gt 0) { "Red" } else { "Gray" })
}

Write-Host ""
Write-Host "Press any key to exit..."
$null = $Host.UI.RawUI.ReadKey("NoEcho,IncludeKeyDown")
==================================================

File: c:\GH\ras-commander-hydro\Resources\RAS Commander Arc Hydro Tools - ESRI 2025 User Conference Launch Slides.txt
==================================================
RAS Commander is coming to 
Arc Hydro Tools

Bringing HEC-RAS 6.x Direct Data Access to ArcGIS

•ESRI has Partnered with CLB Engineering 
Corporation's Willliam "Bill" Katzenmeyer, creator 
of the RAS CommanderOpen Source Python 
Library
•Usingan LLM Forward approach, CLB has rapidly 
developed a subset of HDF data 
accessfunctionality into an ArcGIS Toolbox
•Features for Initial Release:
o1D and 2D Geometry Extraction
oMaxWSE and Velocity as 2DMesh Results
oTerrain Import for Inundation Mapping
oSupport for HEC-RAS 2D Models






Free and Open Source Tools Brought to You By: 


Load 1D and 2D 
Geometry Elements

Assist with Report Figures and Visualizations

•Bring all of your 1D and 2D geometry 
elements right into ArcGIS
•Includes Pipe Networks for Urban 
Drainage! 
•Column names are preserved and should 
be identical to RAS Mapper except where 
special characters were used in the HDF


Bonus: Exporting Mesh Cells as polygons 
allows for mesh-based spatial analysis 
methods not available in RAS Mapper.



2D Results Summaries

Raw Mesh Results Data as Points

•Max WSEL at Cell Centers
•Max Velocity at Cell Faces
•Timestamps for Maximum Values



Note: 1D-only results can still be loaded via SDF and existing Arc Hydro workflows

Direct 2D Mesh Results Export Provides Unique 
Opportunities for Advanced Spatial Analysis


Organize an Entire HEC-RAS Projectas Geodatabase

Create Rich HEC-RAS Data Inventories in ArcGIS

•Finds All Results HDF Files .pXX.hdf
•Extracts all 1D, 2D Geometry and 2D Mesh 
Results Data 
•Creates Non-Overlapping Layer Names
•Organizes layers by Plan Number


Coming Soon: 

Improved Schemas

Improved Layer Styling

Instructional Webinar



Roadmap

Where is RAS Commander in Arc Hydro going?

This is an LLM Forward Experiment, coded after 
an ASFPM brainstorming session in Late May 
2025. With public release in Early July 2025, this 
was alittle over a month of time. 

Our First Goal is to Get Community Feedback 
about desired HEC-RAS 6.x data integration 
capabilities in ArcGIS

Our Current Roadmap: 

Layer Styling

Schemas

1D Results

2D PipesResults

Fluvial/Pluvial 
Delineation

Probabilistic Approaches

Land Use Layers

Plan and Unsteady 
Metadata

Syncing Changes back 
to HEC-RAS

Community-Requested 
Features

Tutorials

Webinar this Fall!




Github Repository

Contributors 
Welcome!


LLM Forward Engineering for 
Data-Driven Hazard Mitigation



==================================================

File: c:\GH\ras-commander-hydro\Resources\SELF_CONTAINED_TOOLBOX.md
==================================================
# Self-Contained Python Toolbox Implementation

This document describes the implementation of a fully self-contained Python toolbox for the Arc Hydro RAS Commander tools, eliminating the need for separate XML metadata files.

## Overview

The toolbox has been enhanced to embed all metadata directly in the Python code, making it:
- Easier to maintain (single source of truth)
- Compatible with system toolbox installations
- Version control friendly
- Simpler to distribute

## Changes Made

### 1. Enhanced Toolbox Class (`RAS Commander.pyt`)

Added comprehensive metadata properties:
- `author` - Attribution information
- `credits` - Sponsorship details
- `version` - Version tracking
- `homepage` - Project URL
- `tags` - Searchability keywords

### 2. Enhanced Tool Classes

Each tool now includes:

#### Core Properties
- `label` - Tool display name
- `description` - Full multi-line description
- `summary` - Brief one-line summary
- `usage` - Detailed usage instructions

#### Extended Metadata
- `category` - Tool categorization
- `tags` - Search keywords
- `credits` - Attribution
- `author` - Tool author
- `version` - Version number

#### Enhanced Methods
- `getHelp()` - Returns help documentation with fallback options
- `getCodeSamples()` - Provides programmatic usage examples

### 3. Enhanced Parameter Documentation

Each parameter now includes:
- Multi-line descriptions with formatting
- Usage guidelines
- Performance considerations
- Category assignments for organization

### 4. Code Samples

Each tool provides multiple code samples demonstrating:
- Basic usage
- Advanced features
- Performance optimization
- Batch processing
- Integration workflows

## Example Implementation

```python
class LoadHECRAS1DGeometry(object):
    def __init__(self):
        # Core properties
        self.label = "Load HEC-RAS 1D Geometry Layers"
        self.description = """Detailed description..."""
        
        # Extended metadata
        self.summary = "Extract 1D geometry elements from HEC-RAS HDF files"
        self.usage = """Usage instructions..."""
        self.category = "HEC-RAS Data Import"
        self.tags = ["HEC-RAS", "1D Geometry", "Cross Sections"]
        self.author = "CLB Engineering Corporation"
        self.version = "1.0.0"
    
    def getHelp(self, tool_name=None):
        """Return help documentation"""
        # Try local file first, fallback to online
        return help_url
    
    def getCodeSamples(self):
        """Provide usage examples"""
        return [
            {
                "title": "Basic Import",
                "description": "...",
                "code": "..."
            }
        ]
```

## Installation Changes

The `install_toolbox.ps1` script has been updated to:
- Only copy the .pyt file (no XML files)
- Clean up old XML files during installation
- Support both copy and symlink modes

## Benefits

1. **Single Source of Truth** - All documentation in Python code
2. **Version Control** - Changes tracked in one file
3. **System Toolbox Compatible** - Works with read-only installations
4. **No Sync Issues** - Eliminates XML/Python mismatch problems
5. **Programmatic Access** - Documentation accessible via Python API

## Testing

To test the self-contained toolbox:

1. Run the installation script
2. Open ArcGIS Pro
3. Navigate to the toolbox
4. Verify:
   - Tool descriptions appear correctly
   - Parameter help is displayed
   - Help buttons work
   - Code samples are accessible

## Future Enhancements

- Add localization support
- Implement dynamic help based on user context
- Add video tutorial links
- Include performance benchmarks in code samples
==================================================

File: c:\GH\ras-commander-hydro\Resources\uninstall_toolbox.ps1
==================================================
# uninstall_toolbox.ps1
#
# Uninstalls the Arc Hydro RAS Commander toolbox from ArcGIS Pro.
# This script removes all toolbox files from the ArcGIS Pro installation directories.
#
# Usage:
#   Right-click on this file and select "Run with PowerShell" 
#   OR
#   Open PowerShell as Administrator and run: .\uninstall_toolbox.ps1
#
# Note: This script requires administrator privileges to remove files from Program Files.

# Check if running as administrator
if (-NOT ([Security.Principal.WindowsPrincipal] [Security.Principal.WindowsIdentity]::GetCurrent()).IsInRole([Security.Principal.WindowsBuiltInRole] "Administrator")) {
    Write-Host "This script requires Administrator privileges." -ForegroundColor Red
    Write-Host "Please run PowerShell as Administrator and try again." -ForegroundColor Yellow
    Write-Host ""
    Write-Host "Press any key to exit..."
    $null = $Host.UI.RawUI.ReadKey("NoEcho,IncludeKeyDown")
    exit 1
}

Write-Host "Arc Hydro RAS Commander Toolbox Uninstaller" -ForegroundColor Cyan
Write-Host ("=" * 50) -ForegroundColor Cyan
Write-Host ""

# Get the script directory (Resources folder) and then get parent (repository root)
$scriptDir = Split-Path -Parent $MyInvocation.MyCommand.Definition
$repoRoot = Split-Path -Parent $scriptDir

Write-Host "Script location: $scriptDir" -ForegroundColor Gray
Write-Host "Repository root: $repoRoot" -ForegroundColor Gray
Write-Host ""

# Function to find ArcGIS Pro installation
function Find-ArcGISPro {
    $potentialPaths = @(
        "C:\Program Files\ArcGIS\Pro",
        "C:\Program Files (x86)\ArcGIS\Pro",
        "$env:ProgramFiles\ArcGIS\Pro",
        "${env:ProgramFiles(x86)}\ArcGIS\Pro"
    )
    
    # Check each potential path
    foreach ($path in $potentialPaths) {
        if (Test-Path $path) {
            $toolboxPath = Join-Path $path "Resources\ArcToolBox"
            if (Test-Path $toolboxPath) {
                return $path
            }
        }
    }
    
    # Check registry
    try {
        $regPath = "HKLM:\SOFTWARE\ESRI\ArcGISPro"
        if (Test-Path $regPath) {
            $installDir = (Get-ItemProperty -Path $regPath -Name InstallDir -ErrorAction SilentlyContinue).InstallDir
            if ($installDir -and (Test-Path $installDir)) {
                return $installDir
            }
        }
    } catch {
        # Registry check failed, continue
    }
    
    return $null
}

# Find ArcGIS Pro installation
$arcgisProPath = Find-ArcGISPro
if (-not $arcgisProPath) {
    Write-Host "ERROR: Could not find ArcGIS Pro installation." -ForegroundColor Red
    Write-Host "ArcGIS Pro may not be installed or the toolbox may already be uninstalled." -ForegroundColor Yellow
    Write-Host ""
    Write-Host "Press any key to exit..."
    $null = $Host.UI.RawUI.ReadKey("NoEcho,IncludeKeyDown")
    exit 1
}

Write-Host "Found ArcGIS Pro at: $arcgisProPath" -ForegroundColor Green
Write-Host ""

# Define paths to remove
$pathsToRemove = @(
    @{
        Name = "Python Scripts"
        Path = Join-Path $arcgisProPath "Resources\ArcToolBox\Scripts\archydro\rc_*.py"
        Type = "Files"
    },
    @{
        Name = "Python Toolbox (New)"
        Path = Join-Path $arcgisProPath "Resources\ArcToolBox\toolboxes\RAS-Commander.pyt"
        Type = "File"
    },
    @{
        Name = "Python Toolbox (Old)"
        Path = Join-Path $arcgisProPath "Resources\ArcToolBox\toolboxes\RAS Commander.pyt"
        Type = "File"
    },
    @{
        Name = "Python Toolbox XML (New)"
        Path = Join-Path $arcgisProPath "Resources\ArcToolBox\toolboxes\RAS-Commander.pyt.xml"
        Type = "File"
    },
    @{
        Name = "Python Toolbox XML (Old)"
        Path = Join-Path $arcgisProPath "Resources\ArcToolBox\toolboxes\RAS Commander.pyt.xml"
        Type = "File"
    },
    @{
        Name = "Tool-specific XML files"
        Path = Join-Path $arcgisProPath "Resources\ArcToolBox\toolboxes\RAS*.pyt.xml"
        Type = "Files"
    },
    @{
        Name = "Layer Templates"
        Path = Join-Path $arcgisProPath "Resources\ArcToolBox\Templates\Layers\archydro"
        Type = "Directory"
    },
    @{
        Name = "Geodatabase Template"
        Path = Join-Path $arcgisProPath "Resources\ArcToolBox\Data\archydro\Ras2DTemplate.gdb"
        Type = "Directory"
    }
)

# Note: We don't remove Images directory as it might contain files from other tools

Write-Host "This will remove the following Arc Hydro RAS Commander components:" -ForegroundColor Yellow
foreach ($item in $pathsToRemove) {
    if (Test-Path $item.Path) {
        # Check if it's a symlink
        $itemInfo = Get-Item $item.Path -Force -ErrorAction SilentlyContinue
        if ($itemInfo -and $itemInfo.LinkType) {
            Write-Host "  - $($item.Name) (symlink): $($item.Path)" -ForegroundColor Cyan
        } else {
            Write-Host "  - $($item.Name): $($item.Path)" -ForegroundColor White
        }
    }
}

Write-Host ""
$response = Read-Host "Do you want to proceed with uninstallation? (y/N)"

if ($response -ne 'y' -and $response -ne 'Y') {
    Write-Host ""
    Write-Host "Uninstallation cancelled." -ForegroundColor Yellow
    Write-Host "Press any key to exit..."
    $null = $Host.UI.RawUI.ReadKey("NoEcho,IncludeKeyDown")
    exit 0
}

Write-Host ""
Write-Host "Uninstalling Arc Hydro RAS Commander components..." -ForegroundColor Yellow
Write-Host ""

$successCount = 0
$errorCount = 0
$notFoundCount = 0

foreach ($item in $pathsToRemove) {
    Write-Host "Removing: $($item.Name)" -ForegroundColor White
    
    if (-not (Test-Path $item.Path)) {
        Write-Host "  Not found - skipping" -ForegroundColor Gray
        $notFoundCount++
        continue
    }
    
    try {
        # Check if it's a symlink
        $itemInfo = Get-Item $item.Path -Force -ErrorAction SilentlyContinue
        $isSymlink = $itemInfo -and $itemInfo.LinkType
        
        if ($item.Type -eq "Directory") {
            if ($isSymlink) {
                # For symlinked directories, just remove the link
                (Get-Item $item.Path).Delete()
                Write-Host "  Success: Removed symlink" -ForegroundColor Green
            } else {
                # For regular directories, remove recursively
                Remove-Item -Path $item.Path -Recurse -Force
                Write-Host "  Success: Removed directory and all contents" -ForegroundColor Green
            }
        } elseif ($item.Type -eq "Files") {
            # For file patterns (like rc_*.py)
            $parentPath = Split-Path $item.Path -Parent
            $pattern = Split-Path $item.Path -Leaf
            Get-ChildItem -Path $parentPath -Filter $pattern -ErrorAction SilentlyContinue | ForEach-Object {
                Remove-Item -Path $_.FullName -Force
            }
            Write-Host "  Success: Removed matching files" -ForegroundColor Green
        } else {
            # For single files (including symlinked files)
            Remove-Item -Path $item.Path -Force
            Write-Host "  Success: Removed file" -ForegroundColor Green
        }
        
        $successCount++
        
    } catch {
        Write-Host "  ERROR: Failed to remove - $_" -ForegroundColor Red
        $errorCount++
    }
}

Write-Host ""
Write-Host ("=" * 50) -ForegroundColor Cyan
Write-Host "Uninstallation Summary:" -ForegroundColor Cyan
Write-Host "  Removed: $successCount components" -ForegroundColor Green
Write-Host "  Failed: $errorCount components" -ForegroundColor $(if ($errorCount -gt 0) { "Red" } else { "Gray" })
Write-Host "  Not found: $notFoundCount components" -ForegroundColor Gray
Write-Host ""

if ($errorCount -gt 0) {
    Write-Host "Some components failed to uninstall. Please check the errors above." -ForegroundColor Yellow
    Write-Host "You may need to manually remove these items." -ForegroundColor Yellow
} elseif ($successCount -eq 0 -and $notFoundCount -gt 0) {
    Write-Host "No components were found to uninstall." -ForegroundColor Yellow
    Write-Host "The toolbox may have already been uninstalled." -ForegroundColor Yellow
} else {
    Write-Host "Arc Hydro RAS Commander has been successfully uninstalled!" -ForegroundColor Green
}

# Clean up empty parent directories if they exist
Write-Host ""
Write-Host "Cleaning up empty directories..." -ForegroundColor Yellow

$parentDirsToCheck = @(
    Join-Path $arcgisProPath "Resources\ArcToolBox\Scripts\ras_commander"
    Join-Path $arcgisProPath "Resources\ArcToolBox\Templates\Layers\archydro"
    Join-Path $arcgisProPath "Resources\ArcToolBox\Data\archydro"
)

foreach ($dir in $parentDirsToCheck) {
    $parent = Split-Path $dir -Parent
    if ((Test-Path $parent) -and (Get-ChildItem $parent -Force | Measure-Object).Count -eq 0) {
        try {
            Remove-Item $parent -Force
            Write-Host "  Removed empty directory: $parent" -ForegroundColor Green
        } catch {
            # Ignore errors for cleanup
        }
    }
}

Write-Host ""
Write-Host "Press any key to exit..."
$null = $Host.UI.RawUI.ReadKey("NoEcho,IncludeKeyDown")
==================================================

File: c:\GH\ras-commander-hydro\toolboxes\RAS-Commander.pyt
==================================================
# -*- coding: utf-8 -*-
#
# RAS Commander.pyt
#
# ArcGIS Python Toolbox for HEC-RAS HDF5 Data Integration
# ===================================================================================
#
# ESRI PARTNERSHIP ANNOUNCEMENT:
# ESRI has partnered with CLB Engineering Corporation's William "Bill" Katzenmeyer,
# Vice President and creator of the RAS Commander Open Source Python Library, to bring
# powerful HEC-RAS 6.x HDF5 data extraction capabilities directly into ArcGIS Pro.
#
# LAUNCHING AT ESRI USER CONFERENCE 2025
# This toolbox represents a groundbreaking application of LLM Forward engineering,
# developed in just over a month following an ASFPM brainstorming session in May 2025.
#
# DESCRIPTION:
# This toolbox provides comprehensive tools for loading and visualizing HEC-RAS 1D and 2D
# geometry, terrain, and results data from HDF5 files directly within ArcGIS Pro.
#
# KEY FEATURES:
# • Direct HDF5 data access without manual conversion
# • Support for 1D and 2D geometry elements
# • Pipe network extraction (storm/sewer systems)
# • Results visualization (Max WSE and Velocity)
# • Terrain import from RAS Mapper
# • Complete project organization
#
# ORIGIN AND ATTRIBUTION:
# This toolbox is a direct port of the HDF5 data extraction logic from the
# ras-commander library, adapted for the ArcGIS platform using CLB's innovative
# LLM Forward approach.
#
# RESOURCES AND LINKS:
# • RAS Commander Arc Hydro Tools: https://github.com/gpt-cmdr/ras-commander-hydro
# • RAS Commander Library: https://github.com/gpt-cmdr/ras-commander
# • CLB Engineering Corporation: https://clbengineering.com/
# • LLM Forward Approach: https://clbengineering.com/
# • Engineering with LLMs: https://engineeringwithllms.info/
#
# COMMUNITY DRIVEN:
# This is a community-driven effort. We're actively seeking feedback from:
# - Municipalities integrating HEC-RAS data into dashboards
# - Engineers communicating multi-hazard flood risk
# - GIS professionals preparing 2D model data
# - Researchers analyzing model results
#
# Share your ideas: https://github.com/gpt-cmdr/ras-commander-hydro/issues
#
# ===================================================================================

import sys
import os

# Add the Scripts directory to the Python path so we can import our modules
toolbox_dir = os.path.dirname(os.path.abspath(__file__))
scripts_dir = os.path.join(os.path.dirname(toolbox_dir), 'Scripts', 'archydro')
if scripts_dir not in sys.path:
    sys.path.insert(0, scripts_dir)

# Import the tool classes from our modules
from rc_load_ras_terrain import LoadRASTerrain
from rc_load_hecras_2d_geometry import LoadHECRAS2DGeometry
from rc_load_hecras_2d_results import LoadHECRAS2DResults
from rc_load_hecras_1d_geometry import LoadHECRAS1DGeometry
from rc_organize_ras_project import OrganizeRASProject


class Toolbox(object):
    """
    ArcGIS Python Toolbox for loading HEC-RAS 1D and 2D geometry, terrain, and results layers.
    
    ESRI USER CONFERENCE 2025 LAUNCH EDITION
    
    Developed through ESRI's partnership with CLB Engineering Corporation's William "Bill" Katzenmeyer,
    this toolbox brings the power of the RAS Commander library directly into Arc Hydro Tools.
    """
    def __init__(self):
        # Core toolbox properties
        self.label = "RAS Commander Tools"
        self.alias = "RASCommander"
        self.description = """RAS Commander Arc Hydro Tools - Bringing HEC-RAS 6.x Direct Data Access to ArcGIS
        
        🎉 LAUNCHING AT ESRI USER CONFERENCE 2025 🎉
        
        This toolbox is the result of ESRI's partnership with CLB Engineering Corporation's 
        William "Bill" Katzenmeyer, creator of the RAS Commander Open Source Python Library.
        
        Using CLB's innovative LLM Forward approach, this toolbox was developed in just over 
        a month, demonstrating the transformative potential of AI-assisted development in 
        the water resources sector.
        
        KEY CAPABILITIES:
        • Direct HDF5 Import - No conversion needed
        • 1D and 2D Geometry - Complete model extraction
        • Pipe Networks - Storm/sewer system support
        • Results Analysis - Max WSE and velocity visualization
        • Terrain Integration - RAS Mapper VRT import
        • Project Organization - Batch processing tools
        
        COMMUNITY DRIVEN:
        We're seeking feedback to shape future development! Are you a:
        • Municipality looking to integrate HEC-RAS data into dashboards?
        • Engineer communicating multi-hazard flood risk?
        • GIS Professional preparing 2D model data?
        • Researcher analyzing model results?
        
        Share your use cases and feature requests!
        
        RESOURCES:
        • RAS Commander Arc Hydro Tools: https://github.com/gpt-cmdr/ras-commander-hydro
        • RAS Commander Library: https://github.com/gpt-cmdr/ras-commander
        • CLB Engineering: https://clbengineering.com/
        • Engineering with LLMs: https://engineeringwithllms.info/
        
        Developed by CLB Engineering in partnership with ESRI."""
        
        # Tool list
        self.tools = [LoadHECRAS1DGeometry, LoadHECRAS2DGeometry, LoadHECRAS2DResults, LoadRASTerrain, OrganizeRASProject]
        
        # Toolbox metadata
        self.author = "William 'Bill' Katzenmeyer, P.E. - CLB Engineering Corporation"
        self.credits = """ESRI PARTNERSHIP:
        Developed through ESRI's partnership with CLB Engineering Corporation
        
        DEVELOPMENT APPROACH:
        Created using CLB's LLM Forward methodology - Learn more at https://clbengineering.com/
        
        BASED ON:
        RAS Commander library: https://github.com/gpt-cmdr/ras-commander
        
        LEARN MORE:
        • About the author's work: https://engineeringwithllms.info/
        • CLB Engineering: https://clbengineering.com/"""
        
        self.version = "1.0.0 - Esri UC 2025 Launch Edition"
        self.homepage = "https://github.com/gpt-cmdr/ras-commander-hydro"
        
        # Keywords for searchability
        self.tags = ["HEC-RAS", "Arc Hydro", "HDF5", "River Analysis", "Hydraulic Modeling", 
                     "Flood Modeling", "2D Mesh", "RAS Mapper", "CLB Engineering", "LLM Forward",
                     "Esri UC 2025", "William Katzenmeyer", "Community Driven"]
==================================================

File: c:\GH\ras-commander-hydro\Scripts\archydro\rc_load_hecras_1d_geometry.py
==================================================
# -*- coding: utf-8 -*-
"""
LoadHECRAS1DGeometry.py

Tool for loading HEC-RAS 1D geometry layers from HDF files including cross sections,
river centerlines, bank lines, and hydraulic structures.
"""

import arcpy
import os
import h5py
import numpy as np

# Import helper functions from utils
from rc_utils import (
    get_ras_projection_wkt,
    cache_hdf_metadata,
    write_features_to_fc,
    get_dynamic_fields_from_data,
    setup_geodatabase_output,
    get_unique_fc_name,
    add_feature_class_metadata,
    extract_project_and_plan_info,
    create_geodatabase_from_hdf,
    get_feature_dataset_name,
    get_feature_class_name
)


class LoadHECRAS1DGeometry(object):
    """
    Loads 1D geometry elements from a HEC-RAS HDF file.
    """
    def __init__(self):
        # Core properties
        self.label = "Load HEC-RAS 1D Geometry Layers"
        self.description = """Extracts 1D geometry elements from a HEC-RAS HDF file including cross sections, river centerlines, bank lines, and hydraulic structures.
        
        This tool extracts various 1D geometry elements from HEC-RAS geometry (g*.hdf) or plan (p*.hdf) files.
        
        Available geometry elements include:
        • Cross Sections - River cross section cut lines with station-elevation data
        • River Centerlines - Main river/reach centerlines
        • Bank Lines - Left and right bank lines
        • Edge Lines - River edge lines for terrain processing
        • 1D Structures - Bridges, culverts, weirs, and other structures
        
        Note: Each selected element will create a separate feature class."""
        
        # Extended metadata properties
        self.summary = "Extract 1D geometry elements from HEC-RAS HDF files"
        self.usage = """Select a HEC-RAS geometry or plan HDF file and choose which 1D geometry elements to extract.
        
        Steps:
        1. Browse to a HEC-RAS geometry (g*.hdf) or plan (p*.hdf) file
        2. Select which geometry elements to extract
        3. Specify output locations for each selected element
        4. Optionally create an organized geodatabase
        
        The tool will automatically detect the coordinate system from the HDF file or associated .prj files."""
        
        # Tool behavior
        self.canRunInBackground = False
        # self.category = "HEC-RAS Data Import"  # REMOVE THIS LINE
        
        # Documentation and credits
        self.tags = ["HEC-RAS", "1D Geometry", "Cross Sections", "River Centerlines", "Hydraulic Modeling", "Arc Hydro"]
        self.credits = "CLB Engineering Corporation"
        self.author = "CLB Engineering Corporation"
        self.version = "1.0.0"
        
        # Geometry elements
        self.CROSS_SECTIONS = "Cross Sections"
        self.RIVER_CENTERLINES = "River Centerlines"
        self.BANK_LINES = "Bank Lines"
        self.EDGE_LINES = "Edge Lines"
        self.STRUCTURES = "1D Structures"
        
        # Cache for HDF metadata
        self._hdf_cache = {}

    def getParameterInfo(self):
        geometry_elements = [self.CROSS_SECTIONS, self.RIVER_CENTERLINES, self.BANK_LINES, 
                           self.EDGE_LINES, self.STRUCTURES]

        params = [
            arcpy.Parameter(displayName="Geometry or Plan HDF File", name="input_hdf", datatype="DEFile", 
                          parameterType="Required", direction="Input"),
            arcpy.Parameter(displayName="Override CRS (Optional)", name="override_crs", datatype="GPSpatialReference", 
                          parameterType="Optional", direction="Input"),
            
            # Geometry elements to load
            arcpy.Parameter(displayName="Geometry Elements to Load", name="geometry_elements", datatype="GPString", 
                          parameterType="Required", direction="Input", multiValue=True),
            
            # Output parameters
            arcpy.Parameter(displayName="Output Cross Sections", name="output_cross_sections", datatype="DEFeatureClass", 
                          parameterType="Optional", direction="Output"),
            arcpy.Parameter(displayName="Output River Centerlines", name="output_centerlines", datatype="DEFeatureClass", 
                          parameterType="Optional", direction="Output"),
            arcpy.Parameter(displayName="Output Bank Lines", name="output_bank_lines", datatype="DEFeatureClass", 
                          parameterType="Optional", direction="Output"),
            arcpy.Parameter(displayName="Output Edge Lines", name="output_edge_lines", datatype="DEFeatureClass", 
                          parameterType="Optional", direction="Output"),
            arcpy.Parameter(displayName="Output 1D Structures", name="output_structures", datatype="DEFeatureClass", 
                          parameterType="Optional", direction="Output"),
            
            # Geodatabase organization parameters
            arcpy.Parameter(displayName="Output Geodatabase (Optional)", name="output_gdb", datatype="DEWorkspace", 
                          parameterType="Optional", direction="Output"),
            arcpy.Parameter(displayName="Create New Geodatabase", name="create_gdb", datatype="GPBoolean", 
                          parameterType="Optional", direction="Input")
        ]
        
        # Configure HDF file filter
        params[0].filter.list = ["hdf", "g*.hdf", "p*.hdf"]
        params[0].description = """Select a HEC-RAS geometry file (g*.hdf) or plan file (p*.hdf) containing 1D geometry data.
        
        The tool will automatically detect available geometry elements in the file and extract the selected ones.
        
        Supported file types:
        • Geometry files (g01.hdf, g02.hdf, etc.)
        • Plan files with geometry (p01.hdf, p02.hdf, etc.)"""
        # params[0].category = "Input Data"  # Remove category grouping
        
        params[1].description = """Specify a coordinate reference system if it cannot be determined from the HEC-RAS project files.
        
        The tool will first attempt to read the CRS from:
        1. The HDF file metadata
        2. Associated .prj files in the project directory
        3. The RAS Mapper projection file
        
        Only provide this parameter if automatic detection fails."""
        # params[1].category = "Input Data"  # Remove category grouping
        
        # Set filters for multi-value parameters
        params[2].filter.type = "ValueList"
        params[2].filter.list = geometry_elements
        params[2].value = [self.CROSS_SECTIONS, self.RIVER_CENTERLINES]  # Default selection
        params[2].description = """Select one or more geometry elements to extract from the HDF file.
        
        Available elements:
        • Cross Sections - River cross section cut lines with detailed station-elevation data
        • River Centerlines - Main channel centerlines for each river/reach
        • Bank Lines - Left and right bank station definitions
        • Edge Lines - River edge boundaries for terrain integration
        • 1D Structures - Hydraulic structures including bridges, culverts, inline/lateral weirs
        
        Each selected element will create a separate output feature class with appropriate attributes."""
        # params[2].category = "Geometry Selection"  # Remove category grouping
        
        # Set default output paths and descriptions
        params[3].value = r"memory\CrossSections"
        params[3].description = """Output feature class for 1D cross sections.
        
        Attributes include:
        • River and Reach names
        • Cross section ID
        • Station locations
        • Geometry reference information"""
        
        params[4].value = r"memory\RiverCenterlines"
        params[4].description = """Output feature class for river/reach centerlines.
        
        Attributes include:
        • River name
        • Reach name
        • Length
        • Flow direction"""
        
        params[5].value = r"memory\BankLines"
        params[5].description = """Output feature class for left and right bank lines.
        
        Attributes include:
        • River and Reach names
        • Bank position (Left/Right)
        • Station references"""
        
        params[6].value = r"memory\EdgeLines"
        params[6].description = """Output feature class for river edge lines.
        
        Used for terrain modification and 2D mesh generation.
        Includes river/reach identification attributes."""
        
        params[7].value = r"memory\Structures1D"
        params[7].description = """Output feature class for 1D hydraulic structures.
        
        Structure types include:
        • Bridges
        • Culverts
        • Inline structures (weirs, gates)
        • Lateral structures
        • Pumping stations
        
        Attributes include structure type, name, and hydraulic parameters."""
        
        # Geodatabase parameters
        params[8].description = """Specify a geodatabase to organize all output feature classes.
        
        If provided:
        • All outputs will be created in feature datasets within this geodatabase
        • Feature datasets will be organized by geometry type
        • Automatic naming conventions will be applied
        
        Leave empty to use default output locations."""
        # params[8].category = "Output Organization"  # Remove category grouping
        
        params[9].value = True  # Default to creating new geodatabase
        params[9].description = """Create a new geodatabase based on the HDF file name.
        
        When enabled:
        • Creates geodatabase named: ProjectName.pXX.gdb
        • Organizes outputs in feature datasets
        • Maintains HEC-RAS project structure
        • Preserves all attribute relationships
        
        Recommended for organizing complete HEC-RAS projects."""
        # params[9].category = "Output Organization"  # Remove category grouping
        
        return params

    def isLicensed(self):
        """Set whether tool is licensed to execute."""
        return True

    def updateParameters(self, parameters):
        """Modify the values and properties of parameters before internal validation."""
        # Enable/disable output parameters based on selected elements
        if parameters[2].value:
            selected = parameters[2].valueAsText.split(';') if parameters[2].valueAsText else []
            
            # Enable/disable outputs based on selection
            parameters[3].enabled = self.CROSS_SECTIONS in selected
            parameters[4].enabled = self.RIVER_CENTERLINES in selected
            parameters[5].enabled = self.BANK_LINES in selected
            parameters[6].enabled = self.EDGE_LINES in selected
            parameters[7].enabled = self.STRUCTURES in selected
            
        # Auto-populate geodatabase path when HDF file is selected
        if parameters[0].value and parameters[0].altered:  # input_hdf
            hdf_path = parameters[0].valueAsText
            
            # If create_gdb is True, auto-populate geodatabase path
            if parameters[9].value:  # create_gdb
                project_name, plan_number, base_name = extract_project_and_plan_info(hdf_path)
                gdb_name = f"{base_name}.gdb"
                gdb_path = os.path.join(os.path.dirname(hdf_path), gdb_name)
                parameters[8].value = gdb_path
        return
    
    def updateMessages(self, parameters):
        """Modify messages created by internal validation."""
        # Clear geodatabase validation error if create_gdb is True
        if parameters[9].value and parameters[8].hasError():  # create_gdb and output_gdb has error
            parameters[8].clearMessage()
        return

    # --- HDF Data Extraction Methods ---

    def _get_cross_sections_direct(self, hdf_file, sr):
        """Extracts cross sections from HDF file."""
        try:
            xs_path = "Geometry/Cross Sections"
            if xs_path not in hdf_file:
                arcpy.AddMessage("No cross sections found in HDF file.")
                return [], []
            
            # Check if required datasets exist
            required_datasets = ["Attributes", "Polyline Info", "Polyline Points", 
                               "Station Elevation Info", "Station Elevation Values"]
            for dataset in required_datasets:
                if f"{xs_path}/{dataset}" not in hdf_file:
                    arcpy.AddWarning(f"Cross sections data incomplete: missing '{dataset}' dataset.")
                    return [], []
            
            # Get attributes
            attributes = hdf_file[f"{xs_path}/Attributes"][()]
            
            # Get polyline geometry
            polyline_info = hdf_file[f"{xs_path}/Polyline Info"][()]
            polyline_points = hdf_file[f"{xs_path}/Polyline Points"][()]
            
            # Get station-elevation data
            sta_elev_info = hdf_file[f"{xs_path}/Station Elevation Info"][()]
            sta_elev_values = hdf_file[f"{xs_path}/Station Elevation Values"][()]
            
            # Get Manning's n data
            mannings_info = hdf_file[f"{xs_path}/Manning's n Info"][()]
            mannings_values = hdf_file[f"{xs_path}/Manning's n Values"][()]
            
            valid_data, geometries = [], []
            
            for idx, attr in enumerate(attributes):
                # Get polyline info
                pnt_start, pnt_cnt, _, _ = polyline_info[idx]
                
                if pnt_cnt < 2:
                    continue
                
                # Extract points and create polyline
                points = polyline_points[pnt_start:pnt_start + pnt_cnt]
                arcpy_array = arcpy.Array([arcpy.Point(p[0], p[1]) for p in points])
                geom = arcpy.Polyline(arcpy_array, sr)
                
                # Extract attributes
                river = attr["River"].decode('utf-8', 'ignore').strip()
                reach = attr["Reach"].decode('utf-8', 'ignore').strip()
                rs = attr["RS"].decode('utf-8', 'ignore').strip()
                
                # Get station-elevation profile
                se_start, se_count = sta_elev_info[idx]
                sta_elev_pairs = []
                if se_count > 0:
                    se_data = sta_elev_values[se_start:se_start + se_count]
                    sta_elev_pairs = [(float(s), float(e)) for s, e in se_data]
                
                # Get Manning's n profile
                mn_start, mn_count = mannings_info[idx]
                mannings_pairs = []
                if mn_count > 0:
                    mn_data = mannings_values[mn_start:mn_start + mn_count]
                    mannings_pairs = [(float(s), float(n)) for s, n in mn_data]
                
                valid_data.append({
                    'xs_id': int(idx),
                    'River': river,
                    'Reach': reach,
                    'RS': rs,
                    'LeftBank': float(attr["Left Bank"]),
                    'RightBank': float(attr["Right Bank"]),
                    'LenLeft': float(attr["Len Left"]),
                    'LenChannel': float(attr["Len Channel"]),
                    'LenRight': float(attr["Len Right"]),
                    'StationElevation': str(sta_elev_pairs)[:255],  # Convert to string for field storage
                    'ManningsN': str(mannings_pairs)[:255]
                })
                geometries.append(geom)
            
            return valid_data, geometries
            
        except Exception as e:
            arcpy.AddWarning(f"Error reading cross sections: {e}")
            return [], []

    def _get_river_centerlines_direct(self, hdf_file, sr):
        """Extracts river centerlines from HDF file."""
        try:
            centerlines_path = "Geometry/River Centerlines"
            if centerlines_path not in hdf_file:
                return [], []
            
            # Get attributes
            attributes = hdf_file[f"{centerlines_path}/Attributes"][()]
            
            # Get polyline geometry
            polyline_info = hdf_file[f"{centerlines_path}/Polyline Info"][()]
            polyline_points = hdf_file[f"{centerlines_path}/Polyline Points"][()]
            
            valid_data, geometries = [], []
            
            for idx, attr in enumerate(attributes):
                # Get polyline info
                pnt_start, pnt_cnt, _, _ = polyline_info[idx]
                
                if pnt_cnt < 2:
                    continue
                
                # Extract points and create polyline
                points = polyline_points[pnt_start:pnt_start + pnt_cnt]
                arcpy_array = arcpy.Array([arcpy.Point(p[0], p[1]) for p in points])
                geom = arcpy.Polyline(arcpy_array, sr)
                
                # Extract attributes
                river_name = attr["River Name"].decode('utf-8', 'ignore').strip()
                reach_name = attr["Reach Name"].decode('utf-8', 'ignore').strip()
                
                valid_data.append({
                    'river_id': int(idx),
                    'RiverName': river_name,
                    'ReachName': reach_name,
                    'USType': attr["US Type"].decode('utf-8', 'ignore').strip(),
                    'DSType': attr["DS Type"].decode('utf-8', 'ignore').strip()
                })
                geometries.append(geom)
            
            return valid_data, geometries
            
        except Exception as e:
            arcpy.AddError(f"HDF Read Error (River Centerlines): {e}")
            raise arcpy.ExecuteError("Failed to read river centerlines from HDF file")

    def _get_bank_lines_direct(self, hdf_file, sr):
        """Extracts bank lines from HDF file."""
        try:
            bank_lines_path = "Geometry/River Bank Lines"
            if bank_lines_path not in hdf_file:
                arcpy.AddMessage("No bank lines found in HDF file.")
                return [], []
            
            # Get polyline geometry
            polyline_info = hdf_file[f"{bank_lines_path}/Polyline Info"][()]
            polyline_points = hdf_file[f"{bank_lines_path}/Polyline Points"][()]
            
            valid_data, geometries = [], []
            
            # Bank lines typically come in pairs (left and right)
            bank_sides = ['Left', 'Right']
            
            for idx in range(len(polyline_info)):
                # Get polyline info
                pnt_start, pnt_cnt, _, _ = polyline_info[idx]
                
                if pnt_cnt < 2:
                    continue
                
                # Extract points and create polyline
                points = polyline_points[pnt_start:pnt_start + pnt_cnt]
                arcpy_array = arcpy.Array([arcpy.Point(p[0], p[1]) for p in points])
                geom = arcpy.Polyline(arcpy_array, sr)
                
                # Determine bank side
                bank_side = bank_sides[idx % 2] if idx < len(bank_sides) else f"Bank_{idx}"
                
                valid_data.append({
                    'bank_id': int(idx),
                    'BankSide': bank_side,
                    'Length': float(geom.length)
                })
                geometries.append(geom)
            
            return valid_data, geometries
            
        except Exception as e:
            arcpy.AddWarning(f"Error reading bank lines: {e}")
            return [], []

    def _get_edge_lines_direct(self, hdf_file, sr):
        """Extracts edge lines from HDF file."""
        try:
            edge_lines_path = "Geometry/River Edge Lines"
            if edge_lines_path not in hdf_file:
                arcpy.AddMessage("No edge lines found in HDF file.")
                return [], []
            
            # Get polyline geometry
            polyline_info = hdf_file[f"{edge_lines_path}/Polyline Info"][()]
            polyline_points = hdf_file[f"{edge_lines_path}/Polyline Points"][()]
            
            valid_data, geometries = [], []
            
            for idx in range(len(polyline_info)):
                # Get polyline info
                pnt_start, pnt_cnt, _, _ = polyline_info[idx]
                
                if pnt_cnt < 2:
                    continue
                
                # Extract points and create polyline
                points = polyline_points[pnt_start:pnt_start + pnt_cnt]
                arcpy_array = arcpy.Array([arcpy.Point(p[0], p[1]) for p in points])
                geom = arcpy.Polyline(arcpy_array, sr)
                
                valid_data.append({
                    'edge_id': int(idx),
                    'EdgeType': f"Edge_{idx}",
                    'Length': float(geom.length)
                })
                geometries.append(geom)
            
            return valid_data, geometries
            
        except Exception as e:
            arcpy.AddWarning(f"Error reading edge lines: {e}")
            return [], []

    def _get_structures_direct(self, hdf_file, sr):
        """Extracts hydraulic structures from HDF file."""
        try:
            structures_path = "Geometry/Structures"
            if structures_path not in hdf_file:
                arcpy.AddMessage("No hydraulic structures found in HDF file.")
                return [], []
            
            # Get attributes
            attributes = hdf_file[f"{structures_path}/Attributes"][()]
            
            # Get centerline geometry
            centerline_info = hdf_file[f"{structures_path}/Centerline Info"][()]
            centerline_points = hdf_file[f"{structures_path}/Centerline Points"][()]
            
            valid_data, geometries = [], []
            
            for idx, attr in enumerate(attributes):
                # Get centerline info
                pnt_start, pnt_cnt, _, _ = centerline_info[idx]
                
                if pnt_cnt < 2:
                    continue
                
                # Extract points and create polyline
                points = centerline_points[pnt_start:pnt_start + pnt_cnt]
                arcpy_array = arcpy.Array([arcpy.Point(p[0], p[1]) for p in points])
                geom = arcpy.Polyline(arcpy_array, sr)
                
                # Extract attributes
                struct_type = attr["Type"].decode('utf-8', 'ignore').strip()
                river = attr["River"].decode('utf-8', 'ignore').strip()
                reach = attr["Reach"].decode('utf-8', 'ignore').strip()
                rs = attr["RS"].decode('utf-8', 'ignore').strip()
                
                valid_data.append({
                    'struct_id': int(idx),
                    'Type': struct_type,
                    'River': river,
                    'Reach': reach,
                    'RS': rs,
                    'Description': attr["Description"].decode('utf-8', 'ignore').strip()[:255]
                })
                geometries.append(geom)
            
            return valid_data, geometries
            
        except Exception as e:
            arcpy.AddWarning(f"Error reading 1D structures: {e}")
            return [], []

    # --- Main Execution Logic ---
    def execute(self, parameters, messages):
        hdf_path = parameters[0].valueAsText
        
        # Get selected elements
        geometry_elements = parameters[2].values if parameters[2].values else []
        
        if not geometry_elements:
            messages.addErrorMessage("No geometry elements selected for loading. Please select at least one element.")
            raise arcpy.ExecuteError
        
        # Get geodatabase parameters
        output_gdb = parameters[8].valueAsText
        create_gdb = parameters[9].value
        output_workspace = None
        
        # Extract project and plan info
        project_name, plan_number, base_name = extract_project_and_plan_info(hdf_path)
        
        # Get projection
        proj_wkt = get_ras_projection_wkt(hdf_path)
        sr = None
        if proj_wkt:
            sr = arcpy.SpatialReference()
            sr.loadFromString(proj_wkt)
            messages.addMessage(f"CRS '{sr.name}' found in HEC-RAS project files.")
        elif parameters[1].value:
            sr = parameters[1].value
            messages.addMessage(f"Using user-defined override CRS: {sr.name}")
        else:
            messages.addErrorMessage("CRS could not be determined. Please use the Override CRS parameter.")
            raise arcpy.ExecuteError
        
        # Setup geodatabase
        if create_gdb or output_gdb:
            if create_gdb and not output_gdb:
                # Auto-create geodatabase based on HDF name
                output_gdb = create_geodatabase_from_hdf(hdf_path, messages)
            
            # Create feature dataset with project/plan naming
            feature_dataset_name = get_feature_dataset_name(hdf_path)
            output_workspace = setup_geodatabase_output(output_gdb, feature_dataset_name, sr, messages)
            messages.addMessage(f"Output workspace set to: {output_workspace}")
        
        # Open HDF file once
        with h5py.File(hdf_path, 'r') as hdf_file:
            messages.addMessage("Reading HDF file structure...")
            
            # Process geometry elements
            if self.CROSS_SECTIONS in geometry_elements and parameters[3].valueAsText:
                output_fc = parameters[3].valueAsText
                # Update output path if using geodatabase
                if output_workspace:
                    fc_name = "CrossSections"
                    output_fc = os.path.join(output_workspace, fc_name)
                    parameters[3].value = output_fc
                
                messages.addMessage("Extracting Cross Sections...")
                data, geoms = self._get_cross_sections_direct(hdf_file, sr)
                fields = [("xs_id", "LONG"), ("River", "TEXT"), ("Reach", "TEXT"), 
                         ("RS", "TEXT"), ("LeftBank", "DOUBLE"), ("RightBank", "DOUBLE"),
                         ("LenLeft", "DOUBLE"), ("LenChannel", "DOUBLE"), ("LenRight", "DOUBLE"),
                         ("StationElevation", "TEXT", 255), ("ManningsN", "TEXT", 255)]
                write_features_to_fc(output_fc, sr, "POLYLINE", fields, data, geoms, messages)
                if output_workspace and data:
                    add_feature_class_metadata(output_fc, "River cross section cut lines with station-elevation data", hdf_path)
            
            if self.RIVER_CENTERLINES in geometry_elements and parameters[4].valueAsText:
                output_fc = parameters[4].valueAsText
                # Update output path if using geodatabase
                if output_workspace:
                    fc_name = "RiverCenterlines"
                    output_fc = os.path.join(output_workspace, fc_name)
                    parameters[4].value = output_fc
                
                messages.addMessage("Extracting River Centerlines...")
                data, geoms = self._get_river_centerlines_direct(hdf_file, sr)
                fields = [("river_id", "LONG"), ("RiverName", "TEXT"), ("ReachName", "TEXT"),
                         ("USType", "TEXT"), ("DSType", "TEXT")]
                write_features_to_fc(output_fc, sr, "POLYLINE", fields, data, geoms, messages)
                if output_workspace and data:
                    add_feature_class_metadata(output_fc, "Main river/reach centerlines", hdf_path)
            
            if self.BANK_LINES in geometry_elements and parameters[5].valueAsText:
                output_fc = parameters[5].valueAsText
                # Update output path if using geodatabase
                if output_workspace:
                    fc_name = "BankLines"
                    output_fc = os.path.join(output_workspace, fc_name)
                    parameters[5].value = output_fc
                
                messages.addMessage("Extracting Bank Lines...")
                data, geoms = self._get_bank_lines_direct(hdf_file, sr)
                fields = [("bank_id", "LONG"), ("BankSide", "TEXT"), ("Length", "DOUBLE")]
                write_features_to_fc(output_fc, sr, "POLYLINE", fields, data, geoms, messages)
                if output_workspace and data:
                    add_feature_class_metadata(output_fc, "Left and right bank lines", hdf_path)
            
            if self.EDGE_LINES in geometry_elements and parameters[6].valueAsText:
                output_fc = parameters[6].valueAsText
                # Update output path if using geodatabase
                if output_workspace:
                    fc_name = "EdgeLines"
                    output_fc = os.path.join(output_workspace, fc_name)
                    parameters[6].value = output_fc
                
                messages.addMessage("Extracting Edge Lines...")
                data, geoms = self._get_edge_lines_direct(hdf_file, sr)
                fields = [("edge_id", "LONG"), ("EdgeType", "TEXT"), ("Length", "DOUBLE")]
                write_features_to_fc(output_fc, sr, "POLYLINE", fields, data, geoms, messages)
                if output_workspace and data:
                    add_feature_class_metadata(output_fc, "River edge lines for terrain processing", hdf_path)
            
            if self.STRUCTURES in geometry_elements and parameters[7].valueAsText:
                output_fc = parameters[7].valueAsText
                # Update output path if using geodatabase
                if output_workspace:
                    fc_name = "Structures1D"
                    output_fc = os.path.join(output_workspace, fc_name)
                    parameters[7].value = output_fc
                
                messages.addMessage("Extracting 1D Structures...")
                data, geoms = self._get_structures_direct(hdf_file, sr)
                fields = [("struct_id", "LONG"), ("Type", "TEXT"), ("River", "TEXT"),
                         ("Reach", "TEXT"), ("RS", "TEXT"), ("Description", "TEXT", 255)]
                write_features_to_fc(output_fc, sr, "POLYLINE", fields, data, geoms, messages)
                if output_workspace and data:
                    add_feature_class_metadata(output_fc, "Bridges, culverts, weirs, and other 1D structures", hdf_path)
        
        messages.addMessage("\nProcessing complete.")
        return
    
    def getHelp(self, tool_name=None):
        """Return help documentation for the tool.
        
        This method is called when the user clicks the help button.
        It can return:
        - A URL (starting with http:// or https://)
        - A local file path (starting with file:///)
        - HTML content directly (for embedded help)
        """
        # Try local help file first
        help_file = os.path.join(
            os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 
            "Doc", "RASCommander_Help.html"
        )
        
        if os.path.exists(help_file):
            # Return local help file
            anchor = "#load-hec-ras-1d-geometry-layers"
            return f"file:///{help_file.replace(os.sep, '/')}{anchor}"
        else:
            # Fallback to online documentation
            return "https://github.com/gpt-cmdr/ras-commander-hydro#load-hec-ras-1d-geometry-layers"
    
    def getCodeSamples(self):
        """Provide code samples for using this tool programmatically."""
        return [
            {
                "title": "Basic 1D Geometry Import",
                "description": "Import cross sections and river centerlines to memory",
                "code": """import arcpy

# Set input parameters
hdf_file = r"C:\\RAS_Projects\\MyProject\\MyProject.g01.hdf"
geometry_elements = ["Cross Sections", "River Centerlines"]

# Run the tool
result = arcpy.RASCommander.LoadHECRAS1DGeometry(
    input_hdf=hdf_file,
    geometry_elements=geometry_elements,
    output_cross_sections=r"memory\\CrossSections",
    output_centerlines=r"memory\\RiverCenterlines"
)

print("1D geometry loaded successfully!")
print(f"Cross sections: {result[0]}")
print(f"Centerlines: {result[1]}")"""
            },
            {
                "title": "Organize to Geodatabase",
                "description": "Extract all 1D geometry to an organized geodatabase",
                "code": """import arcpy
import os

# Input HDF file
hdf_file = r"C:\\RAS_Projects\\MyProject\\MyProject.p01.hdf"

# Create geodatabase path
gdb_path = os.path.join(os.path.dirname(hdf_file), "MyProject.p01.gdb")

# Extract all 1D geometry elements
arcpy.RASCommander.LoadHECRAS1DGeometry(
    input_hdf=hdf_file,
    geometry_elements=["Cross Sections", "River Centerlines", "Bank Lines", "1D Structures"],
    output_gdb=gdb_path,
    create_gdb=True
)

print(f"1D geometry organized in: {gdb_path}")"""
            },
            {
                "title": "With Custom Projection",
                "description": "Load geometry with a specific coordinate system",
                "code": """import arcpy

# Define custom spatial reference
sr = arcpy.SpatialReference(26915)  # NAD83 UTM Zone 15N

# Run tool with override CRS
arcpy.RASCommander.LoadHECRAS1DGeometry(
    input_hdf=r"C:\\RAS_Projects\\MyProject.g01.hdf",
    override_crs=sr,
    geometry_elements=["Cross Sections", "River Centerlines"],
    create_gdb=True
)"""
            }
        ]
==================================================

File: c:\GH\ras-commander-hydro\Scripts\archydro\rc_load_hecras_2d_geometry.py
==================================================
# -*- coding: utf-8 -*-
"""
LoadHECRAS2DGeometry.py

Tool for loading HEC-RAS 2D geometry layers from HDF files including mesh elements,
breaklines, boundary conditions, and pipe networks.
"""

import arcpy
import os
import h5py
import numpy as np
from collections import defaultdict

# Import helper functions from utils
from rc_utils import (
    get_ras_projection_wkt,
    polygonize_arcpy_optimized,
    get_polyline_centroid_vectorized,
    cache_hdf_metadata,
    write_features_to_fc,
    get_dynamic_fields_from_data,
    setup_geodatabase_output,
    get_unique_fc_name,
    add_feature_class_metadata,
    extract_project_and_plan_info,
    create_geodatabase_from_hdf,
    get_feature_dataset_name,
    get_feature_class_name
)


class LoadHECRAS2DGeometry(object):
    """
    Loads 2D geometry elements from a HEC-RAS HDF file.
    """
    def __init__(self):
        # Core properties
        self.label = "Load HEC-RAS 2D Geometry Layers"
        self.description = """Extracts 2D geometry elements from a HEC-RAS HDF file including mesh elements, breaklines, boundary conditions, and pipe networks.
        
        This tool extracts various 2D geometry elements from HEC-RAS geometry (g*.hdf) or plan (p*.hdf) files.
        
        Available geometry elements include:
        • 2D Breaklines - Mesh refinement lines with cell spacing attributes
        • 2D Boundary Condition Lines - External and internal boundary conditions
        • Mesh Area Perimeters - 2D flow area boundaries
        • Mesh Cell Centers - Point locations at the center of each mesh cell
        • Mesh Cell Faces - Line geometries representing cell edges
        • Mesh Cells (Polygons) - Full polygon representation of mesh cells
        • Pipe Conduits - Storm/sewer pipe networks (if present)
        • Pipe Nodes - Junction points in pipe networks (if present)
        
        Note: Mesh cell polygon creation can be time-consuming for large meshes (>100,000 cells)."""
        
        # Extended metadata properties
        self.summary = "Extract 2D mesh geometry and pipe networks from HEC-RAS HDF files"
        self.usage = """Select a HEC-RAS geometry or plan HDF file and choose which 2D geometry elements to extract.
        
        Steps:
        1. Browse to a HEC-RAS geometry (g*.hdf) or plan (p*.hdf) file
        2. Select which 2D geometry elements to extract
        3. Specify output locations for each selected element
        4. Optionally create an organized geodatabase
        
        Performance considerations:
        • Mesh polygon creation can be slow for meshes > 100,000 cells
        • Consider extracting only cell centers/faces for large meshes
        • Use geodatabase output for better performance with large datasets"""
        
        # Tool behavior
        self.canRunInBackground = False
        # self.category = "HEC-RAS Data Import"  # REMOVE THIS LINE
        
        # Documentation and credits
        self.tags = ["HEC-RAS", "2D Geometry", "Mesh", "Breaklines", "Boundary Conditions", 
                     "Pipe Networks", "Storm Sewer", "Arc Hydro"]
        self.credits = "CLB Engineering Corporation"
        self.author = "CLB Engineering Corporation"
        self.version = "1.0.0"
        
        # Geometry elements
        self.BREAKLINES = "2D Breaklines"
        self.BC_LINES = "2D Boundary Condition Lines"
        self.PERIMETERS = "Mesh Area Perimeters"
        self.CELL_POINTS = "Mesh Cell Centers"
        self.CELL_FACES = "Mesh Cell Faces"
        self.CELL_POLYS = "Mesh Cells (Polygons)"
        
        # Pipe network elements
        self.PIPE_CONDUITS = "Pipe Conduits"
        self.PIPE_NODES = "Pipe Nodes"
        self.PIPE_NETWORKS = "Pipe Networks"
        
        # Cache for HDF metadata
        self._hdf_cache = {}

    def getParameterInfo(self):
        geometry_elements = [self.BREAKLINES, self.BC_LINES, self.PERIMETERS, self.CELL_POINTS, 
                           self.CELL_FACES, self.CELL_POLYS, self.PIPE_CONDUITS, self.PIPE_NODES, self.PIPE_NETWORKS]

        params = [
            arcpy.Parameter(displayName="Geometry or Plan HDF File", name="input_hdf", datatype="DEFile", 
                          parameterType="Required", direction="Input"),
            arcpy.Parameter(displayName="Override CRS (Optional)", name="override_crs", datatype="GPSpatialReference", 
                          parameterType="Optional", direction="Input"),
            
            # Geometry elements to load
            arcpy.Parameter(displayName="Geometry Elements to Load", name="geometry_elements", datatype="GPString", 
                          parameterType="Required", direction="Input", multiValue=True),
            
            # Output parameters
            arcpy.Parameter(displayName="Output 2D Breaklines", name="output_breaklines", datatype="DEFeatureClass", 
                          parameterType="Optional", direction="Output"),
            arcpy.Parameter(displayName="Output 2D Boundary Condition Lines", name="output_bc_lines", datatype="DEFeatureClass", 
                          parameterType="Optional", direction="Output"),
            arcpy.Parameter(displayName="Output Mesh Area Perimeters", name="output_perimeters", datatype="DEFeatureClass", 
                          parameterType="Optional", direction="Output"),
            arcpy.Parameter(displayName="Output Mesh Cell Centers", name="output_cell_points", datatype="DEFeatureClass", 
                          parameterType="Optional", direction="Output"),
            arcpy.Parameter(displayName="Output Mesh Cell Faces", name="output_cell_faces", datatype="DEFeatureClass", 
                          parameterType="Optional", direction="Output"),
            arcpy.Parameter(displayName="Output Mesh Cells (Polygons)", name="output_cell_polys", datatype="DEFeatureClass", 
                          parameterType="Optional", direction="Output"),
            arcpy.Parameter(displayName="Output Pipe Conduits", name="output_pipe_conduits", datatype="DEFeatureClass", 
                          parameterType="Optional", direction="Output"),
            arcpy.Parameter(displayName="Output Pipe Nodes", name="output_pipe_nodes", datatype="DEFeatureClass", 
                          parameterType="Optional", direction="Output"),
            arcpy.Parameter(displayName="Output Pipe Networks", name="output_pipe_networks", datatype="DEFeatureClass", 
                          parameterType="Optional", direction="Output"),
            
            # Geodatabase organization parameters
            arcpy.Parameter(displayName="Output Geodatabase (Optional)", name="output_gdb", datatype="DEWorkspace", 
                          parameterType="Optional", direction="Output"),
            arcpy.Parameter(displayName="Create New Geodatabase", name="create_gdb", datatype="GPBoolean", 
                          parameterType="Optional", direction="Input")
        ]
        
        # Configure HDF file filter
        params[0].filter.list = ["hdf", "g*.hdf", "p*.hdf"]
        params[0].description = """Select a HEC-RAS geometry file (g*.hdf) or plan file (p*.hdf) containing 2D geometry data.
        
        The tool will automatically detect available 2D flow areas and pipe networks in the file.
        
        Supported file types:
        • Geometry files (g01.hdf, g02.hdf, etc.)
        • Plan files with geometry (p01.hdf, p02.hdf, etc.)"""
        # params[0].category = "Input Data"  # Remove category grouping
        
        params[1].description = """Specify a coordinate reference system if it cannot be determined from the HEC-RAS project files.
        
        The tool will first attempt to read the CRS from:
        1. The HDF file metadata
        2. Associated .prj files in the project directory
        3. The RAS Mapper projection file
        
        Only provide this parameter if automatic detection fails."""
        # params[1].category = "Input Data"  # Remove category grouping
        
        # Set filters for multi-value parameters
        params[2].filter.type = "ValueList"
        params[2].filter.list = geometry_elements
        params[2].value = [self.PERIMETERS]  # Default selection
        params[2].description = """Select one or more geometry elements to extract from the HDF file.
        
        Mesh Elements:
        • 2D Breaklines - Enforce mesh refinement along important features
        • 2D Boundary Condition Lines - Define inflow/outflow boundaries
        • Mesh Area Perimeters - 2D flow area boundaries
        • Mesh Cell Centers - Point at center of each computational cell
        • Mesh Cell Faces - Lines representing cell edges
        • Mesh Cells (Polygons) - Full polygon cells (slow for large meshes)
        
        Pipe Networks:
        • Pipe Conduits - Storm/sewer pipe segments
        • Pipe Nodes - Manholes and junctions
        • Pipe Networks - Complete network elements
        
        Each selected element creates a separate output feature class."""
        # params[2].category = "Geometry Selection"  # Remove category grouping
        
        # Set default output paths and descriptions
        params[3].value = r"memory\Breaklines"
        params[3].description = """Output feature class for 2D breaklines.
        
        Attributes include:
        • Name and type
        • Cell spacing along breakline
        • 2D flow area association"""
        
        params[4].value = r"memory\BoundaryConditionLines"
        params[4].description = """Output feature class for 2D boundary condition lines.
        
        Attributes include:
        • BC type (Flow, Stage, Normal Depth, etc.)
        • BC name
        • 2D flow area association"""
        
        params[5].value = r"memory\MeshPerimeters"
        params[5].description = """Output feature class for 2D flow area perimeter polygons.
        
        Attributes include:
        • 2D area name
        • Cell count
        • Minimum cell size
        • Area in acres"""
        
        params[6].value = r"memory\MeshCellCenters"
        params[6].description = """Output feature class for mesh cell center points.
        
        Attributes include:
        • Cell ID
        • 2D flow area name
        • Cell area
        • Elevation (if available)"""
        
        params[7].value = r"memory\MeshCellFaces"
        params[7].description = """Output feature class for mesh cell face polylines.
        
        Represents the edges between computational cells.
        Useful for understanding mesh connectivity."""
        
        params[8].value = r"memory\MeshCellPolygons"
        params[8].description = """Output feature class for mesh cell polygons.
        
        WARNING: Polygon creation can be very slow for large meshes (>100,000 cells).
        Consider using cell centers and faces for large models.
        
        Attributes include:
        • Cell ID
        • Cell area
        • 2D flow area name"""
        
        params[9].value = r"memory\PipeConduits"
        params[9].description = """Output feature class for pipe conduits.
        
        Storm/sewer pipe segments with attributes:
        • Pipe name and material
        • Diameter/dimensions
        • Upstream/downstream nodes
        • Length and slope"""
        
        params[10].value = r"memory\PipeNodes"
        params[10].description = """Output feature class for pipe junction nodes.
        
        Manholes and junctions with attributes:
        • Node name
        • Rim elevation
        • Invert elevation
        • Node type"""
        
        params[11].value = r"memory\PipeNetworks"
        params[11].description = """Output feature class for complete pipe network elements.
        
        Combined pipe network geometry including both conduits and nodes."""
        
        # Geodatabase parameters
        params[12].description = """Specify a geodatabase to organize all output feature classes.
        
        If provided:
        • All outputs will be created in feature datasets within this geodatabase
        • Feature datasets will be organized by geometry type (Mesh, Pipes, etc.)
        • Automatic naming conventions will be applied
        
        Leave empty to use default output locations."""
        # params[12].category = "Output Organization"  # Remove category grouping
        
        params[13].value = True  # Default to creating new geodatabase
        params[13].description = """Create a new geodatabase based on the HDF file name.
        
        When enabled:
        • Creates geodatabase named: ProjectName.pXX.gdb
        • Organizes outputs in feature datasets by type
        • Maintains HEC-RAS project structure
        • Optimizes performance for large datasets
        
        Recommended for organizing complete 2D models."""
        # params[13].category = "Output Organization"  # Remove category grouping
        
        return params

    def isLicensed(self):
        """Set whether tool is licensed to execute."""
        return True

    def updateParameters(self, parameters):
        """Modify the values and properties of parameters before internal validation."""
        # Enable/disable output parameters based on selected elements
        if parameters[2].value:
            selected = parameters[2].valueAsText.split(';') if parameters[2].valueAsText else []
            
            # Enable/disable outputs based on selection
            parameters[3].enabled = self.BREAKLINES in selected
            parameters[4].enabled = self.BC_LINES in selected
            parameters[5].enabled = self.PERIMETERS in selected
            parameters[6].enabled = self.CELL_POINTS in selected
            parameters[7].enabled = self.CELL_FACES in selected
            parameters[8].enabled = self.CELL_POLYS in selected
            parameters[9].enabled = self.PIPE_CONDUITS in selected
            parameters[10].enabled = self.PIPE_NODES in selected
            parameters[11].enabled = self.PIPE_NETWORKS in selected
            
        # Auto-populate geodatabase path when HDF file is selected
        if parameters[0].value and parameters[0].altered:  # input_hdf
            hdf_path = parameters[0].valueAsText
            
            # If create_gdb is True, auto-populate geodatabase path
            if parameters[13].value:  # create_gdb
                project_name, plan_number, base_name = extract_project_and_plan_info(hdf_path)
                gdb_name = f"{base_name}.gdb"
                gdb_path = os.path.join(os.path.dirname(hdf_path), gdb_name)
                parameters[12].value = gdb_path
        return
    
    def updateMessages(self, parameters):
        """Modify messages created by internal validation."""
        # Add warning for large mesh polygon creation
        if parameters[2].value and self.CELL_POLYS in str(parameters[2].value):
            parameters[8].setWarningMessage(
                "Creating cell polygons can be time-consuming for large meshes (>100,000 cells). "
                "Consider using cell centers or faces for visualization instead."
            )
        
        # Clear geodatabase validation error if create_gdb is True
        if parameters[13].value and parameters[12].hasError():  # create_gdb and output_gdb has error
            parameters[12].clearMessage()
        
        return

    # --- HDF Data Extraction Methods ---

    def _get_breaklines_direct(self, hdf_file, sr):
        """Extracts 2D breaklines from HDF file with optimized numpy operations."""
        try:
            breaklines_path = "Geometry/2D Flow Area Break Lines"
            if breaklines_path not in hdf_file:
                return [], []
            
            bl_line_data = hdf_file[breaklines_path]
            attributes = bl_line_data["Attributes"][()]
            polyline_info = bl_line_data["Polyline Info"][()]
            polyline_points = bl_line_data["Polyline Points"][()]
            
            # Vectorized filtering of valid breaklines
            valid_mask = polyline_info[:, 1] >= 2  # pnt_cnt >= 2
            valid_indices = np.where(valid_mask)[0]
            
            valid_data, geometries = [], []
            
            for idx in valid_indices:
                pnt_start, pnt_cnt, part_start, part_cnt = polyline_info[idx]
                attr_row = attributes[idx]
                
                name = attr_row["Name"]
                name = name.decode('utf-8', 'ignore').strip() if isinstance(name, bytes) else str(name)
                
                try:
                    # Extract points efficiently
                    points = polyline_points[pnt_start:pnt_start + pnt_cnt]
                    
                    if part_cnt == 1:
                        # Single part - direct creation
                        arcpy_array = arcpy.Array([arcpy.Point(p[0], p[1]) for p in points])
                        geom = arcpy.Polyline(arcpy_array, sr)
                    else:
                        # Multi-part polyline
                        parts = bl_line_data["Polyline Parts"][()][part_start:part_start + part_cnt]
                        all_parts_array = arcpy.Array()
                        
                        for part_pnt_start, part_pnt_cnt in parts:
                            if part_pnt_cnt > 1:
                                part_points = points[part_pnt_start:part_pnt_start + part_pnt_cnt]
                                part_array = arcpy.Array([arcpy.Point(p[0], p[1]) for p in part_points])
                                all_parts_array.add(part_array)
                        
                        if all_parts_array.count == 0:
                            continue
                        geom = arcpy.Polyline(all_parts_array, sr)
                    
                    valid_data.append({
                        'bl_id': int(idx),
                        'Name': name,
                        'CellSpaceNear': float(attr_row["Cell Spacing Near"]),
                        'CellSpaceFar': float(attr_row["Cell Spacing Far"]),
                        'NearRepeats': int(attr_row["Near Repeats"]),
                        'ProtectRadius': int(attr_row["Protection Radius"])
                    })
                    geometries.append(geom)
                    
                except Exception as e:
                    arcpy.AddWarning(f"Error processing breakline {idx}: {str(e)}")
                    continue
            
            return valid_data, geometries
            
        except Exception as e:
            arcpy.AddError(f"HDF Read Error (Breaklines): {e}")
            raise arcpy.ExecuteError("Failed to read breaklines from HDF file")

    def _get_bc_lines_direct(self, hdf_file, sr):
        """Extracts 2D boundary condition lines from HDF file."""
        try:
            bc_lines_path = "Geometry/Boundary Condition Lines"
            if bc_lines_path not in hdf_file:
                return [], []
            
            # Get boundary condition line data
            bc_attrs = hdf_file[f"{bc_lines_path}/Attributes"][()]
            polyline_info = hdf_file[f"{bc_lines_path}/Polyline Info"][()]
            polyline_points = hdf_file[f"{bc_lines_path}/Polyline Points"][()]
            
            # Check if multi-part data exists
            has_parts = f"{bc_lines_path}/Polyline Parts" in hdf_file
            if has_parts:
                polyline_parts = hdf_file[f"{bc_lines_path}/Polyline Parts"][()]
            
            # Vectorized filtering of valid boundary condition lines
            valid_mask = polyline_info[:, 1] >= 2  # pnt_cnt >= 2
            valid_indices = np.where(valid_mask)[0]
            
            valid_data, geometries = [], []
            
            for idx in valid_indices:
                pnt_start, pnt_cnt, part_start, part_cnt = polyline_info[idx]
                attr_row = bc_attrs[idx]
                
                # Extract attributes
                name = attr_row["Name"]
                name = name.decode('utf-8', 'ignore').strip() if isinstance(name, bytes) else str(name)
                
                bc_type = attr_row["Type"]
                bc_type = bc_type.decode('utf-8', 'ignore').strip() if isinstance(bc_type, bytes) else str(bc_type)
                
                try:
                    # Extract points efficiently
                    points = polyline_points[pnt_start:pnt_start + pnt_cnt]
                    
                    if part_cnt == 1 or not has_parts:
                        # Single part - direct creation
                        arcpy_array = arcpy.Array([arcpy.Point(p[0], p[1]) for p in points])
                        geom = arcpy.Polyline(arcpy_array, sr)
                    else:
                        # Multi-part polyline
                        parts = polyline_parts[part_start:part_start + part_cnt]
                        all_parts_array = arcpy.Array()
                        
                        for part_pnt_start, part_pnt_cnt in parts:
                            if part_pnt_cnt > 1:
                                part_points = points[part_pnt_start:part_pnt_start + part_pnt_cnt]
                                part_array = arcpy.Array([arcpy.Point(p[0], p[1]) for p in part_points])
                                all_parts_array.add(part_array)
                        
                        if all_parts_array.count == 0:
                            continue
                        geom = arcpy.Polyline(all_parts_array, sr)
                    
                    valid_data.append({
                        'bc_id': int(idx),
                        'Name': name,
                        'Type': bc_type
                    })
                    geometries.append(geom)
                    
                except Exception as e:
                    arcpy.AddWarning(f"Error processing boundary condition line {idx}: {str(e)}")
                    continue
            
            return valid_data, geometries
            
        except Exception as e:
            arcpy.AddError(f"HDF Read Error (Boundary Condition Lines): {e}")
            raise arcpy.ExecuteError("Failed to read boundary condition lines from HDF file")

    def _get_pipe_conduits_direct(self, hdf_file, sr):
        """Extracts pipe conduits from HDF file with dynamic attributes."""
        try:
            conduits_path = "Geometry/Pipe Conduits"
            if conduits_path not in hdf_file:
                return [], []
            
            conduits_group = hdf_file[conduits_path]
            
            # Get attributes
            if 'Attributes' not in conduits_group:
                return [], []
            
            attributes = conduits_group['Attributes'][()]
            
            # Get polyline geometry data
            if 'Polyline Info' not in conduits_group or 'Polyline Points' not in conduits_group:
                return [], []
            
            polyline_info = conduits_group['Polyline Info'][()]
            polyline_points = conduits_group['Polyline Points'][()]
            
            valid_data, geometries = [], []
            
            # Debug: Show original field names
            if len(attributes) > 0:
                arcpy.AddMessage(f"DEBUG: Original HDF field names: {list(attributes.dtype.names)}")
            
            # Process each conduit
            for idx, (info, attr_row) in enumerate(zip(polyline_info, attributes)):
                point_start_idx, point_count = info[0], info[1]
                
                if point_count < 2:
                    continue
                
                try:
                    # Extract points for this conduit
                    coords = polyline_points[point_start_idx:point_start_idx + point_count]
                    
                    # Create polyline geometry
                    arcpy_array = arcpy.Array([arcpy.Point(p[0], p[1]) for p in coords])
                    geom = arcpy.Polyline(arcpy_array, sr)
                    
                    # Build attribute dictionary dynamically
                    attr_dict = {'conduit_id': int(idx)}
                    
                    # Process all attribute fields
                    attr_names = attributes.dtype.names
                    for field_name in attr_names:
                        value = attr_row[field_name]
                        
                        # Decode bytes to string if necessary
                        if isinstance(value, (bytes, np.bytes_)):
                            value = value.decode('utf-8', 'ignore').strip()
                        elif isinstance(value, np.ndarray) and value.dtype.kind == 'S':
                            value = value.tobytes().decode('utf-8', 'ignore').strip()
                        
                        # Clean field name for ArcGIS compatibility
                        clean_name = field_name.replace(' ', '_').replace(':', '_').replace(';', '_').replace(',', '_').replace('(', '_').replace(')', '_').replace("'", '')
                        
                        # Fix known typos in HDF field names
                        if 'Condtui_Connections' in clean_name:
                            clean_name = clean_name.replace('Condtui_Connections', 'Conduit_Connections')
                            if idx == 0:  # Only log for first record
                                arcpy.AddMessage(f"DEBUG: Fixed typo in field name 'Condtui_Connections' to 'Conduit_Connections'")
                        
                        # Special handling for exact "Shape" field (case insensitive)
                        if field_name.upper() == 'SHAPE':
                            clean_name = 'Shape_Type'
                            if idx == 0:
                                arcpy.AddMessage(f"DEBUG: Renamed 'Shape' field to 'Shape_Type' to avoid system field conflict")
                        # Rename other fields that conflict with system fields
                        elif clean_name.upper() in ['OBJECTID', 'SHAPE', 'SHAPE_LENGTH', 'SHAPE_AREA', 'SHAPE_LENG']:
                            original_clean = clean_name
                            clean_name = f"{clean_name}_USER"
                            if idx == 0:  # Only log for first record to avoid spam
                                arcpy.AddMessage(f"DEBUG: Renamed system field '{original_clean}' to '{clean_name}'")
                            
                        attr_dict[clean_name] = value
                    
                    valid_data.append(attr_dict)
                    geometries.append(geom)
                    
                except Exception as e:
                    arcpy.AddWarning(f"Error processing pipe conduit {idx}: {str(e)}")
                    continue
            
            # Debug: Show cleaned field names from first record
            if valid_data:
                arcpy.AddMessage(f"DEBUG: Cleaned field names: {list(valid_data[0].keys())}")
            
            return valid_data, geometries
            
        except Exception as e:
            arcpy.AddWarning(f"Could not read pipe conduits: {e}")
            return [], []

    def _get_pipe_nodes_direct(self, hdf_file, sr):
        """Extracts pipe nodes from HDF file with dynamic attributes."""
        try:
            nodes_path = "Geometry/Pipe Nodes"
            if nodes_path not in hdf_file:
                return [], []
            
            nodes_group = hdf_file[nodes_path]
            
            # Get attributes
            if 'Attributes' not in nodes_group:
                return [], []
            
            attributes = nodes_group['Attributes'][()]
            
            # Get points data
            if 'Points' not in nodes_group:
                return [], []
            
            points = nodes_group['Points'][()]
            
            valid_data, geometries = [], []
            
            # Debug: Show original field names
            if len(attributes) > 0:
                arcpy.AddMessage(f"DEBUG: Original HDF field names: {list(attributes.dtype.names)}")
            
            # Process each node
            for idx, (xy, attr_row) in enumerate(zip(points, attributes)):
                if len(xy) < 2:
                    continue
                
                try:
                    # Create point geometry
                    geom = arcpy.PointGeometry(arcpy.Point(xy[0], xy[1]), sr)
                    
                    # Build attribute dictionary dynamically
                    attr_dict = {'node_id': int(idx)}
                    
                    # Process all attribute fields
                    attr_names = attributes.dtype.names
                    for field_name in attr_names:
                        value = attr_row[field_name]
                        
                        # Decode bytes to string if necessary
                        if isinstance(value, (bytes, np.bytes_)):
                            value = value.decode('utf-8', 'ignore').strip()
                        elif isinstance(value, np.ndarray) and value.dtype.kind == 'S':
                            value = value.tobytes().decode('utf-8', 'ignore').strip()
                        
                        # Clean field name for ArcGIS compatibility
                        clean_name = field_name.replace(' ', '_').replace(':', '_').replace(';', '_').replace(',', '_').replace('(', '_').replace(')', '_').replace("'", '')
                        
                        # Fix known typos in HDF field names
                        if 'Condtui_Connections' in clean_name:
                            clean_name = clean_name.replace('Condtui_Connections', 'Conduit_Connections')
                            if idx == 0:  # Only log for first record
                                arcpy.AddMessage(f"DEBUG: Fixed typo in field name 'Condtui_Connections' to 'Conduit_Connections'")
                        
                        # Special handling for exact "Shape" field (case insensitive)
                        if field_name.upper() == 'SHAPE':
                            clean_name = 'Shape_Type'
                            if idx == 0:
                                arcpy.AddMessage(f"DEBUG: Renamed 'Shape' field to 'Shape_Type' to avoid system field conflict")
                        # Rename other fields that conflict with system fields
                        elif clean_name.upper() in ['OBJECTID', 'SHAPE', 'SHAPE_LENGTH', 'SHAPE_AREA', 'SHAPE_LENG']:
                            original_clean = clean_name
                            clean_name = f"{clean_name}_USER"
                            if idx == 0:  # Only log for first record to avoid spam
                                arcpy.AddMessage(f"DEBUG: Renamed system field '{original_clean}' to '{clean_name}'")
                            
                        attr_dict[clean_name] = value
                    
                    valid_data.append(attr_dict)
                    geometries.append(geom)
                    
                except Exception as e:
                    arcpy.AddWarning(f"Error processing pipe node {idx}: {str(e)}")
                    continue
            
            # Debug: Show cleaned field names from first record
            if valid_data:
                arcpy.AddMessage(f"DEBUG: Cleaned field names: {list(valid_data[0].keys())}")
            
            return valid_data, geometries
            
        except Exception as e:
            arcpy.AddWarning(f"Could not read pipe nodes: {e}")
            return [], []

    def _get_pipe_networks_direct(self, hdf_file, sr):
        """Extracts pipe network cell polygons from HDF file."""
        try:
            networks_path = "Geometry/Pipe Networks"
            if networks_path not in hdf_file:
                return [], []
            
            networks_group = hdf_file[networks_path]
            
            # Get network attributes
            if 'Attributes' not in networks_group:
                return [], []
            
            attributes = networks_group['Attributes'][()]
            if len(attributes) == 0:
                return [], []
            
            # Get the first network name (or could iterate through all)
            network_name = attributes[0]['Name']
            if isinstance(network_name, bytes):
                network_name = network_name.decode('utf-8', 'ignore').strip()
            
            arcpy.AddMessage(f"Processing pipe network: {network_name}")
            
            # Access the specific network group
            network_path = f"{networks_path}/{network_name}"
            if network_path not in hdf_file:
                arcpy.AddWarning(f"Network path '{network_path}' not found in HDF file")
                return [], []
            
            network_group = hdf_file[network_path]
            
            # Check for required datasets
            required_datasets = ['Cell Polygons Info', 'Cell Polygons Parts', 'Cell Polygons Points']
            for ds in required_datasets:
                if ds not in network_group:
                    arcpy.AddWarning(f"Required dataset '{ds}' not found in pipe network")
                    return [], []
            
            # Read cell polygon data
            cell_info = network_group['Cell Polygons Info'][()]
            cell_parts = network_group['Cell Polygons Parts'][()]
            cell_points = network_group['Cell Polygons Points'][()]
            
            # Read additional cell attributes if available
            cell_attributes = {}
            if 'Cell Property Table' in network_group:
                cell_property_table = network_group['Cell Property Table'][()]
                # Convert to dictionary for easier access
                for i, row in enumerate(cell_property_table):
                    cell_attributes[i] = {}
                    for field_name in cell_property_table.dtype.names:
                        value = row[field_name]
                        if isinstance(value, (bytes, np.bytes_)):
                            value = value.decode('utf-8', 'ignore').strip()
                        cell_attributes[i][field_name] = value
            
            # Read minimum elevations if available
            min_elevations = None
            if 'Cells Minimum Elevations' in network_group:
                min_elevations = network_group['Cells Minimum Elevations'][()]
            
            # Read node and conduit IDs if available
            node_conduit_ids = None
            if 'Cells Node and Conduit IDs' in network_group:
                node_conduit_ids = network_group['Cells Node and Conduit IDs'][()]
            
            valid_data, geometries = [], []
            
            # Process each cell
            for cell_idx, info in enumerate(cell_info):
                point_start_idx, point_count, part_start_idx, part_count = info
                
                try:
                    # Build polygon from parts
                    if part_count == 0:
                        continue
                    
                    parts_list = []
                    for p in range(part_start_idx, part_start_idx + part_count):
                        if p >= len(cell_parts):
                            continue
                        
                        part_info = cell_parts[p]
                        part_point_start = part_info[0]
                        part_point_count = part_info[1]
                        
                        # Extract coordinates for this part
                        coords = cell_points[part_point_start:part_point_start + part_point_count]
                        if len(coords) < 3:  # Need at least 3 points for a polygon
                            continue
                        
                        # Create arcpy array for this part
                        part_array = arcpy.Array([arcpy.Point(c[0], c[1]) for c in coords])
                        parts_list.append(part_array)
                    
                    if not parts_list:
                        continue
                    
                    # Create polygon geometry
                    if len(parts_list) == 1:
                        geom = arcpy.Polygon(parts_list[0], sr)
                    else:
                        # Multi-part polygon
                        all_parts = arcpy.Array()
                        for part in parts_list:
                            all_parts.add(part)
                        geom = arcpy.Polygon(all_parts, sr)
                    
                    # Build attribute dictionary
                    attr_dict = {
                        'cell_id': int(cell_idx),
                        'network_name': network_name
                    }
                    
                    # Add cell properties if available
                    if cell_idx in cell_attributes:
                        for key, value in cell_attributes[cell_idx].items():
                            # Clean field name
                            clean_key = key.replace(' ', '_').replace(':', '_').replace(';', '_').replace(',', '_')
                            attr_dict[clean_key] = value
                    
                    # Add minimum elevation if available
                    if min_elevations is not None and cell_idx < len(min_elevations):
                        attr_dict['min_elevation'] = float(min_elevations[cell_idx])
                    
                    # Add node and conduit IDs if available
                    if node_conduit_ids is not None and cell_idx < len(node_conduit_ids):
                        attr_dict['node_id'] = int(node_conduit_ids[cell_idx][0])
                        attr_dict['conduit_id'] = int(node_conduit_ids[cell_idx][1])
                    
                    valid_data.append(attr_dict)
                    geometries.append(geom)
                    
                except Exception as e:
                    arcpy.AddWarning(f"Error processing pipe network cell {cell_idx}: {str(e)}")
                    continue
            
            return valid_data, geometries
            
        except Exception as e:
            arcpy.AddWarning(f"Could not read pipe networks: {e}")
            return [], []

    def _get_mesh_areas_direct(self, hdf_file, sr):
        """Extracts mesh area perimeters from HDF file."""
        try:
            if not self._hdf_cache['mesh_names']:
                return [], []
            
            raw_data = [{'mesh_name': name} for name in self._hdf_cache['mesh_names']]
            geometries = []
            
            flow_areas_path = "Geometry/2D Flow Areas"
            for mesh_name in self._hdf_cache['mesh_names']:
                perimeter_path = f"{flow_areas_path}/{mesh_name}/Perimeter"
                if perimeter_path in hdf_file:
                    coords = hdf_file[perimeter_path][()]
                    # Create polygon directly from numpy array
                    arcpy_array = arcpy.Array([arcpy.Point(p[0], p[1]) for p in coords])
                    geometries.append(arcpy.Polygon(arcpy_array, sr))
                else:
                    geometries.append(None)
            
            return raw_data, geometries
            
        except Exception as e:
            arcpy.AddError(f"HDF Read Error (Perimeters): {e}")
            raise arcpy.ExecuteError()

    def _get_mesh_cell_points_direct(self, hdf_file, sr):
        """Extracts mesh cell centers using vectorized operations."""
        try:
            if not self._hdf_cache['mesh_names']:
                return [], []
            
            raw_data, geometries = [], []
            
            for mesh_name in self._hdf_cache['mesh_names']:
                cell_centers_path = f"Geometry/2D Flow Areas/{mesh_name}/Cells Center Coordinate"
                if cell_centers_path not in hdf_file:
                    arcpy.AddWarning(f"No cell center data found for mesh '{mesh_name}'")
                    continue
                
                # Read all cell centers at once
                cell_centers = hdf_file[cell_centers_path][()]
                num_cells = len(cell_centers)
                
                # Vectorized data creation
                mesh_data = [{'mesh_name': mesh_name, 'cell_id': i} for i in range(num_cells)]
                raw_data.extend(mesh_data)
                
                # Batch create point geometries
                mesh_geometries = [arcpy.PointGeometry(arcpy.Point(coords[0], coords[1]), sr) 
                                 for coords in cell_centers]
                geometries.extend(mesh_geometries)
            
            return raw_data, geometries
            
        except Exception as e:
            arcpy.AddError(f"HDF Read Error (Cell Points): {e}")
            raise arcpy.ExecuteError()

    def _get_mesh_cell_faces_direct(self, hdf_file, sr):
        """Extracts mesh cell faces with optimized coordinate handling."""
        try:
            if not self._hdf_cache['mesh_names']:
                return [], []
            
            raw_data, geometries = [], []
            
            for mesh_name in self._hdf_cache['mesh_names']:
                try:
                    base = f"Geometry/2D Flow Areas/{mesh_name}"
                    
                    # Load all data at once
                    facepoints_index = hdf_file[f"{base}/Faces FacePoint Indexes"][()]
                    facepoints_coords = hdf_file[f"{base}/FacePoints Coordinate"][()]
                    faces_perim_info = hdf_file[f"{base}/Faces Perimeter Info"][()]
                    faces_perim_values = hdf_file[f"{base}/Faces Perimeter Values"][()]
                    
                    # Process faces in batches
                    for face_id, ((p_a, p_b), (s_row, count)) in enumerate(
                        zip(facepoints_index, faces_perim_info)):
                        
                        # Build coordinate array efficiently
                        if count > 0:
                            coords = np.vstack([
                                facepoints_coords[p_a:p_a+1],
                                faces_perim_values[s_row:s_row + count],
                                facepoints_coords[p_b:p_b+1]
                            ])
                        else:
                            coords = np.vstack([
                                facepoints_coords[p_a:p_a+1],
                                facepoints_coords[p_b:p_b+1]
                            ])
                        
                        # Create polyline
                        arcpy_array = arcpy.Array([arcpy.Point(p[0], p[1]) for p in coords])
                        geometries.append(arcpy.Polyline(arcpy_array, sr))
                        raw_data.append({'mesh_name': mesh_name, 'face_id': face_id})
                    
                except KeyError:
                    arcpy.AddWarning(f"No face data for mesh '{mesh_name}'.")
            
            return raw_data, geometries
            
        except Exception as e:
            arcpy.AddError(f"HDF Read Error (Cell Faces): {e}")
            raise arcpy.ExecuteError()

    def _get_mesh_cells_direct(self, hdf_file, sr, precomputed_faces, messages):
        """
        Optimized mesh cell extraction using numpy arrays and pre-computed lookups.
        """
        try:
            messages.addMessage("Starting optimized cell polygon creation...")
            
            if not self._hdf_cache['mesh_names']:
                return [], []
            
            # Build optimized face lookup with numpy arrays
            face_lookup = {}
            face_arrays = {}  # Store face coordinates as numpy arrays
            
            for i, (face_attr, face_geom) in enumerate(zip(precomputed_faces[0], precomputed_faces[1])):
                mesh_name = face_attr['mesh_name']
                face_id = face_attr['face_id']
                
                if mesh_name not in face_lookup:
                    face_lookup[mesh_name] = {}
                    face_arrays[mesh_name] = {}
                
                face_lookup[mesh_name][face_id] = face_geom
                
                # Store coordinates as numpy array for faster access
                if face_geom and hasattr(face_geom, 'getPart'):
                    part = face_geom.getPart(0)
                    coords = np.array([[part.getObject(i).X, part.getObject(i).Y]
                                     for i in range(part.count) if part.getObject(i)])
                    if len(coords) > 0:
                        face_arrays[mesh_name][face_id] = coords
            
            raw_data, geometries = [], []
            total_cells_processed = 0
            
            for mesh_name in self._hdf_cache['mesh_names']:
                messages.addMessage(f"\nProcessing mesh '{mesh_name}'...")
                
                try:
                    base = f"Geometry/2D Flow Areas/{mesh_name}"
                    
                    # Load cell-face relationships
                    cell_face_info = hdf_file[f"{base}/Cells Face and Orientation Info"][()]
                    cell_face_values = hdf_file[f"{base}/Cells Face and Orientation Values"][()]
                    
                    # Extract as numpy arrays for efficiency
                    face_indices = cell_face_values[:, 0].astype(np.int32)
                    orientations = cell_face_values[:, 1].astype(np.int32)
                    
                    mesh_faces = face_lookup.get(mesh_name, {})
                    mesh_face_arrays = face_arrays.get(mesh_name, {})
                    
                    num_cells = len(cell_face_info)
                    cells_created = 0
                    
                    # Process in batches for progress reporting
                    batch_size = 5000
                    
                    for batch_start in range(0, num_cells, batch_size):
                        batch_end = min(batch_start + batch_size, num_cells)
                        batch_created = 0
                        
                        for cell_id in range(batch_start, batch_end):
                            start, length = cell_face_info[cell_id]
                            
                            if length < 3:  # Need at least 3 faces
                                continue
                            
                            # Get face indices for this cell
                            cell_face_ids = face_indices[start:start + length]
                            cell_orientations = orientations[start:start + length]
                            
                            # Collect face geometries
                            face_geoms = []
                            
                            for j, (face_id, orientation) in enumerate(zip(cell_face_ids, cell_orientations)):
                                if face_id in mesh_faces:
                                    face_geom = mesh_faces[face_id]
                                    
                                    # Handle orientation
                                    if orientation < 0 and face_id in mesh_face_arrays:
                                        # Create reversed geometry using numpy array
                                        coords = mesh_face_arrays[face_id][::-1]
                                        arcpy_array = arcpy.Array([arcpy.Point(x, y) for x, y in coords])
                                        face_geom = arcpy.Polyline(arcpy_array, sr)
                                    
                                    if face_geom:
                                        face_geoms.append(face_geom)
                            
                            if len(face_geoms) >= 3:
                                # Use optimized polygon construction
                                polygon = polygonize_arcpy_optimized(face_geoms, sr)
                                
                                if polygon and polygon.area > 0:
                                    raw_data.append({'mesh_name': mesh_name, 'cell_id': cell_id})
                                    geometries.append(polygon)
                                    cells_created += 1
                                    batch_created += 1
                                    total_cells_processed += 1
                        
                        # Progress update
                        if batch_end % 10000 == 0 or batch_end == num_cells:
                            messages.addMessage(
                                f"  Processed {batch_end}/{num_cells} cells in mesh '{mesh_name}' "
                                f"({cells_created} valid polygons)"
                            )
                    
                    messages.addMessage(
                        f"  Completed mesh '{mesh_name}': {cells_created} cells created"
                    )
                    
                except Exception as e:
                    messages.addErrorMessage(f"Error processing mesh '{mesh_name}': {str(e)}")
                    continue
            
            messages.addMessage(f"\nTotal cells processed: {total_cells_processed}")
            return raw_data, geometries
            
        except Exception as e:
            messages.addErrorMessage(f"Fatal error in cell creation: {str(e)}")
            raise arcpy.ExecuteError()

    # --- Main Execution Logic ---
    def execute(self, parameters, messages):
        hdf_path = parameters[0].valueAsText
        
        # Get selected elements
        geometry_elements = parameters[2].values if parameters[2].values else []
        
        if not geometry_elements:
            messages.addErrorMessage("No geometry elements selected for loading. Please select at least one element.")
            raise arcpy.ExecuteError
        
        # Get geodatabase parameters
        output_gdb = parameters[12].valueAsText
        create_gdb = parameters[13].value
        output_workspace = None
        
        # Extract project and plan info
        project_name, plan_number, base_name = extract_project_and_plan_info(hdf_path)
        
        # Get projection
        proj_wkt = get_ras_projection_wkt(hdf_path)
        sr = None
        if proj_wkt:
            sr = arcpy.SpatialReference()
            sr.loadFromString(proj_wkt)
            messages.addMessage(f"CRS '{sr.name}' found in HEC-RAS project files.")
        elif parameters[1].value:
            sr = parameters[1].value
            messages.addMessage(f"Using user-defined override CRS: {sr.name}")
        else:
            messages.addErrorMessage("CRS could not be determined. Please use the Override CRS parameter.")
            raise arcpy.ExecuteError
        
        # Setup geodatabase
        if create_gdb or output_gdb:
            if create_gdb and not output_gdb:
                # Auto-create geodatabase based on HDF name
                output_gdb = create_geodatabase_from_hdf(hdf_path, messages)
            
            # Create feature dataset with project/plan naming
            feature_dataset_name = get_feature_dataset_name(hdf_path)
            output_workspace = setup_geodatabase_output(output_gdb, feature_dataset_name, sr, messages)
            messages.addMessage(f"Output workspace set to: {output_workspace}")
        
        # Open HDF file once and cache metadata
        with h5py.File(hdf_path, 'r') as hdf_file:
            messages.addMessage("Caching HDF metadata...")
            self._hdf_cache = cache_hdf_metadata(hdf_file)
            
            # Pre-compute faces if needed
            precomputed_faces = None
            if (self.CELL_FACES in geometry_elements or 
                self.CELL_POLYS in geometry_elements):
                messages.addMessage("Pre-loading Mesh Cell Faces...")
                precomputed_faces = self._get_mesh_cell_faces_direct(hdf_file, sr)
            
            # Process geometry elements
            if self.BREAKLINES in geometry_elements and parameters[3].valueAsText:
                output_fc = parameters[3].valueAsText
                # Update output path if using geodatabase
                if output_workspace:
                    fc_name = "Breaklines2D"
                    output_fc = os.path.join(output_workspace, fc_name)
                    parameters[3].value = output_fc
                
                data, geoms = self._get_breaklines_direct(hdf_file, sr)
                fields = [("bl_id", "LONG"), ("Name", "TEXT"), ("CellSpaceNear", "FLOAT"), 
                         ("CellSpaceFar", "FLOAT"), ("NearRepeats", "LONG"), ("ProtectRadius", "LONG")]
                write_features_to_fc(output_fc, sr, "POLYLINE", fields, data, geoms, messages)
                if output_workspace and data:
                    add_feature_class_metadata(output_fc, "2D breaklines with cell spacing attributes", hdf_path)
            
            if self.BC_LINES in geometry_elements and parameters[4].valueAsText:
                output_fc = parameters[4].valueAsText
                # Update output path if using geodatabase
                if output_workspace:
                    fc_name = "BCLines2D"
                    output_fc = os.path.join(output_workspace, fc_name)
                    parameters[4].value = output_fc
                
                data, geoms = self._get_bc_lines_direct(hdf_file, sr)
                fields = [("bc_id", "LONG"), ("Name", "TEXT"), ("Type", "TEXT")]
                write_features_to_fc(output_fc, sr, "POLYLINE", fields, data, geoms, messages)
                if output_workspace and data:
                    add_feature_class_metadata(output_fc, "2D boundary condition lines", hdf_path)
            
            if self.PERIMETERS in geometry_elements and parameters[5].valueAsText:
                output_fc = parameters[5].valueAsText
                # Update output path if using geodatabase
                if output_workspace:
                    fc_name = "MeshPerimeters"
                    output_fc = os.path.join(output_workspace, fc_name)
                    parameters[5].value = output_fc
                
                data, geoms = self._get_mesh_areas_direct(hdf_file, sr)
                fields = [("mesh_name", "TEXT")]
                write_features_to_fc(output_fc, sr, "POLYGON", fields, data, geoms, messages)
                if output_workspace and data:
                    add_feature_class_metadata(output_fc, "2D flow area perimeter polygons", hdf_path)
            
            if self.CELL_POINTS in geometry_elements and parameters[6].valueAsText:
                output_fc = parameters[6].valueAsText
                # Update output path if using geodatabase
                if output_workspace:
                    fc_name = "MeshCellCenters"
                    output_fc = os.path.join(output_workspace, fc_name)
                    parameters[6].value = output_fc
                
                data, geoms = self._get_mesh_cell_points_direct(hdf_file, sr)
                fields = [("mesh_name", "TEXT"), ("cell_id", "LONG")]
                write_features_to_fc(output_fc, sr, "POINT", fields, data, geoms, messages)
                if output_workspace and data:
                    add_feature_class_metadata(output_fc, "Mesh cell center points", hdf_path)
            
            if self.CELL_FACES in geometry_elements and parameters[7].valueAsText:
                output_fc = parameters[7].valueAsText
                # Update output path if using geodatabase
                if output_workspace:
                    fc_name = "MeshCellFaces"
                    output_fc = os.path.join(output_workspace, fc_name)
                    parameters[7].value = output_fc
                
                if precomputed_faces:
                    data, geoms = precomputed_faces
                    fields = [("mesh_name", "TEXT"), ("face_id", "LONG")]
                    write_features_to_fc(output_fc, sr, "POLYLINE", fields, data, geoms, messages)
                    if output_workspace and data:
                        add_feature_class_metadata(output_fc, "Mesh cell face polylines", hdf_path)
            
            if self.CELL_POLYS in geometry_elements and parameters[8].valueAsText:
                output_fc = parameters[8].valueAsText
                # Update output path if using geodatabase
                if output_workspace:
                    fc_name = "MeshCellPolygons"
                    output_fc = os.path.join(output_workspace, fc_name)
                    parameters[8].value = output_fc
                
                if precomputed_faces:
                    messages.addMessage("Constructing cell polygons from faces...")
                    data, geoms = self._get_mesh_cells_direct(hdf_file, sr, precomputed_faces, messages)
                    fields = [("mesh_name", "TEXT"), ("cell_id", "LONG")]
                    write_features_to_fc(output_fc, sr, "POLYGON", fields, data, geoms, messages)
                    if output_workspace and data:
                        add_feature_class_metadata(output_fc, "Mesh cell polygons", hdf_path)
            
            # Process pipe network elements
            # Pipe networks use the same feature dataset as other geometry
            pipe_workspace = output_workspace
            
            if self.PIPE_CONDUITS in geometry_elements and parameters[9].valueAsText:
                # Check cache first
                if not self._hdf_cache.get('has_pipe_conduits', False):
                    messages.addMessage("No pipe conduits found in the HDF file.")
                else:
                    output_fc = parameters[9].valueAsText
                    # Update output path if using geodatabase
                    if pipe_workspace:
                        fc_name = "PipeConduits"
                        output_fc = os.path.join(pipe_workspace, fc_name)
                        parameters[9].value = output_fc
                    
                    messages.addMessage("Extracting Pipe Conduits...")
                    data, geoms = self._get_pipe_conduits_direct(hdf_file, sr)
                    if data:
                        # Get dynamic fields from the data
                        fields = get_dynamic_fields_from_data(data)
                        write_features_to_fc(output_fc, sr, "POLYLINE", fields, data, geoms, messages)
                        if pipe_workspace:
                            add_feature_class_metadata(output_fc, "Pipe conduits (storm/sewer networks)", hdf_path)
                    else:
                        messages.addMessage("No pipe conduits data extracted.")
            
            if self.PIPE_NODES in geometry_elements and parameters[10].valueAsText:
                # Check cache first
                if not self._hdf_cache.get('has_pipe_nodes', False):
                    messages.addMessage("No pipe nodes found in the HDF file.")
                else:
                    output_fc = parameters[10].valueAsText
                    # Update output path if using geodatabase
                    if pipe_workspace:
                        fc_name = "PipeNodes"
                        output_fc = os.path.join(pipe_workspace, fc_name)
                        parameters[10].value = output_fc
                    
                    messages.addMessage("Extracting Pipe Nodes...")
                    data, geoms = self._get_pipe_nodes_direct(hdf_file, sr)
                    if data:
                        # Get dynamic fields from the data
                        fields = get_dynamic_fields_from_data(data)
                        write_features_to_fc(output_fc, sr, "POINT", fields, data, geoms, messages)
                        if pipe_workspace:
                            add_feature_class_metadata(output_fc, "Pipe junction nodes", hdf_path)
                    else:
                        messages.addMessage("No pipe nodes data extracted.")
            
            if self.PIPE_NETWORKS in geometry_elements and parameters[11].valueAsText:
                # Check cache first - pipe networks might not have a specific cache flag
                output_fc = parameters[11].valueAsText
                # Update output path if using geodatabase
                if pipe_workspace:
                    fc_name = "PipeNetworks"
                    output_fc = os.path.join(pipe_workspace, fc_name)
                    parameters[11].value = output_fc
                
                messages.addMessage("Extracting Pipe Networks...")
                data, geoms = self._get_pipe_networks_direct(hdf_file, sr)
                if data:
                    # Get dynamic fields from the data
                    fields = get_dynamic_fields_from_data(data)
                    write_features_to_fc(output_fc, sr, "POLYGON", fields, data, geoms, messages)
                    if pipe_workspace:
                        add_feature_class_metadata(output_fc, "Pipe network elements", hdf_path)
                else:
                    messages.addMessage("No pipe networks data extracted.")
        
        messages.addMessage("\nProcessing complete.")
        return
    
    def getHelp(self, tool_name=None):
        """Return help documentation for the tool.
        
        This method is called when the user clicks the help button.
        It can return:
        - A URL (starting with http:// or https://)
        - A local file path (starting with file:///)
        - HTML content directly (for embedded help)
        """
        # Try local help file first
        help_file = os.path.join(
            os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 
            "Doc", "RASCommander_Help.html"
        )
        
        if os.path.exists(help_file):
            # Return local help file
            anchor = "#load-hec-ras-2d-geometry-layers"
            return f"file:///{help_file.replace(os.sep, '/')}{anchor}"
        else:
            # Fallback to online documentation
            return "https://github.com/gpt-cmdr/ras-commander-hydro#load-hec-ras-2d-geometry-layers"
    
    def getCodeSamples(self):
        """Provide code samples for using this tool programmatically."""
        return [
            {
                "title": "Basic 2D Mesh Import",
                "description": "Import mesh cell centers and perimeters",
                "code": """import arcpy

# Set input parameters
hdf_file = r"C:\\RAS_Projects\\MyProject\\MyProject.g01.hdf"
geometry_elements = ["Mesh Cell Centers", "Mesh Area Perimeters"]

# Run the tool
result = arcpy.RASCommander.LoadHECRAS2DGeometry(
    input_hdf=hdf_file,
    geometry_elements=geometry_elements,
    output_cell_points=r"memory\\MeshCenters",
    output_perimeters=r"memory\\MeshPerimeters"
)

print("2D geometry loaded successfully!")
print(f"Cell centers: {result[0]}")
print(f"Perimeters: {result[1]}")"""
            },
            {
                "title": "Complete 2D Mesh Extract",
                "description": "Extract full mesh geometry including polygons",
                "code": """import arcpy
import os

# Input HDF file
hdf_file = r"C:\\RAS_Projects\\MyProject\\MyProject.p01.hdf"

# Create geodatabase for outputs
gdb_path = os.path.join(os.path.dirname(hdf_file), "MyProject.p01.gdb")

# Extract all 2D geometry elements (including slow polygon creation)
arcpy.RASCommander.LoadHECRAS2DGeometry(
    input_hdf=hdf_file,
    geometry_elements=["2D Breaklines", "2D Boundary Condition Lines", 
                      "Mesh Area Perimeters", "Mesh Cell Centers", 
                      "Mesh Cells (Polygons)"],
    output_gdb=gdb_path,
    create_gdb=True
)

print(f"2D geometry organized in: {gdb_path}")"""
            },
            {
                "title": "Pipe Network Extraction",
                "description": "Extract storm/sewer pipe network components",
                "code": """import arcpy

# Extract pipe network elements
arcpy.RASCommander.LoadHECRAS2DGeometry(
    input_hdf=r"C:\\RAS_Projects\\Urban\\Urban.g01.hdf",
    geometry_elements=["Pipe Networks"],
    output_pipe_conduits=r"memory\\PipeConduits",
    output_pipe_nodes=r"memory\\PipeNodes"
)

# Query pipe statistics
with arcpy.da.SearchCursor("memory\\PipeConduits", ["Shape_Length", "Diameter"]) as cursor:
    total_length = 0
    for row in cursor:
        total_length += row[0]
    print(f"Total pipe length: {total_length:.2f} feet")"""
            },
            {
                "title": "Performance Optimized Extract",
                "description": "Extract mesh for large models (>100k cells)",
                "code": """import arcpy

# For large meshes, avoid polygon creation
arcpy.RASCommander.LoadHECRAS2DGeometry(
    input_hdf=r"C:\\RAS_Projects\\LargeModel.g01.hdf",
    geometry_elements=["Mesh Cell Centers", "Mesh Cell Faces", "Mesh Area Perimeters"],
    create_gdb=True
)

# Cell centers and faces provide mesh structure without expensive polygon creation
print("Large mesh geometry extracted efficiently")"""
            }
        ]
==================================================

File: c:\GH\ras-commander-hydro\Scripts\archydro\rc_load_hecras_2d_results.py
==================================================
# -*- coding: utf-8 -*-
"""
LoadHECRAS2DResults.py

Tool for loading HEC-RAS 2D results summary layers from HDF files including
maximum water surface elevation and face velocities.
"""

import arcpy
import os
import h5py
import numpy as np
from datetime import datetime, timedelta

# Import helper functions from utils
from rc_utils import (
    get_ras_projection_wkt,
    get_polyline_centroid_vectorized,
    cache_hdf_metadata,
    write_features_to_fc,
    setup_geodatabase_output,
    get_unique_fc_name,
    add_feature_class_metadata,
    extract_project_and_plan_info,
    create_geodatabase_from_hdf,
    get_feature_dataset_name,
    get_feature_class_name
)


class LoadHECRAS2DResults(object):
    """
    Loads 2D results summary data from a HEC-RAS HDF file.
    """
    def __init__(self):
        # Core properties
        self.label = "Load HEC-RAS 2D Results Summary Layers"
        self.description = """Extracts 2D results summary data from a HEC-RAS HDF file including maximum water surface elevation and face velocities.
        
        This tool extracts summary results data from HEC-RAS plan files (p*.hdf) that contain simulation results.
        
        Available results include:
        • Max WSE at Cell Centers - Maximum water surface elevation achieved at each cell center during the simulation, with the time of occurrence
        • Max Vel at Cell Faces - Maximum velocity achieved at each cell face during the simulation, with the time of occurrence
        
        Both results types create point feature classes with attributes for the maximum value and the time when it occurred.
        
        Note: This tool requires a plan HDF file that contains results data. Geometry-only files will not work."""
        
        # Extended metadata properties
        self.summary = "Extract maximum WSE and velocity results from HEC-RAS 2D simulations"
        self.usage = """Select a HEC-RAS plan HDF file containing simulation results and choose which summary statistics to extract.
        
        Steps:
        1. Browse to a HEC-RAS plan file (p*.hdf) with results
        2. Select which results to extract (Max WSE, Max Velocity)
        3. Specify output locations
        4. Optionally create an organized geodatabase
        
        The tool extracts:
        • Maximum values achieved during the entire simulation
        • Time of occurrence for each maximum
        • Cell/face identification and area information
        
        Use this tool for flood mapping and hazard assessment."""
        
        # Tool behavior
        self.canRunInBackground = False
        # self.category = "HEC-RAS Results Analysis"  # REMOVE THIS LINE
        
        # Documentation and credits
        self.tags = ["HEC-RAS", "2D Results", "Water Surface Elevation", "Velocity", 
                     "Flood Mapping", "Hazard Analysis", "Arc Hydro"]
        self.credits = "CLB Engineering Corporation"
        self.author = "CLB Engineering Corporation"
        self.version = "1.0.0"
        
        # Results elements
        self.MAX_WSE_POINTS = "Max WSE at Cell Centers"
        self.MAX_FACE_VEL_POINTS = "Max Vel at Cell Faces"
        
        # Cache for HDF metadata
        self._hdf_cache = {}

    def getParameterInfo(self):
        results_elements = [self.MAX_WSE_POINTS, self.MAX_FACE_VEL_POINTS]

        params = [
            arcpy.Parameter(displayName="Plan HDF File with Results", name="input_hdf", datatype="DEFile", 
                          parameterType="Required", direction="Input"),
            arcpy.Parameter(displayName="Override CRS (Optional)", name="override_crs", datatype="GPSpatialReference", 
                          parameterType="Optional", direction="Input"),
            
            # Results elements to load
            arcpy.Parameter(displayName="Results to Load", name="results_elements", datatype="GPString", 
                          parameterType="Required", direction="Input", multiValue=True),
            
            # Output parameters
            arcpy.Parameter(displayName="Output Max WSE at Cell Centers", name="output_max_wse", datatype="DEFeatureClass", 
                          parameterType="Optional", direction="Output"),
            arcpy.Parameter(displayName="Output Max Vel at Cell Faces", name="output_max_face_vel", datatype="DEFeatureClass", 
                          parameterType="Optional", direction="Output"),
            
            # Geodatabase organization parameters
            arcpy.Parameter(displayName="Output Geodatabase (Optional)", name="output_gdb", datatype="DEWorkspace", 
                          parameterType="Optional", direction="Output"),
            arcpy.Parameter(displayName="Create New Geodatabase", name="create_gdb", datatype="GPBoolean", 
                          parameterType="Optional", direction="Input")
        ]
        
        # Configure HDF file filter
        params[0].filter.list = ["hdf", "p*.hdf"]
        params[0].description = """Select a HEC-RAS plan file (p*.hdf) that contains simulation results. 
        Geometry-only files will not contain the required results data."""
        
        params[1].description = """Specify a coordinate reference system if it cannot be determined from the HEC-RAS project files. 
        The tool will first attempt to read the CRS from the HDF file or associated .prj files."""
        
        # Set filters for multi-value parameters
        params[2].filter.type = "ValueList"
        params[2].filter.list = results_elements
        params[2].value = [self.MAX_WSE_POINTS]  # Default selection
        params[2].description = """Select one or more results types to extract from the HDF file. 
        Each selected result will create a separate output feature class."""
        
        # Set default output paths and descriptions
        params[3].value = r"memory\MaximumWSE"
        params[3].description = """Output feature class for maximum water surface elevation points. 
        Includes attributes for cell ID, mesh name, maximum WSE value, and time of occurrence."""
        
        params[4].value = r"memory\MaximumFaceVelocity"
        params[4].description = """Output feature class for maximum face velocity points.
        
        Attributes include:
        • Face ID and 2D area name
        • Maximum velocity magnitude (ft/s or m/s)
        • Time of maximum occurrence
        • Face location (between cell centers)
        
        Use for identifying high velocity areas and erosion potential."""
        
        # Geodatabase parameters
        params[5].description = """Specify a geodatabase to organize all output feature classes. 
        If provided, outputs will be created in this geodatabase instead of the default locations."""
        
        params[6].value = True  # Default to creating new geodatabase
        params[6].description = """Create a new geodatabase based on the HDF file name. 
        The geodatabase will be named using the pattern: ProjectName.pXX.gdb"""
        
        return params

    def isLicensed(self):
        """Set whether tool is licensed to execute."""
        return True

    def updateParameters(self, parameters):
        """Modify the values and properties of parameters before internal validation."""
        # Enable/disable output parameters based on selected elements
        if parameters[2].value:
            selected = parameters[2].valueAsText.split(';') if parameters[2].valueAsText else []
            
            # Enable/disable outputs based on selection
            parameters[3].enabled = self.MAX_WSE_POINTS in selected
            parameters[4].enabled = self.MAX_FACE_VEL_POINTS in selected
        
        # Auto-populate geodatabase path when HDF file is selected
        if parameters[0].value and parameters[0].altered:  # input_hdf
            hdf_path = parameters[0].valueAsText
            
            # If create_gdb is True, auto-populate geodatabase path
            if parameters[6].value:  # create_gdb
                project_name, plan_number, base_name = extract_project_and_plan_info(hdf_path)
                gdb_name = f"{base_name}.gdb"
                gdb_path = os.path.join(os.path.dirname(hdf_path), gdb_name)
                parameters[5].value = gdb_path
        
        return
    
    def updateMessages(self, parameters):
        """Modify messages created by internal validation."""
        # Check if HDF file is plan file with results
        if parameters[0].value:
            hdf_path = parameters[0].valueAsText
            if hdf_path and os.path.basename(hdf_path).lower().startswith('g'):
                parameters[0].setWarningMessage(
                    "This appears to be a geometry file (g*.hdf). Results data is typically in plan files (p*.hdf)."
                )
        
        # Clear geodatabase validation error if create_gdb is True
        if parameters[6].value and parameters[5].hasError():  # create_gdb and output_gdb has error
            parameters[5].clearMessage()
        
        return

    # --- HDF Data Extraction Methods ---

    def _get_max_wse_points_direct(self, hdf_file, sr):
        """Extracts maximum water surface elevation points with vectorized operations."""
        try:
            if not self._hdf_cache['has_results']:
                arcpy.AddError("No results data found in HDF file.")
                return [], []
            
            raw_data, geometries = [], []
            start_time = self._hdf_cache['simulation_start_time']
            
            arcpy.AddMessage(f'Simulation start time: {start_time}')
            
            for mesh_name in self._hdf_cache['mesh_names']:
                arcpy.AddMessage(f'Processing max WSE for mesh: {mesh_name}')
                
                # Get cell centers
                centers_path = f"Geometry/2D Flow Areas/{mesh_name}/Cells Center Coordinate"
                if centers_path not in hdf_file:
                    arcpy.AddWarning(f"No cell centers found for mesh '{mesh_name}'. Skipping.")
                    continue
                
                cell_centers = hdf_file[centers_path][()]
                
                # Get maximum water surface data
                summary_path = f"Results/Unsteady/Output/Output Blocks/Base Output/Summary Output/2D Flow Areas/{mesh_name}/Maximum Water Surface"
                if summary_path not in hdf_file:
                    arcpy.AddWarning(f"No 'Maximum Water Surface' data for mesh '{mesh_name}'. Skipping.")
                    continue
                
                max_wse_data = hdf_file[summary_path][:]
                
                # Data is 2D array: row 0 = values, row 1 = times (in days)
                if max_wse_data.ndim == 2 and max_wse_data.shape[0] == 2:
                    wse_values = max_wse_data[0, :]
                    time_in_days = max_wse_data[1, :]
                else:
                    arcpy.AddWarning(f"Unexpected data format for mesh '{mesh_name}'. Skipping.")
                    continue
                
                num_cells = min(len(cell_centers), len(wse_values))
                
                # Vectorized time conversion
                time_deltas = np.array([timedelta(days=float(t)) for t in time_in_days[:num_cells]])
                times_of_max = [start_time + td for td in time_deltas]
                
                # Create data and geometries in batches
                mesh_data = [{
                    'mesh_name': mesh_name,
                    'cell_id': i,
                    'max_wse': float(wse_values[i]),
                    'max_wse_time': times_of_max[i]
                } for i in range(num_cells)]
                
                mesh_geometries = [
                    arcpy.PointGeometry(arcpy.Point(cell_centers[i][0], cell_centers[i][1]), sr)
                    for i in range(num_cells)
                ]
                
                raw_data.extend(mesh_data)
                geometries.extend(mesh_geometries)
            
            return raw_data, geometries
            
        except Exception as e:
            arcpy.AddError(f"HDF Read Error (Max WSE Points): {e}")
            raise arcpy.ExecuteError()

    def _get_mesh_cell_faces_direct(self, hdf_file, sr):
        """Extracts mesh cell faces needed for face velocity calculation."""
        try:
            if not self._hdf_cache['mesh_names']:
                return [], []
            
            raw_data, geometries = [], []
            
            for mesh_name in self._hdf_cache['mesh_names']:
                try:
                    base = f"Geometry/2D Flow Areas/{mesh_name}"
                    
                    # Load all data at once
                    facepoints_index = hdf_file[f"{base}/Faces FacePoint Indexes"][()]
                    facepoints_coords = hdf_file[f"{base}/FacePoints Coordinate"][()]
                    faces_perim_info = hdf_file[f"{base}/Faces Perimeter Info"][()]
                    faces_perim_values = hdf_file[f"{base}/Faces Perimeter Values"][()]
                    
                    # Process faces in batches
                    for face_id, ((p_a, p_b), (s_row, count)) in enumerate(
                        zip(facepoints_index, faces_perim_info)):
                        
                        # Build coordinate array efficiently
                        if count > 0:
                            coords = np.vstack([
                                facepoints_coords[p_a:p_a+1],
                                faces_perim_values[s_row:s_row + count],
                                facepoints_coords[p_b:p_b+1]
                            ])
                        else:
                            coords = np.vstack([
                                facepoints_coords[p_a:p_a+1],
                                facepoints_coords[p_b:p_b+1]
                            ])
                        
                        # Create polyline
                        arcpy_array = arcpy.Array([arcpy.Point(p[0], p[1]) for p in coords])
                        geometries.append(arcpy.Polyline(arcpy_array, sr))
                        raw_data.append({'mesh_name': mesh_name, 'face_id': face_id})
                    
                except KeyError:
                    arcpy.AddWarning(f"No face data for mesh '{mesh_name}'.")
            
            return raw_data, geometries
            
        except Exception as e:
            arcpy.AddError(f"HDF Read Error (Cell Faces): {e}")
            raise arcpy.ExecuteError()

    def _get_max_face_velocity_points_direct(self, hdf_file, sr):
        """Extracts maximum face velocity points with optimized centroid calculation."""
        try:
            if not self._hdf_cache['has_results']:
                arcpy.AddError("No results data found in HDF file.")
                return [], []
            
            raw_data, geometries = [], []
            start_time = self._hdf_cache['simulation_start_time']
            
            arcpy.AddMessage(f'Simulation start time: {start_time}')
            
            # Get face geometries
            face_data, face_geoms = self._get_mesh_cell_faces_direct(hdf_file, sr)
            
            # Build lookup
            face_lookup = {}
            for i, (face_attr, face_geom) in enumerate(zip(face_data, face_geoms)):
                mesh_name = face_attr['mesh_name']
                face_id = face_attr['face_id']
                
                if mesh_name not in face_lookup:
                    face_lookup[mesh_name] = {}
                
                face_lookup[mesh_name][face_id] = face_geom
            
            for mesh_name in self._hdf_cache['mesh_names']:
                arcpy.AddMessage(f'Processing max face velocity for mesh: {mesh_name}')
                
                # Get maximum face velocity data
                summary_path = f"Results/Unsteady/Output/Output Blocks/Base Output/Summary Output/2D Flow Areas/{mesh_name}/Maximum Face Velocity"
                if summary_path not in hdf_file:
                    arcpy.AddWarning(f"No 'Maximum Face Velocity' data for mesh '{mesh_name}'. Skipping.")
                    continue
                
                max_vel_data = hdf_file[summary_path][:]
                
                # Data is 2D array: row 0 = values, row 1 = times (in days)
                if max_vel_data.ndim == 2 and max_vel_data.shape[0] == 2:
                    vel_values = max_vel_data[0, :]
                    time_in_days = max_vel_data[1, :]
                else:
                    arcpy.AddWarning(f"Unexpected data format for mesh '{mesh_name}'. Skipping.")
                    continue
                
                # Process faces
                mesh_faces = face_lookup.get(mesh_name, {})
                
                for face_id in range(len(vel_values)):
                    if face_id not in mesh_faces:
                        continue
                    
                    face_geom = mesh_faces[face_id]
                    if not face_geom:
                        continue
                    
                    # Calculate centroid using optimized function
                    centroid_pt = get_polyline_centroid_vectorized(face_geom)
                    if not centroid_pt:
                        continue
                    
                    # Convert time
                    time_of_max = start_time + timedelta(days=float(time_in_days[face_id]))
                    
                    raw_data.append({
                        'mesh_name': mesh_name,
                        'face_id': face_id,
                        'max_vel': float(vel_values[face_id]),
                        'time_of_max': time_of_max
                    })
                    
                    geometries.append(arcpy.PointGeometry(centroid_pt, sr))
            
            return raw_data, geometries
            
        except Exception as e:
            arcpy.AddError(f"HDF Read Error (Max Face Velocity Points): {e}")
            raise arcpy.ExecuteError()

    # --- Main Execution Logic ---
    def execute(self, parameters, messages):
        hdf_path = parameters[0].valueAsText
        
        # Get selected elements
        results_elements = parameters[2].values if parameters[2].values else []
        
        if not results_elements:
            messages.addErrorMessage("No results elements selected for loading. Please select at least one element.")
            raise arcpy.ExecuteError
        
        # Get geodatabase parameters
        output_gdb = parameters[5].valueAsText if len(parameters) > 5 else None
        create_gdb = parameters[6].value if len(parameters) > 6 else False
        output_workspace = None
        
        # Extract project and plan info
        project_name, plan_number, base_name = extract_project_and_plan_info(hdf_path)
        
        # Get projection
        proj_wkt = get_ras_projection_wkt(hdf_path)
        sr = None
        if proj_wkt:
            sr = arcpy.SpatialReference()
            sr.loadFromString(proj_wkt)
            messages.addMessage(f"CRS '{sr.name}' found in HEC-RAS project files.")
        elif parameters[1].value:
            sr = parameters[1].value
            messages.addMessage(f"Using user-defined override CRS: {sr.name}")
        else:
            messages.addErrorMessage("CRS could not be determined. Please use the Override CRS parameter.")
            raise arcpy.ExecuteError
        
        # Setup geodatabase
        if create_gdb or output_gdb:
            if create_gdb and not output_gdb:
                # Auto-create geodatabase based on HDF name
                output_gdb = create_geodatabase_from_hdf(hdf_path, messages)
            
            # Create feature dataset with project/plan naming
            feature_dataset_name = get_feature_dataset_name(hdf_path)
            output_workspace = setup_geodatabase_output(output_gdb, feature_dataset_name, sr, messages)
            messages.addMessage(f"Output workspace set to: {output_workspace}")
        
        # Open HDF file once and cache metadata
        with h5py.File(hdf_path, 'r') as hdf_file:
            messages.addMessage("Caching HDF metadata...")
            self._hdf_cache = cache_hdf_metadata(hdf_file)
            
            # Check if results exist
            if not self._hdf_cache['has_results']:
                messages.addErrorMessage("No results data found in the HDF file. Please ensure this is a plan HDF file with results.")
                raise arcpy.ExecuteError
            
            # Process results elements
            if self.MAX_WSE_POINTS in results_elements and parameters[3].valueAsText:
                output_fc = parameters[3].valueAsText
                
                # Update output path if using geodatabase
                if output_workspace:
                    fc_name = "MaxWSE"
                    output_fc = os.path.join(output_workspace, fc_name)
                    parameters[3].value = output_fc
                
                messages.addMessage("Extracting Maximum Water Surface Elevation points...")
                data, geoms = self._get_max_wse_points_direct(hdf_file, sr)
                fields = [("mesh_name", "TEXT"), ("cell_id", "LONG"), ("max_wse", "DOUBLE"), 
                         ("max_wse_time", "DATE")]
                write_features_to_fc(output_fc, sr, "POINT", fields, data, geoms, messages)
                if output_workspace and data:
                    add_feature_class_metadata(output_fc, "Maximum water surface elevation at cell centers", hdf_path)
            
            if self.MAX_FACE_VEL_POINTS in results_elements and parameters[4].valueAsText:
                output_fc = parameters[4].valueAsText
                
                # Update output path if using geodatabase
                if output_workspace:
                    fc_name = "MaxVelocity"
                    output_fc = os.path.join(output_workspace, fc_name)
                    parameters[4].value = output_fc
                
                messages.addMessage("Extracting Maximum Face Velocity points...")
                data, geoms = self._get_max_face_velocity_points_direct(hdf_file, sr)
                fields = [("mesh_name", "TEXT"), ("face_id", "LONG"), ("max_vel", "DOUBLE"), 
                         ("time_of_max", "DATE")]
                write_features_to_fc(output_fc, sr, "POINT", fields, data, geoms, messages)
                if output_workspace and data:
                    add_feature_class_metadata(output_fc, "Maximum velocity at cell faces", hdf_path)
        
        messages.addMessage("\nProcessing complete.")
        return
    
    def getHelp(self, tool_name=None):
        """Return help documentation for the tool.
        
        This method is called when the user clicks the help button.
        It can return:
        - A URL (starting with http:// or https://)
        - A local file path (starting with file:///)
        - HTML content directly (for embedded help)
        """
        # Try local help file first
        help_file = os.path.join(
            os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 
            "Doc", "RASCommander_Help.html"
        )
        
        if os.path.exists(help_file):
            # Return local help file
            anchor = "#load-hec-ras-2d-results-summary-layers"
            return f"file:///{help_file.replace(os.sep, '/')}{anchor}"
        else:
            # Fallback to online documentation
            return "https://github.com/gpt-cmdr/ras-commander-hydro#load-hec-ras-2d-results-summary-layers"
    
    def getCodeSamples(self):
        """Provide code samples for using this tool programmatically."""
        return [
            {
                "title": "Basic Results Extraction",
                "description": "Extract maximum WSE from simulation results",
                "code": """import arcpy

# Set input parameters
hdf_file = r"C:\\RAS_Projects\\MyProject\\MyProject.p01.hdf"
results_elements = ["Max WSE at Cell Centers"]

# Run the tool
result = arcpy.RASCommander.LoadHECRAS2DResults(
    input_hdf=hdf_file,
    results_elements=results_elements,
    output_max_wse=r"memory\\MaxWSE"
)

print("Maximum WSE extracted successfully!")

# Query statistics
with arcpy.da.SearchCursor(result[0], ["Max_WSE", "Time_of_Max"]) as cursor:
    max_wse = max(row[0] for row in cursor)
    print(f"Peak WSE in model: {max_wse:.2f} feet")"""
            },
            {
                "title": "Complete Results Analysis",
                "description": "Extract both WSE and velocity results",
                "code": """import arcpy
import os

# Input plan file with results
hdf_file = r"C:\\RAS_Projects\\MyProject\\MyProject.p01.hdf"

# Create geodatabase for results
gdb_path = os.path.join(os.path.dirname(hdf_file), "MyProject_Results.gdb")

# Extract all results
arcpy.RASCommander.LoadHECRAS2DResults(
    input_hdf=hdf_file,
    results_elements=["Max WSE at Cell Centers", "Max Vel at Cell Faces"],
    output_gdb=gdb_path,
    create_gdb=True
)

print(f"Results organized in: {gdb_path}")

# Create flood depth raster (requires Spatial Analyst)
if arcpy.CheckExtension("Spatial") == "Available":
    arcpy.CheckOutExtension("Spatial")
    wse_points = os.path.join(gdb_path, "Results", "MaxWSE_CellCenters")
    depth_raster = arcpy.sa.Idw(wse_points, "Max_WSE")
    depth_raster.save(os.path.join(gdb_path, "FloodDepth"))"""
            },
            {
                "title": "Time Analysis",
                "description": "Analyze when peak conditions occurred",
                "code": """import arcpy
from datetime import datetime

# Extract results
result = arcpy.RASCommander.LoadHECRAS2DResults(
    input_hdf=r"C:\\RAS_Projects\\TimeSeries.p01.hdf",
    results_elements=["Max WSE at Cell Centers", "Max Vel at Cell Faces"]
)

# Analyze timing of peaks
wse_fc = result[0]
vel_fc = result[1]

# Find when most cells peaked
time_counts = {}
with arcpy.da.SearchCursor(wse_fc, ["Time_of_Max"]) as cursor:
    for row in cursor:
        time_str = row[0]
        time_counts[time_str] = time_counts.get(time_str, 0) + 1

peak_time = max(time_counts, key=time_counts.get)
print(f"Most cells peaked at: {peak_time}")
print(f"Number of cells: {time_counts[peak_time]}")"""
            },
            {
                "title": "Hazard Classification",
                "description": "Classify flood hazard based on depth and velocity",
                "code": """import arcpy

# Extract results
arcpy.RASCommander.LoadHECRAS2DResults(
    input_hdf=r"C:\\RAS_Projects\\Hazard.p01.hdf",
    results_elements=["Max WSE at Cell Centers", "Max Vel at Cell Faces"],
    create_gdb=True
)

# Join velocity to WSE points for hazard analysis
# (Additional spatial join and hazard calculation code would go here)

print("Results ready for hazard classification")"""
            }
        ]
==================================================

File: c:\GH\ras-commander-hydro\Scripts\archydro\rc_load_ras_terrain.py
==================================================
# -*- coding: utf-8 -*-
"""
LoadRASTerrain.py

Tool for loading HEC-RAS terrain layers from a project's rasmap file.
"""

import arcpy
import os
import xml.etree.ElementTree as ET
from pathlib import Path


class LoadRASTerrain(object):
    """
    Loads one or more HEC-RAS terrain layers from a project's rasmap file.
    
    IMPORTANT: This tool loads the underlying terrain TIFFs as a VRT (Virtual Raster) 
    with the priority from HEC-RAS in place. It does NOT include terrain modifications 
    done as vector terrain modifications in RAS Mapper. Only the base terrain raster 
    data will be loaded.
    """
    def __init__(self):
        # Core properties
        self.label = "Load HEC-RAS Terrain"
        self.description = """Loads terrain layers defined in a HEC-RAS project's .rasmap file into the current map.
        
        ⚠️ IMPORTANT REMINDER: The loaded terrain layers are base VRT files only. Any vector terrain 
        modifications (breaklines, high ground, etc.) made in RAS Mapper are NOT included in these layers."""
        
        # Extended metadata properties
        self.summary = "Import HEC-RAS terrain layers from RAS Mapper VRT files"
        self.usage = """Select a HEC-RAS project file to load associated terrain layers.
        
        Steps:
        1. Browse to a HEC-RAS project file (*.prj)
        2. Choose to import all terrains or select specific ones
        3. Terrain layers will be added to the current map
        
        Limitations:
        • Only loads base terrain rasters (VRT files)
        • Does NOT include vector terrain modifications
        • Does NOT include breaklines or high ground modifications
        • For complete terrain with modifications, use HEC-RAS itself
        
        The tool reads terrain definitions from the .rasmap file and loads
        the corresponding Virtual Raster (VRT) files."""
        
        # Tool behavior
        self.canRunInBackground = False
        # self.category = "HEC-RAS Data Import"  # REMOVE THIS LINE
        
        # Documentation and credits
        self.tags = ["HEC-RAS", "Terrain", "DEM", "RAS Mapper", "VRT", "Elevation", "Arc Hydro"]
        self.credits = "CLB Engineering Corporation"
        self.author = "CLB Engineering Corporation"
        self.version = "1.0.0"
        
        # Cache for terrain metadata
        self._terrain_cache = {}

    def getParameterInfo(self):
        params = [
            arcpy.Parameter(
                displayName="HEC-RAS Project File (*.prj)",
                name="in_ras_project",
                datatype="DEFile",
                parameterType="Required",
                direction="Input"
            ),
            arcpy.Parameter(
                displayName="Import All Terrains",
                name="import_all",
                datatype="GPBoolean",
                parameterType="Optional",
                direction="Input"
            ),
            arcpy.Parameter(
                displayName="Terrains to Load",
                name="terrains_to_load",
                datatype="GPString",
                parameterType="Optional",
                direction="Input",
                multiValue=True
            )
        ]
        
        params[0].filter.list = ["prj"]
        params[0].description = """Select the HEC-RAS project file (*.prj).
        
        The tool will:
        1. Automatically find the associated .rasmap file
        2. Read terrain layer definitions
        3. Locate the corresponding VRT files
        
        Ensure the project has been opened in RAS Mapper at least once
        to generate the necessary terrain files."""
        # params[0].category = "Input Data"  # Remove category grouping
        
        params[1].value = False
        params[1].description = """Check this box to import all terrain layers found in the project.
        
        When enabled:
        • All terrain layers will be loaded
        • Terrain selection list will be disabled
        • Useful for complete project visualization
        
        When disabled:
        • Select specific terrain layers to load
        • Reduces memory usage for large projects"""
        # params[1].category = "Import Options"  # Remove category grouping
        
        params[2].filter.type = "ValueList"
        params[2].description = """Select specific terrain layers to load.
        
        Available terrains:
        • Listed from the .rasmap file
        • Each represents a VRT (Virtual Raster)
        • May include primary terrain and alternates
        
        Note: Disabled when 'Import All Terrains' is checked.
        
        ⚠️ Remember: These are base terrain files only.
        Vector modifications are not included."""
        # params[2].category = "Import Options"  # Remove category grouping
        
        return params
    
    def updateMessages(self, parameters):
        """Modify the messages created by internal parameter validation."""
        return

    def isLicensed(self):
        return True

    def _get_rasmap_path_from_prj(self, prj_path_str):
        """Finds the .rasmap file associated with a .prj file."""
        if not prj_path_str or not os.path.exists(prj_path_str):
            return None
        
        p = Path(prj_path_str)
        rasmap_path = p.with_suffix('.rasmap')
        
        if rasmap_path.exists():
            return str(rasmap_path)
        
        arcpy.AddWarning(f"Could not find associated .rasmap file for {p.name}")
        return None

    def _get_terrain_info_from_rasmap(self, rasmap_path_str):
        """Parses a .rasmap file to get terrain names and their HDF file paths."""
        if not rasmap_path_str or not os.path.exists(rasmap_path_str):
            return {}
            
        terrains = {}
        try:
            tree = ET.parse(rasmap_path_str)
            root = tree.getroot()
            project_folder = os.path.dirname(rasmap_path_str)
            
            for layer in root.findall(".//Terrains/Layer"):
                name = layer.get('Name')
                filename_rel = layer.get('Filename')
                
                if name and filename_rel:
                    # HEC-RAS paths can start with '.\', remove it for robust joining
                    clean_rel_path = filename_rel.lstrip('.\\/')
                    filename_abs = os.path.join(project_folder, clean_rel_path)
                    terrains[name] = os.path.normpath(filename_abs)
                    
        except ET.ParseError as e:
            arcpy.AddWarning(f"Error parsing .rasmap file {os.path.basename(rasmap_path_str)}: {e}")
        except Exception as e:
            arcpy.AddWarning(f"An unexpected error occurred while reading the .rasmap file: {e}")
            
        return terrains

    def updateParameters(self, parameters):
        """Modify the parameters on the GUI according to user input."""
        if parameters[0].value and parameters[0].altered:
            prj_path = parameters[0].valueAsText
            rasmap_path = self._get_rasmap_path_from_prj(prj_path)
            
            if rasmap_path:
                self._terrain_cache = self._get_terrain_info_from_rasmap(rasmap_path)
                terrain_names = list(self._terrain_cache.keys())
                parameters[2].filter.list = sorted(terrain_names)
            else:
                parameters[2].filter.list = []
                self._terrain_cache = {}

        if parameters[1].value is True:
            parameters[2].enabled = False
            parameters[2].value = None # Clear selection if "Import All" is checked
        else:
            parameters[2].enabled = True
        return

    def execute(self, parameters, messages):
        """The source code of the tool."""
        prj_path = parameters[0].valueAsText
        import_all = parameters[1].value
        selected_terrains = parameters[2].values

        if not self._terrain_cache:
            rasmap_path = self._get_rasmap_path_from_prj(prj_path)
            if rasmap_path:
                self._terrain_cache = self._get_terrain_info_from_rasmap(rasmap_path)

        if not self._terrain_cache:
            messages.addErrorMessage("No terrains found or .rasmap file could not be read. Aborting.")
            return

        terrains_to_load = []
        if import_all:
            terrains_to_load = list(self._terrain_cache.keys())
            messages.addMessage("Import All selected. Loading all available terrains...")
        elif selected_terrains:
            terrains_to_load = selected_terrains
            messages.addMessage(f"Loading selected terrains: {', '.join(terrains_to_load)}")
        else:
            messages.addErrorMessage("No terrains selected for loading.")
            return
            
        try:
            aprx = arcpy.mp.ArcGISProject("CURRENT")
            # Ensure there is an active map
            if not aprx.activeMap:
                 messages.addErrorMessage("No active map found. Please open a map view and try again.")
                 return
            map = aprx.activeMap
        except Exception as e:
            messages.addErrorMessage(f"Could not access the current ArcGIS Pro project or map: {e}")
            return
            
        layers_added = 0
        for terrain_name in terrains_to_load:
            hdf_path_str = self._terrain_cache.get(terrain_name)
            if not hdf_path_str:
                messages.addWarningMessage(f"Could not find path for terrain '{terrain_name}'. Skipping.")
                continue
            
            p = Path(hdf_path_str)
            vrt_path = str(p.with_suffix('.vrt'))
            
            if os.path.exists(vrt_path):
                try:
                    map.addDataFromPath(vrt_path)
                    messages.addMessage(f"Successfully added terrain layer: {terrain_name}")
                    layers_added += 1
                except Exception as e:
                    messages.addWarningMessage(f"Failed to add layer for '{terrain_name}' from path {vrt_path}: {e}")
            else:
                messages.addWarningMessage(f"Associated VRT file not found for terrain '{terrain_name}'. Expected at: {vrt_path}")
        
        messages.addMessage(f"\nProcessing complete. Added {layers_added} terrain layer(s).")
        return
    
    def getHelp(self, tool_name=None):
        """Return help documentation for the tool.
        
        This method is called when the user clicks the help button.
        It can return:
        - A URL (starting with http:// or https://)
        - A local file path (starting with file:///)
        - HTML content directly (for embedded help)
        """
        # Try local help file first
        help_file = os.path.join(
            os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 
            "Doc", "RASCommander_Help.html"
        )
        
        if os.path.exists(help_file):
            # Return local help file
            anchor = "#load-hec-ras-terrain"
            return f"file:///{help_file.replace(os.sep, '/')}{anchor}"
        else:
            # Fallback to online documentation
            return "https://github.com/gpt-cmdr/ras-commander-hydro#load-hec-ras-terrain"
    
    def getCodeSamples(self):
        """Provide code samples for using this tool programmatically."""
        return [
            {
                "title": "Load All Terrains",
                "description": "Import all terrain layers from a HEC-RAS project",
                "code": """import arcpy

# Set input project file
project_file = r"C:\\RAS_Projects\\MyProject\\MyProject.prj"

# Load all terrains
arcpy.RASCommander.LoadRASTerrain(
    in_ras_project=project_file,
    import_all=True
)

print("All terrain layers loaded to current map")

# List loaded raster layers
aprx = arcpy.mp.ArcGISProject("CURRENT")
map_obj = aprx.activeMap
for lyr in map_obj.listLayers():
    if lyr.isRasterLayer:
        print(f"  - {lyr.name}")"""
            },
            {
                "title": "Load Specific Terrains",
                "description": "Select and load specific terrain layers",
                "code": """import arcpy

# Set input project file
project_file = r"C:\\RAS_Projects\\MyProject\\MyProject.prj"

# Load specific terrains
terrains = ["Primary Terrain", "Proposed Conditions"]

arcpy.RASCommander.LoadRASTerrain(
    in_ras_project=project_file,
    import_all=False,
    terrains_to_load=terrains
)

print(f"Loaded {len(terrains)} terrain layers")"""
            },
            {
                "title": "Terrain Analysis",
                "description": "Load terrain and perform elevation analysis",
                "code": """import arcpy

# Load primary terrain
project_file = r"C:\\RAS_Projects\\FloodStudy\\FloodStudy.prj"

arcpy.RASCommander.LoadRASTerrain(
    in_ras_project=project_file,
    import_all=False,
    terrains_to_load=["Primary Terrain"]
)

# Get loaded terrain layer
aprx = arcpy.mp.ArcGISProject("CURRENT")
map_obj = aprx.activeMap
terrain_lyr = None

for lyr in map_obj.listLayers():
    if lyr.isRasterLayer and "Primary Terrain" in lyr.name:
        terrain_lyr = lyr
        break

if terrain_lyr:
    # Get elevation statistics
    desc = arcpy.Describe(terrain_lyr)
    print(f"Terrain extent: {desc.extent}")
    
    # Calculate statistics if needed
    arcpy.management.CalculateStatistics(terrain_lyr)"""
            },
            {
                "title": "Batch Processing",
                "description": "Load terrains from multiple projects",
                "code": """import arcpy
import os

# Directory containing HEC-RAS projects
project_dir = r"C:\\RAS_Projects"

# Find all project files
for root, dirs, files in os.walk(project_dir):
    for file in files:
        if file.endswith(".prj"):
            project_path = os.path.join(root, file)
            
            try:
                # Load primary terrain from each project
                arcpy.RASCommander.LoadRASTerrain(
                    in_ras_project=project_path,
                    import_all=False,
                    terrains_to_load=["Primary Terrain"]
                )
                print(f"Loaded terrain from: {file}")
            except Exception as e:
                print(f"Failed to load from {file}: {str(e)}")"""
            }
        ]
==================================================

File: c:\GH\ras-commander-hydro\Scripts\archydro\rc_organize_ras_project.py
==================================================
# -*- coding: utf-8 -*-
"""
OrganizeRASProject.py

Master tool for organizing all HEC-RAS data from HDF files into a structured geodatabase.
This tool extracts all available geometry and results data in a single operation.
"""

import arcpy
import os
import h5py
import numpy as np

# Import helper functions from utils
from rc_utils import (
    get_ras_projection_wkt,
    setup_geodatabase_output,
    get_unique_fc_name,
    add_feature_class_metadata,
    extract_project_and_plan_info,
    create_geodatabase_from_hdf,
    get_feature_dataset_name,
    get_feature_class_name
)

# Import the individual tool classes
from rc_load_hecras_1d_geometry import LoadHECRAS1DGeometry
from rc_load_hecras_2d_geometry import LoadHECRAS2DGeometry
from rc_load_hecras_2d_results import LoadHECRAS2DResults


class OrganizeRASProject(object):
    """
    Organizes all HEC-RAS data into a structured geodatabase.
    """
    def __init__(self):
        # Core properties
        self.label = "Organize HEC-RAS Project"
        self.description = """Extracts all geometry and results from HEC-RAS files into an organized geodatabase.
        
        This tool automatically:
        • Creates a well-organized geodatabase structure
        • Extracts all available 1D geometry elements
        • Extracts all available 2D geometry elements
        • Extracts pipe network data if present
        • Extracts results data if available
        
        The geodatabase will be organized with feature datasets named by project and plan:
        • {ProjectName}_Plan{XX} - Contains all geometry and results for each plan
        
        Each plan's feature dataset will contain:
        • 1D Geometry - Cross sections, river centerlines, bank lines, structures
        • 2D Geometry - Breaklines, boundary conditions, mesh elements
        • Pipe Networks - Storm/sewer pipe networks (if present)
        • Results - Maximum WSE, velocity, and other results (if available)
        
        Note: This operation may take several minutes for large models."""
        
        # Extended metadata properties
        self.summary = "Comprehensive HEC-RAS project organization into geodatabase"
        self.usage = """Select a HEC-RAS project directory or individual HDF file to organize all data.
        
        Steps:
        1. Choose input (project folder or single HDF file)
        2. Specify output geodatabase location
        3. Select which data types to include
        4. Set processing options
        5. Tool extracts and organizes all available data
        
        Input options:
        • Project directory - Processes all HDF files found
        • Single HDF file - Processes just that file
        
        Organization structure:
        • Feature datasets by plan (Plan01, Plan02, etc.)
        • Within each plan: 1D_Geometry, 2D_Geometry, Results
        • Automatic naming and metadata
        • Preserves all HEC-RAS attributes
        
        Performance tips:
        • Disable mesh polygons for large models (>100k cells)
        • Use local drives for better performance
        • Close other applications during processing"""
        
        # Tool behavior
        self.canRunInBackground = False
        # self.category = "HEC-RAS Project Management"  # REMOVE THIS LINE
        
        # Documentation and credits
        self.tags = ["HEC-RAS", "Project Organization", "Geodatabase", "Batch Processing", 
                     "1D Geometry", "2D Geometry", "Results", "Pipe Networks", "Arc Hydro"]
        self.credits = "CLB Engineering Corporation"
        self.author = "CLB Engineering Corporation"
        self.version = "1.0.0"

    def getParameterInfo(self):
        params = [
            # Input files
            arcpy.Parameter(displayName="HEC-RAS Project Directory or HDF File", name="input_path", 
                          datatype=["DEFolder", "DEFile"], 
                          parameterType="Required", direction="Input"),
            
            # Output geodatabase
            arcpy.Parameter(displayName="Output Geodatabase", name="output_gdb", datatype="DEWorkspace", 
                          parameterType="Required", direction="Output"),
            
            # CRS override
            arcpy.Parameter(displayName="Override CRS (Optional)", name="override_crs", datatype="GPSpatialReference", 
                          parameterType="Optional", direction="Input"),
            
            # Data type selection
            arcpy.Parameter(displayName="Include 1D Geometry", name="include_1d_geometry", datatype="GPBoolean", 
                          parameterType="Optional", direction="Input"),
            arcpy.Parameter(displayName="Include 2D Geometry", name="include_2d_geometry", datatype="GPBoolean", 
                          parameterType="Optional", direction="Input"),
            arcpy.Parameter(displayName="Include 2D Results Summary Layers", name="include_2d_results", datatype="GPBoolean", 
                          parameterType="Optional", direction="Input"),
            
            # Options
            arcpy.Parameter(displayName="Include Mesh Cell Polygons", name="include_cell_polygons", datatype="GPBoolean", 
                          parameterType="Optional", direction="Input"),
        ]
        
        # Configure parameters
        params[0].description = """Select a HEC-RAS project directory to process all plan files (p*.hdf), 
        or select a single HDF file to process."""
        # params[0].category = "Input Data"  # Remove category grouping
        
        params[1].description = "Output geodatabase that will contain all extracted data in an organized structure."
        # params[1].category = "Output"  # Remove category grouping
        
        params[2].description = """Specify a coordinate reference system if it cannot be determined from the HEC-RAS project files."""
        # params[2].category = "Input Data"  # Remove category grouping
        
        # Data type selection parameters
        params[3].value = True
        params[3].description = """Include all 1D geometry elements in the output.
        
        When enabled, extracts:
        • Cross sections with station-elevation data
        • River centerlines
        • Bank lines
        • Edge lines
        • 1D structures (bridges, culverts, weirs)
        
        Disable if you only need 2D data."""
        # params[3].category = "Data Selection"  # Remove category grouping
        
        params[4].value = True
        params[4].description = """Include all 2D geometry elements in the output.
        
        When enabled, extracts:
        • 2D breaklines
        • Boundary condition lines
        • Mesh area perimeters
        • Mesh cell centers
        • Mesh cell faces
        • Mesh cell polygons (if enabled)
        • Pipe networks (conduits, nodes, and network cells)
        
        Disable if you only need 1D data."""
        # params[4].category = "Data Selection"  # Remove category grouping
        
        params[5].value = True
        params[5].description = """Include 2D results summary layers in the output.
        
        When enabled, extracts:
        • Maximum water surface elevation at cell centers
        • Maximum velocity at cell faces
        • Time of maximum occurrence
        
        Disable if you only need geometry without results."""
        # params[5].category = "Data Selection"  # Remove category grouping
        
        params[6].value = False
        params[6].description = """Include full polygon representation of mesh cells.
        
        WARNING: This can significantly increase processing time for large meshes.
        
        Performance guidelines:
        • < 10,000 cells: Safe to enable
        • 10,000 - 50,000 cells: May take several minutes
        • 50,000 - 100,000 cells: May take 10-30 minutes
        • > 100,000 cells: Not recommended (use cell centers instead)
        
        When disabled, only cell centers and faces are extracted."""
        # params[6].category = "Processing Options"  # Remove category grouping
        
        return params

    def isLicensed(self):
        """Set whether tool is licensed to execute."""
        return True

    def updateParameters(self, parameters):
        """Modify the values and properties of parameters before internal validation."""
        # Set default output geodatabase name based on input
        if parameters[0].value and not parameters[1].altered:
            input_path = parameters[0].valueAsText
            
            if os.path.isdir(input_path):
                # For directory, use directory name
                input_name = os.path.basename(input_path.rstrip(os.sep))
                default_gdb = os.path.join(input_path, f"{input_name}_Organized.gdb")
            else:
                # For single file, use file name
                input_name = os.path.splitext(os.path.basename(input_path))[0]
                default_gdb = os.path.join(os.path.dirname(input_path), f"{input_name}_Organized.gdb")
            
            parameters[1].value = default_gdb
        
        # Enable/disable mesh polygon option based on 2D geometry selection
        if parameters[4].value == False:  # include_2d_geometry
            parameters[6].enabled = False  # include_cell_polygons
            parameters[6].value = False
        else:
            parameters[6].enabled = True
        
        return
    
    def updateMessages(self, parameters):
        """Modify messages created by internal validation."""
        # Warn about mesh polygons
        if parameters[6].value:
            parameters[6].setWarningMessage(
                "Creating cell polygons can be time-consuming for large meshes (>100,000 cells)."
            )
        
        # Warn if no data types selected
        if not any([parameters[3].value, parameters[4].value, parameters[5].value]):
            parameters[3].setErrorMessage("At least one data type must be selected.")
        
        return

    def _check_hdf_contents(self, hdf_file):
        """Check what data is available in the HDF file."""
        contents = {
            'has_1d': False,
            'has_2d': False,
            'has_pipes': False,
            'has_results': False,
            '1d_elements': [],
            '2d_elements': [],
            'pipe_elements': [],
            'result_profiles': []
        }
        
        # Check for 1D geometry
        if "Geometry/Cross Sections" in hdf_file:
            # Verify that it has actual data
            if "Geometry/Cross Sections/Attributes" in hdf_file:
                contents['has_1d'] = True
                contents['1d_elements'].append("Cross Sections")
        if "Geometry/River Centerlines" in hdf_file:
            contents['has_1d'] = True
            contents['1d_elements'].append("River Centerlines")
        if "Geometry/River Bank Lines" in hdf_file:
            contents['has_1d'] = True
            contents['1d_elements'].append("Bank Lines")
        if "Geometry/River Edge Lines" in hdf_file:
            contents['has_1d'] = True
            contents['1d_elements'].append("Edge Lines")
        if "Geometry/Structures" in hdf_file:
            # Verify that it has actual data
            if "Geometry/Structures/Attributes" in hdf_file:
                contents['has_1d'] = True
                contents['1d_elements'].append("1D Structures")
        
        # Check for 2D geometry
        if "Geometry/2D Flow Areas" in hdf_file:
            contents['has_2d'] = True
            contents['2d_elements'].append("Mesh Area Perimeters")
            contents['2d_elements'].append("Mesh Cell Centers")
            contents['2d_elements'].append("Mesh Cell Faces")
        if "Geometry/2D Flow Area Break Lines" in hdf_file:
            contents['has_2d'] = True
            contents['2d_elements'].append("2D Breaklines")
        if "Geometry/Boundary Condition Lines" in hdf_file:
            contents['has_2d'] = True
            contents['2d_elements'].append("2D Boundary Condition Lines")
        
        # Check for pipe networks - these are part of 2D geometry
        if "Geometry/Pipe Conduits" in hdf_file:
            contents['has_pipes'] = True
            contents['has_2d'] = True
            contents['pipe_elements'].append("Pipe Conduits")
            contents['2d_elements'].append("Pipe Conduits")
        if "Geometry/Pipe Nodes" in hdf_file:
            contents['has_pipes'] = True
            contents['has_2d'] = True
            contents['pipe_elements'].append("Pipe Nodes")
            contents['2d_elements'].append("Pipe Nodes")
        if "Geometry/Pipe Networks" in hdf_file:
            contents['has_pipes'] = True
            contents['has_2d'] = True
            contents['pipe_elements'].append("Pipe Networks")
            contents['2d_elements'].append("Pipe Networks")
        
        # Check for results
        if "Results/Unsteady/Output/Output Blocks/Base Output/Summary Output/2D Flow Areas" in hdf_file:
            contents['has_results'] = True
            # Get available output profiles
            results_path = "Results/Unsteady/Output/Output Blocks/Base Output/Summary Output/2D Flow Areas"
            try:
                for flow_area in hdf_file[results_path]:
                    if "Maximum Water Surface" in hdf_file[f"{results_path}/{flow_area}"]:
                        contents['result_profiles'].append("Maximum Water Surface")
                    if "Maximum Face Velocity" in hdf_file[f"{results_path}/{flow_area}"]:
                        contents['result_profiles'].append("Maximum Face Velocity")
                    break
            except:
                pass
        
        return contents

    def _load_geodatabase_to_map(self, gdb_path, messages):
        """Load all feature classes from the geodatabase into the current map, grouped by plan."""
        try:
            # Get the current project and map
            aprx = arcpy.mp.ArcGISProject("CURRENT")
            map_obj = aprx.activeMap
            
            if not map_obj:
                messages.addWarning("No active map found. Feature classes were created but not added to the map.")
                return
            
            # Collect all feature classes and rasters first
            all_layers = []
            
            # Set workspace to the geodatabase
            arcpy.env.workspace = gdb_path
            
            # Collect all feature classes
            for dirpath, dirnames, filenames in arcpy.da.Walk(gdb_path, datatype="FeatureClass"):
                for filename in filenames:
                    fc_path = os.path.join(dirpath, filename)
                    all_layers.append((filename, fc_path, "FeatureClass"))
            
            # Collect all rasters
            for dirpath, dirnames, filenames in arcpy.da.Walk(gdb_path, datatype="RasterDataset"):
                for filename in filenames:
                    raster_path = os.path.join(dirpath, filename)
                    all_layers.append((filename, raster_path, "Raster"))
            
            # Group layers by plan
            plan_groups = {}
            for layer_name, layer_path, layer_type in all_layers:
                # Extract plan info from layer name (e.g., "ProjectName_Plan_03_LayerType")
                if "_Plan_" in layer_name:
                    parts = layer_name.split("_Plan_")
                    if len(parts) >= 2:
                        project_name = parts[0]
                        # Extract plan number (should be next 2 characters after Plan_)
                        remaining = parts[1]
                        if len(remaining) >= 2:
                            plan_number = remaining[:2]
                            plan_key = "{}_Plan_{}".format(project_name, plan_number)
                            
                            if plan_key not in plan_groups:
                                plan_groups[plan_key] = []
                            plan_groups[plan_key].append((layer_name, layer_path, layer_type))
                else:
                    # Layers without plan info go to a general group
                    if "General" not in plan_groups:
                        plan_groups["General"] = []
                    plan_groups["General"].append((layer_name, layer_path, layer_type))
            
            # Sort plans and layers within each plan
            sorted_plans = sorted(plan_groups.keys())
            
            # Add layers to map grouped by plan
            feature_classes_added = []
            messages.addMessage("\nAdding layers to map grouped by plan:")
            
            for plan_key in sorted_plans:
                # Create a group layer for this plan
                group_name = plan_key if plan_key != "General" else "Other Layers"
                
                try:
                    # Create the group layer first
                    group_layer = map_obj.createGroupLayer(group_name)
                    
                    # Sort layers within this plan alphabetically
                    plan_layers = sorted(plan_groups[plan_key], key=lambda x: x[0].lower())
                    
                    # Add layers to the group
                    layers_added_to_group = 0
                    layers_to_remove = []  # Track layers to remove after adding to group
                    
                    for layer_name, layer_path, layer_type in plan_layers:
                        try:
                            # First add the layer to the map
                            layer = map_obj.addDataFromPath(layer_path)
                            if layer:
                                # Then add it to the group layer
                                map_obj.addLayerToGroup(group_layer, layer, "AUTO_ARRANGE")
                                
                                # Track the original layer for removal
                                layers_to_remove.append(layer)
                                
                                feature_classes_added.append(layer_name)
                                layers_added_to_group += 1
                                messages.addMessage("  Added {} to {}".format(layer_name, group_name))
                        except Exception as e:
                            messages.addWarning("  Could not add {} to group: {}".format(layer_name, str(e)))
                    
                    # Remove the original layers that were added to the group
                    for layer in layers_to_remove:
                        try:
                            map_obj.removeLayer(layer)
                        except:
                            pass  # Ignore errors when removing
                    
                    if layers_added_to_group > 0:
                        messages.addMessage("Created group layer: {} with {} layers".format(group_name, layers_added_to_group))
                    else:
                        # Remove empty group layer
                        try:
                            map_obj.removeLayer(group_layer)
                        except:
                            pass
                        
                except Exception as e:
                    messages.addWarning("Could not create group layer {}: {}".format(group_name, str(e)))
                    # Fall back to adding layers to the map directly
                    plan_layers = sorted(plan_groups[plan_key], key=lambda x: x[0].lower())
                    for layer_name, layer_path, layer_type in plan_layers:
                        try:
                            layer = map_obj.addDataFromPath(layer_path)
                            if layer:
                                feature_classes_added.append(layer_name)
                                messages.addMessage("  Added {} to map (ungrouped)".format(layer_name))
                        except Exception as e:
                            messages.addWarning("  Could not add {}: {}".format(layer_name, str(e)))
            
            if feature_classes_added:
                messages.addMessage("\nSuccessfully added {} layers to the map in {} groups.".format(len(feature_classes_added), len(plan_groups)))
                
                # Try to save the project
                try:
                    aprx.save()
                    messages.addMessage("Project saved.")
                except:
                    pass  # Saving might fail in some contexts
            else:
                messages.addWarning("No feature classes were added to the map.")
                
        except Exception as e:
            messages.addWarning("Error loading geodatabase to map: {}".format(str(e)))
            messages.addWarning("Feature classes were created successfully but could not be added to the map.")

    def execute(self, parameters, messages):
        """Execute the tool."""
        input_path = parameters[0].valueAsText
        output_gdb = parameters[1].valueAsText
        override_crs = parameters[2].value
        include_1d = parameters[3].value
        include_2d = parameters[4].value
        include_results = parameters[5].value
        include_cell_polygons = parameters[6].value
        
        # Determine if input is directory or file
        hdf_files = []
        
        if os.path.isdir(input_path):
            # Find all plan files
            import glob
            pattern = os.path.join(input_path, "*.p[0-9][0-9].hdf")
            hdf_files = sorted(glob.glob(pattern))
            
            if not hdf_files:
                # Try geometry files
                pattern = os.path.join(input_path, "*.g[0-9][0-9].hdf")
                hdf_files = sorted(glob.glob(pattern))
        else:
            # Single file
            hdf_files = [input_path]
        
        if not hdf_files:
            messages.addErrorMessage("No HDF files found in the specified location.")
            return
        
        messages.addMessage(f"Found {len(hdf_files)} HDF file(s) to process")
        
        # Create output geodatabase
        gdb_folder = os.path.dirname(output_gdb)
        gdb_name = os.path.basename(output_gdb)
        
        if not arcpy.Exists(output_gdb):
            arcpy.CreateFileGDB_management(gdb_folder, gdb_name)
            messages.addMessage(f"Created geodatabase: {output_gdb}")
        
        # Process each HDF file
        for i, hdf_path in enumerate(hdf_files, 1):
            messages.addMessage(f"\n{'='*60}")
            messages.addMessage(f"Processing file {i}/{len(hdf_files)}: {os.path.basename(hdf_path)}")
            messages.addMessage(f"{'='*60}")
            
            self._process_single_hdf(hdf_path, output_gdb, override_crs, 
                                   include_1d, include_2d, include_results,
                                   include_cell_polygons, messages)
        
        messages.addMessage(f"\n{'='*60}")
        messages.addMessage(f"Processing complete! All plans organized in:")
        messages.addMessage(f"  {output_gdb}")
        
        # Load the geodatabase into the map
        messages.addMessage("\nAdding results to map...")
        self._load_geodatabase_to_map(output_gdb, messages)
    
    def _process_single_hdf(self, hdf_path, output_gdb, override_crs, 
                          include_1d, include_2d, include_results,
                          include_cell_polygons, messages):
        """Process a single HDF file."""
        # Extract project and plan info
        project_name, plan_number, base_name = extract_project_and_plan_info(hdf_path)
        
        # Get projection
        proj_wkt = get_ras_projection_wkt(hdf_path)
        sr = None
        if proj_wkt:
            sr = arcpy.SpatialReference()
            sr.loadFromString(proj_wkt)
            messages.addMessage(f"CRS '{sr.name}' found in HEC-RAS project files.")
        elif override_crs:
            sr = override_crs
            messages.addMessage(f"Using user-defined override CRS: {sr.name}")
        else:
            messages.addErrorMessage("CRS could not be determined. Please use the Override CRS parameter.")
            raise arcpy.ExecuteError
        
        # Create feature dataset for this plan
        feature_dataset_name = get_feature_dataset_name(hdf_path)
        fd_path = setup_geodatabase_output(output_gdb, feature_dataset_name, sr, messages)
        
        # Check HDF contents
        messages.addMessage("\nAnalyzing HDF file contents...")
        with h5py.File(hdf_path, 'r') as hdf_file:
            contents = self._check_hdf_contents(hdf_file)
        
        # Report what was found
        if contents['has_1d']:
            messages.addMessage(f"Found 1D geometry: {', '.join(contents['1d_elements'])}")
        if contents['has_2d']:
            messages.addMessage(f"Found 2D geometry: {', '.join(contents['2d_elements'])}")
        if contents['has_pipes']:
            messages.addMessage(f"Found pipe network data: {', '.join(contents['pipe_elements'])}")
        if contents['has_results']:
            messages.addMessage(f"Found results data with {len(set(contents['result_profiles']))} output variables")
        
        # Create mock parameters class for tool execution
        class MockParam:
            def __init__(self, value):
                self.value = value
                self.valueAsText = str(value) if value else None
                self.values = value if isinstance(value, list) else None
        
        # Process 1D Geometry
        if contents['has_1d'] and include_1d:
            messages.addMessage("\n--- Processing 1D Geometry ---")
            tool_1d = LoadHECRAS1DGeometry()
            
            # Use the plan's feature dataset
            fd_1d = fd_path
            
            # Create parameters for 1D tool
            # Note: We pass None for geodatabase parameter to prevent the individual tools
            # from overriding our carefully constructed feature class names that include
            # the project name and plan number
            params_1d = [
                hdf_path,  # input HDF
                override_crs,  # override CRS
                contents['1d_elements'],  # elements to load
                None, None, None, None, None,  # output paths (will be set individually)
                None,  # geodatabase (None to prevent tools from overriding our paths)
                False  # create_gdb (False because we already created it)
            ]
            
            mock_params = [MockParam(p) for p in params_1d]
            
            # Set output paths with full naming convention for multi-plan processing
            if "Cross Sections" in contents['1d_elements']:
                fc_name = f"{project_name}_Plan_{plan_number}_CrossSections"
                mock_params[3].valueAsText = os.path.join(fd_path, fc_name)
            if "River Centerlines" in contents['1d_elements']:
                fc_name = f"{project_name}_Plan_{plan_number}_RiverCenterlines"
                mock_params[4].valueAsText = os.path.join(fd_path, fc_name)
            if "Bank Lines" in contents['1d_elements']:
                fc_name = f"{project_name}_Plan_{plan_number}_BankLines"
                mock_params[5].valueAsText = os.path.join(fd_path, fc_name)
            if "Edge Lines" in contents['1d_elements']:
                fc_name = f"{project_name}_Plan_{plan_number}_EdgeLines"
                mock_params[6].valueAsText = os.path.join(fd_path, fc_name)
            if "1D Structures" in contents['1d_elements'] or "Hydraulic Structures" in contents['1d_elements']:
                fc_name = f"{project_name}_Plan_{plan_number}_Structures1D"
                mock_params[7].valueAsText = os.path.join(fd_path, fc_name)
            
            # Execute 1D tool
            try:
                tool_1d.execute(mock_params, messages)
            except Exception as e:
                messages.addWarning(f"Error processing 1D geometry: {e}")
                messages.addWarning("Continuing with remaining data...")
        
        # Process 2D Geometry
        if contents['has_2d'] and include_2d:
            messages.addMessage("\n--- Processing 2D Geometry ---")
            tool_2d = LoadHECRAS2DGeometry()
            
            # Determine which elements to load
            elements_2d = contents['2d_elements'][:]
            if include_cell_polygons and "Mesh Cell Centers" in elements_2d:
                elements_2d.append("Mesh Cells (Polygons)")
            
            # Create parameters for 2D tool
            params_2d = [
                hdf_path,  # input HDF
                override_crs,  # override CRS
                elements_2d,  # elements to load
                None, None, None, None, None, None, None, None, None,  # output paths
                None,  # geodatabase (None to prevent tools from overriding our paths)
                False  # create_gdb
            ]
            
            mock_params_2d = [MockParam(p) for p in params_2d]
            
            # Set output paths with full naming convention for multi-plan processing
            # Map indices to element names
            element_indices = {
                3: ("2D Breaklines", "Breaklines2D"),
                4: ("2D Boundary Condition Lines", "BCLines2D"),
                5: ("Mesh Area Perimeters", "MeshPerimeters"),
                6: ("Mesh Cell Centers", "MeshCellCenters"),
                7: ("Mesh Cell Faces", "MeshCellFaces"),
                8: ("Mesh Cells (Polygons)", "MeshCellPolygons"),
                9: ("Pipe Conduits", "PipeConduits"),
                10: ("Pipe Nodes", "PipeNodes"),
                11: ("Pipe Networks", "PipeNetworks")
            }
            
            for idx, (element_name, fc_base) in element_indices.items():
                if element_name in elements_2d:
                    fc_name = f"{project_name}_Plan_{plan_number}_{fc_base}"
                    mock_params_2d[idx].valueAsText = os.path.join(fd_path, fc_name)
            
            # Execute 2D tool
            try:
                tool_2d.execute(mock_params_2d, messages)
            except Exception as e:
                messages.addWarning(f"Error processing 2D geometry: {e}")
                messages.addWarning("Continuing with remaining data...")
        
        # Process Results
        if contents['has_results'] and include_results:
            messages.addMessage("\n--- Processing Results ---")
            tool_results = LoadHECRAS2DResults()
            
            # Create parameters for results tool
            params_results = [
                hdf_path,  # input HDF
                override_crs,  # override CRS
                ["Max WSE at Cell Centers", "Max Vel at Cell Faces"],  # results elements to load
                None,  # output max wse
                None,  # output max vel
                None,  # geodatabase (None to prevent tools from overriding our paths)
                False  # create_gdb
            ]
            
            mock_params_results = [MockParam(p) for p in params_results]
            
            # Set result output paths with full naming convention
            fc_name = f"{project_name}_Plan_{plan_number}_MaxWSE"
            mock_params_results[3].valueAsText = os.path.join(fd_path, fc_name)
            
            fc_name = f"{project_name}_Plan_{plan_number}_MaxVelocity"
            mock_params_results[4].valueAsText = os.path.join(fd_path, fc_name)
            
            # Execute results tool
            try:
                tool_results.execute(mock_params_results, messages)
            except Exception as e:
                messages.addWarning(f"Error processing results: {e}")
                messages.addWarning("Continuing with remaining data...")
        
        messages.addMessage(f"\nCompleted processing: {os.path.basename(hdf_path)}")
        return
    
    def getHelp(self, tool_name=None):
        """Return help documentation for the tool.
        
        This method is called when the user clicks the help button.
        It can return:
        - A URL (starting with http:// or https://)
        - A local file path (starting with file:///)
        - HTML content directly (for embedded help)
        """
        # Try local help file first
        help_file = os.path.join(
            os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 
            "Doc", "RASCommander_Help.html"
        )
        
        if os.path.exists(help_file):
            # Return local help file
            anchor = "#organize-hec-ras-project"
            return f"file:///{help_file.replace(os.sep, '/')}{anchor}"
        else:
            # Fallback to online documentation
            return "https://github.com/gpt-cmdr/ras-commander-hydro#organize-hec-ras-project"
    
    def getCodeSamples(self):
        """Provide code samples for using this tool programmatically."""
        return [
            {
                "title": "Basic Project Organization",
                "description": "Organize a single HEC-RAS plan file",
                "code": """import arcpy
import os

# Set input HDF file
hdf_file = r"C:\\RAS_Projects\\MyProject\\MyProject.p01.hdf"

# Create output geodatabase path
gdb_path = os.path.join(os.path.dirname(hdf_file), "MyProject_Organized.gdb")

# Run the organization tool
arcpy.RASCommander.OrganizeRASProject(
    input_path=hdf_file,
    output_gdb=gdb_path,
    include_1d_geometry=True,
    include_2d_geometry=True,
    include_2d_results=True,
    include_cell_polygons=False  # Skip polygons for performance
)

print(f"Project organized in: {gdb_path}")

# List created feature datasets
arcpy.env.workspace = gdb_path
for fd in arcpy.ListDatasets("*", "Feature"):
    print(f"  Feature Dataset: {fd}")
    arcpy.env.workspace = os.path.join(gdb_path, fd)
    for fc in arcpy.ListFeatureClasses():
        print(f"    - {fc}")"""
            },
            {
                "title": "Batch Process Multiple Projects",
                "description": "Organize all HEC-RAS projects in a directory",
                "code": """import arcpy
import os

# Directory containing multiple HEC-RAS projects
projects_dir = r"C:\\RAS_Projects"

# Process each project directory
for project_name in os.listdir(projects_dir):
    project_path = os.path.join(projects_dir, project_name)
    
    if os.path.isdir(project_path):
        # Create output geodatabase
        gdb_path = os.path.join(project_path, f"{project_name}_Organized.gdb")
        
        try:
            # Organize the entire project
            arcpy.RASCommander.OrganizeRASProject(
                input_path=project_path,
                output_gdb=gdb_path,
                include_1d_geometry=True,
                include_2d_geometry=True,
                include_2d_results=True,
                include_cell_polygons=False
            )
            print(f"Organized: {project_name}")
        except Exception as e:
            print(f"Failed to organize {project_name}: {str(e)}")"""
            },
            {
                "title": "Performance Optimized",
                "description": "Organize large models efficiently",
                "code": """import arcpy

# For large models, optimize performance
arcpy.RASCommander.OrganizeRASProject(
    input_path=r"C:\\RAS_Projects\\LargeModel",
    output_gdb=r"C:\\RAS_Projects\\LargeModel_Fast.gdb",
    include_1d_geometry=True,
    include_2d_geometry=True,
    include_2d_results=True,
    include_cell_polygons=False  # Skip polygon creation for speed
)

# Post-processing tip: Create rasters from point data
# This is often faster than creating polygons
gdb = r"C:\\RAS_Projects\\LargeModel_Fast.gdb"
wse_points = os.path.join(gdb, "Plan01", "MaxWSE_CellCenters")

if arcpy.Exists(wse_points):
    # Create WSE raster (requires Spatial Analyst)
    arcpy.CheckOutExtension("Spatial")
    wse_raster = arcpy.sa.Idw(wse_points, "Max_WSE")
    wse_raster.save(os.path.join(gdb, "WSE_Raster"))"""
            },
            {
                "title": "Custom CRS Override",
                "description": "Organize with specific coordinate system",
                "code": """import arcpy

# Define custom spatial reference
custom_sr = arcpy.SpatialReference(2227)  # NAD83 State Plane CA III

# Organize with CRS override
arcpy.RASCommander.OrganizeRASProject(
    input_path=r"C:\\RAS_Projects\\StatePlane\\Project.p01.hdf",
    output_gdb=r"C:\\RAS_Projects\\StatePlane_Organized.gdb",
    override_crs=custom_sr,
    include_1d_geometry=True,
    include_2d_geometry=True,
    include_2d_results=True,
    include_cell_polygons=True
)

print("Project organized with custom coordinate system")"""
            },
            {
                "title": "Results Analysis Pipeline",
                "description": "Organize and analyze results",
                "code": """import arcpy
import os

# Organize project
project_dir = r"C:\\RAS_Projects\\FloodStudy"
gdb_path = os.path.join(project_dir, "FloodStudy_Analysis.gdb")

arcpy.RASCommander.OrganizeRASProject(
    input_path=project_dir,
    output_gdb=gdb_path,
    include_1d_geometry=True,
    include_2d_geometry=True,
    include_2d_results=True,
    include_cell_polygons=False
)

# Analyze results
arcpy.env.workspace = gdb_path
datasets = arcpy.ListDatasets("*", "Feature")

for dataset in datasets:
    wse_fc = os.path.join(gdb_path, dataset, "MaxWSE_CellCenters")
    
    if arcpy.Exists(wse_fc):
        # Get statistics
        stats = arcpy.Statistics_analysis(
            wse_fc, 
            "memory\\stats",
            [["Max_WSE", "MAX"], ["Max_WSE", "MEAN"]],
            "SA_Name"
        )
        
        print(f"\\nStatistics for {dataset}:")
        with arcpy.da.SearchCursor(stats, ["SA_Name", "MAX_Max_WSE", "MEAN_Max_WSE"]) as cursor:
            for row in cursor:
                print(f"  Area: {row[0]}")
                print(f"    Max WSE: {row[1]:.2f}")
                print(f"    Mean WSE: {row[2]:.2f}")"""
            },
            {
                "title": "2D Only Organization",
                "description": "Extract only 2D geometry and results",
                "code": """import arcpy

# Focus on 2D data only
arcpy.RASCommander.OrganizeRASProject(
    input_path=r"C:\\RAS_Projects\\2DModel",
    output_gdb=r"C:\\RAS_Projects\\2DModel_2DOnly.gdb",
    include_1d_geometry=False,  # Skip 1D
    include_2d_geometry=True,   # Include 2D geometry
    include_2d_results=True,    # Include results
    include_cell_polygons=False
)

print("2D data extracted successfully")"""
            },
            {
                "title": "Geometry Only (No Results)",
                "description": "Extract geometry without results for faster processing",
                "code": """import arcpy

# Extract geometry only - useful for model setup review
arcpy.RASCommander.OrganizeRASProject(
    input_path=r"C:\\RAS_Projects\\LargeModel.p01.hdf",
    output_gdb=r"C:\\RAS_Projects\\LargeModel_GeomOnly.gdb",
    include_1d_geometry=True,
    include_2d_geometry=True,
    include_2d_results=False,  # Skip results for speed
    include_cell_polygons=False
)

print("Geometry extracted without results")"""
            }
        ]
==================================================

File: c:\GH\ras-commander-hydro\Scripts\archydro\rc_utils.py
==================================================
# -*- coding: utf-8 -*-
"""
utils.py

Shared utility functions for RAS Commander tools.
These helpers use arcpy.Add* functions because they don't have access
to the `messages` object from the tool's `execute` method.
"""

import arcpy
import os
import re
import h5py
import numpy as np
from collections import defaultdict


def get_ras_projection_wkt(hdf_path_str: str) -> str or None:
    """
    Gets projection WKT from HDF file or an associated .prj file.
    This is a self-contained port of the HdfBase.get_projection function
    from the ras-commander library.
    """
    hdf_path = os.path.abspath(hdf_path_str)
    project_folder = os.path.dirname(hdf_path)
    wkt = None
    try:
        with h5py.File(hdf_path, 'r') as hdf_file:
            proj_wkt_attr = hdf_file.attrs.get("Projection")
            if proj_wkt_attr:
                if isinstance(proj_wkt_attr, (bytes, np.bytes_)):
                    wkt = proj_wkt_attr.decode("utf-8")
                    arcpy.AddMessage(f"Found projection in HDF file: {os.path.basename(hdf_path)}")
                    return wkt
    except Exception as e:
        arcpy.AddWarning(f"Could not read projection from HDF file attribute: {e}")
    if not wkt:
        try:
            rasmap_files = [f for f in os.listdir(project_folder) if f.lower().endswith(".rasmap")]
            if rasmap_files:
                rasmap_file_path = os.path.join(project_folder, rasmap_files[0])
                with open(rasmap_file_path, 'r', errors='ignore') as f:
                    content = f.read()
                proj_match = re.search(r'<RASProjectionFilename Filename="(.*?)"', content)
                if proj_match:
                    prj_filename = proj_match.group(1).replace('.\\', '')
                    proj_file = os.path.join(project_folder, prj_filename)
                    if os.path.exists(proj_file):
                        with open(proj_file, 'r') as f_prj:
                            wkt = f_prj.read().strip()
                            arcpy.AddMessage(f"Found projection in associated RASMapper file: {os.path.basename(proj_file)}")
                            return wkt
        except Exception as e:
            arcpy.AddWarning(f"Could not read projection from RASMapper file: {e}")
    return None


def polygonize_arcpy_optimized(line_geometries, sr):
    """
    Optimized polygon creation from line geometries using numpy arrays.
    """
    if not line_geometries:
        return None
    
    try:
        # Pre-process edges into numpy arrays for efficiency
        edge_coords = []
        edge_connections = defaultdict(list)
        tolerance = 1e-9
        
        # Extract all edge coordinates at once
        for line_idx, line in enumerate(line_geometries):
            if line is None or (hasattr(line, 'length') and line.length == 0):
                continue
            
            # Get line coordinates as numpy array
            part = line.getPart(0)
            if part.count >= 2:
                coords = np.array([[part.getObject(i).X, part.getObject(i).Y] 
                                 for i in range(part.count) if part.getObject(i)])
                
                if len(coords) >= 2:
                    edge_coords.append(coords)
                    
                    # Store connections using rounded coordinates for tolerance
                    start_key = tuple(np.round(coords[0], decimals=9))
                    end_key = tuple(np.round(coords[-1], decimals=9))
                    
                    edge_connections[start_key].append((end_key, line_idx, False))
                    edge_connections[end_key].append((start_key, line_idx, True))
        
        if not edge_coords:
            return None
        
        # Find starting point with exactly 2 connections (ideal for tracing)
        start_point = None
        for pt, connections in edge_connections.items():
            if len(connections) == 2:
                start_point = pt
                break
        
        if start_point is None:
            start_point = next(iter(edge_connections))
        
        # Trace polygon using optimized lookup
        visited = set()
        ring_coords = [start_point]
        current = start_point
        
        max_edges = len(edge_coords)
        edge_count = 0
        
        while edge_count < max_edges:
            found_next = False
            
            for next_point, edge_idx, is_reversed in edge_connections[current]:
                edge_key = (edge_idx, is_reversed)
                
                if edge_key not in visited:
                    visited.add(edge_key)
                    visited.add((edge_idx, not is_reversed))
                    
                    # Get edge coordinates
                    coords = edge_coords[edge_idx]
                    if is_reversed:
                        coords = coords[::-1]
                    
                    # Add intermediate points
                    if len(coords) > 2:
                        ring_coords.extend([tuple(c) for c in coords[1:-1]])
                    
                    ring_coords.append(next_point)
                    current = next_point
                    found_next = True
                    edge_count += 1
                    
                    # Check if closed
                    if current == start_point and len(ring_coords) > 3:
                        ring_coords = ring_coords[:-1]  # Remove duplicate
                        
                        # Convert to numpy array for efficient operations
                        ring_array = np.array(ring_coords)
                        
                        # Ensure clockwise orientation
                        if not is_clockwise_numpy(ring_array):
                            ring_array = ring_array[::-1]
                        
                        # Create polygon
                        arcpy_array = arcpy.Array([arcpy.Point(x, y) for x, y in ring_array])
                        return arcpy.Polygon(arcpy_array, sr)
                    break
            
            if not found_next:
                break
        
        # If trace failed, try to create from unique points
        if len(ring_coords) >= 3:
            # Remove duplicates while preserving order
            seen = set()
            unique_coords = []
            for coord in ring_coords:
                if coord not in seen:
                    seen.add(coord)
                    unique_coords.append(coord)
            
            if len(unique_coords) >= 3:
                ring_array = np.array(unique_coords)
                
                if not is_clockwise_numpy(ring_array):
                    ring_array = ring_array[::-1]
                
                arcpy_array = arcpy.Array([arcpy.Point(x, y) for x, y in ring_array])
                return arcpy.Polygon(arcpy_array, sr)
        
        return None
        
    except Exception as e:
        arcpy.AddWarning(f"Polygon construction failed: {e}")
        return None


def is_clockwise_numpy(coords):
    """Check if polygon coordinates are in clockwise order using numpy."""
    coords = np.asarray(coords)
    x = coords[:, 0]
    y = coords[:, 1]
    
    # Vectorized shoelace formula
    area = 0.5 * np.sum(x[:-1] * y[1:] - x[1:] * y[:-1])
    area += 0.5 * (x[-1] * y[0] - x[0] * y[-1])
    
    return area < 0


def get_polyline_centroid_vectorized(polyline):
    """Calculate centroid of polyline using vectorized operations."""
    try:
        part = polyline.getPart(0)
        coords = np.array([[part.getObject(i).X, part.getObject(i).Y] 
                          for i in range(part.count) if part.getObject(i)])
        
        if len(coords) < 2:
            return None
        
        # Vectorized segment calculations
        segments = coords[1:] - coords[:-1]
        lengths = np.sqrt(np.sum(segments**2, axis=1))
        
        # Segment midpoints
        midpoints = (coords[:-1] + coords[1:]) / 2.0
        
        # Weighted centroid
        total_length = np.sum(lengths)
        if total_length > 0:
            weighted_coords = np.sum(midpoints * lengths[:, np.newaxis], axis=0) / total_length
            return arcpy.Point(weighted_coords[0], weighted_coords[1])
        
        return None
        
    except Exception as e:
        arcpy.AddWarning(f"Error calculating centroid: {e}")
        return None


def cache_hdf_metadata(hdf_file):
    """Pre-cache HDF metadata for faster access."""
    hdf_cache = {
        'mesh_names': [],
        'mesh_metadata': {},
        'has_results': False,
        'simulation_start_time': None
    }
    
    # Get mesh names
    flow_areas_path = "Geometry/2D Flow Areas"
    if flow_areas_path in hdf_file and "Attributes" in hdf_file[flow_areas_path]:
        attributes = hdf_file[f"{flow_areas_path}/Attributes"][()]
        hdf_cache['mesh_names'] = [n.decode('utf-8', 'ignore').strip() 
                                       for n in attributes["Name"]]
        
        # Cache mesh metadata
        for mesh_name in hdf_cache['mesh_names']:
            base_path = f"{flow_areas_path}/{mesh_name}"
            metadata = {}
            
            # Cache dataset sizes
            if f"{base_path}/Cells Center Coordinate" in hdf_file:
                metadata['cell_count'] = len(hdf_file[f"{base_path}/Cells Center Coordinate"])
            
            if f"{base_path}/Faces FacePoint Indexes" in hdf_file:
                metadata['face_count'] = len(hdf_file[f"{base_path}/Faces FacePoint Indexes"])
            
            hdf_cache['mesh_metadata'][mesh_name] = metadata
    
    # Check for boundary condition lines
    bc_lines_path = "Geometry/Boundary Condition Lines"
    if bc_lines_path in hdf_file:
        hdf_cache['has_bc_lines'] = True
        hdf_cache['bc_lines_count'] = len(hdf_file[f"{bc_lines_path}/Attributes"][()])
    else:
        hdf_cache['has_bc_lines'] = False
    
    # Check for pipe network elements
    pipe_conduits_path = "Geometry/Pipe Conduits"
    if pipe_conduits_path in hdf_file:
        hdf_cache['has_pipe_conduits'] = True
        if 'Attributes' in hdf_file[pipe_conduits_path]:
            hdf_cache['pipe_conduits_count'] = len(
                hdf_file[f"{pipe_conduits_path}/Attributes"][()]
            )
    else:
        hdf_cache['has_pipe_conduits'] = False
        
    pipe_nodes_path = "Geometry/Pipe Nodes"
    if pipe_nodes_path in hdf_file:
        hdf_cache['has_pipe_nodes'] = True
        if 'Attributes' in hdf_file[pipe_nodes_path]:
            hdf_cache['pipe_nodes_count'] = len(
                hdf_file[f"{pipe_nodes_path}/Attributes"][()]
            )
    else:
        hdf_cache['has_pipe_nodes'] = False
    
    # Check for results and get simulation time
    plan_info = hdf_file.get("Plan Data/Plan Information")
    if plan_info and 'Simulation Start Time' in plan_info.attrs:
        from datetime import datetime
        time_str = plan_info.attrs['Simulation Start Time']
        hdf_cache['simulation_start_time'] = datetime.strptime(
            time_str.decode('utf-8'), "%d%b%Y %H:%M:%S"
        )
        hdf_cache['has_results'] = True
    
    return hdf_cache


def get_dynamic_fields_from_data(data_list):
    """Determines field definitions from dynamic attribute data."""
    if not data_list:
        return []
    
    # Get all unique field names and their types
    field_info = {}
    field_lengths = {}  # Track max length for text fields
    
    for record in data_list:
        for field_name, value in record.items():
            # Skip None or NaN values for type detection
            if value is None or (isinstance(value, (float, np.floating)) and np.isnan(value)):
                continue
                
            if field_name not in field_info:
                # Determine field type based on value
                if isinstance(value, (bool, np.bool_)):
                    field_info[field_name] = "SHORT"  # Use SHORT for boolean
                elif isinstance(value, (int, np.integer)):
                    field_info[field_name] = "LONG"
                elif isinstance(value, (float, np.floating)):
                    field_info[field_name] = "DOUBLE"
                else:
                    field_info[field_name] = "TEXT"
                    field_lengths[field_name] = 0
            
            # Track max length for text fields
            if field_info.get(field_name) == "TEXT" and value is not None:
                str_value = str(value)
                field_lengths[field_name] = max(field_lengths.get(field_name, 0), len(str_value))
    
    # Check for fields that might have been skipped due to all NaN values
    all_field_names = set()
    for record in data_list:
        all_field_names.update(record.keys())
    
    # Add any missing fields with default type
    for field_name in all_field_names:
        if field_name not in field_info:
            # Default to DOUBLE for numeric-sounding fields, TEXT otherwise
            if any(keyword in field_name.lower() for keyword in ['elevation', 'area', 'length', 'coefficient', 'offset']):
                field_info[field_name] = "DOUBLE"
            else:
                field_info[field_name] = "TEXT"
                field_lengths[field_name] = 50
    
    # Convert to list of tuples for field creation
    fields = []
    for name, ftype in field_info.items():
        if ftype == "TEXT":
            # Ensure minimum field length of 50, max 255
            length = min(max(field_lengths.get(name, 50), 50), 255)
            fields.append((name, ftype, length))
        else:
            fields.append((name, ftype))
    
    return fields


def setup_geodatabase_output(gdb_path, feature_dataset_name, spatial_reference, messages):
    """
    Sets up geodatabase and feature dataset for organized output.
    
    Args:
        gdb_path: Path to geodatabase
        feature_dataset_name: Name of feature dataset to create (can be None)
        spatial_reference: Spatial reference for the feature dataset
        messages: ArcGIS messages object
        
    Returns:
        Path to feature dataset or geodatabase
    """
    # Create geodatabase if it doesn't exist
    if not arcpy.Exists(gdb_path):
        gdb_folder = os.path.dirname(gdb_path)
        gdb_name = os.path.basename(gdb_path)
        arcpy.CreateFileGDB_management(gdb_folder, gdb_name)
        messages.addMessage(f"Created geodatabase: {gdb_path}")
    
    # Create feature dataset if name provided
    if feature_dataset_name:
        fd_path = os.path.join(gdb_path, feature_dataset_name)
        if not arcpy.Exists(fd_path):
            arcpy.CreateFeatureDataset_management(gdb_path, feature_dataset_name, spatial_reference)
            messages.addMessage(f"Created feature dataset: {feature_dataset_name}")
        return fd_path
    
    return gdb_path


def get_unique_fc_name(workspace, base_name):
    """
    Generates a unique feature class name in the workspace.
    
    Args:
        workspace: Geodatabase or feature dataset path
        base_name: Desired feature class name
        
    Returns:
        Unique feature class name
    """
    fc_name = base_name
    counter = 1
    while arcpy.Exists(os.path.join(workspace, fc_name)):
        fc_name = f"{base_name}_{counter}"
        counter += 1
    return fc_name


def add_feature_class_metadata(fc_path, description, source_file):
    """
    Add metadata to a feature class.
    
    Args:
        fc_path: Path to feature class
        description: Description of the feature class
        source_file: Source HDF file path
    """
    try:
        metadata = arcpy.metadata.Metadata(fc_path)
        metadata.description = description
        metadata.summary = f"Extracted from HEC-RAS file: {os.path.basename(source_file)}"
        metadata.tags = "HEC-RAS, Hydraulic Modeling, RAS Commander"
        metadata.save()
    except Exception as e:
        # Metadata operations may fail in some environments, don't stop execution
        arcpy.AddWarning(f"Could not add metadata to {os.path.basename(fc_path)}: {e}")


def apply_symbology_from_layer(fc_path, layer_file_path):
    """
    Apply symbology from a layer file to a feature class.
    
    Args:
        fc_path: Path to feature class
        layer_file_path: Path to .lyrx file with symbology
    """
    try:
        if arcpy.Exists(layer_file_path):
            arcpy.ApplySymbologyFromLayer_management(fc_path, layer_file_path)
    except Exception as e:
        # Symbology operations are optional, don't stop execution
        arcpy.AddWarning(f"Could not apply symbology: {e}")


def write_features_to_fc(output_fc, sr, geom_type, fields, data, geometries, messages):
    """Optimized feature writing with batch operations."""
    total_features = len(geometries)
    if total_features == 0:
        messages.addWarningMessage(f"No features found for {os.path.basename(output_fc)}. Layer will not be created.")
        return

    messages.addMessage(f"Creating feature class: {os.path.basename(output_fc)} ({total_features} features)")
    
    try:
        output_path, output_name = os.path.split(output_fc)
        if arcpy.Exists(output_fc):
            arcpy.management.Delete(output_fc)
        arcpy.management.CreateFeatureclass(output_path, output_name, geom_type, spatial_reference=sr)
        
        field_names = []
        for field_def in fields:
            field_name = field_def[0]
            field_type = field_def[1]
            
            if field_type == "DATE":
                arcpy.management.AddField(output_fc, field_name, "DATE")
            elif field_type == "TEXT" and len(field_def) > 2:
                # Use the calculated field length for text fields
                field_length = field_def[2]
                arcpy.management.AddField(output_fc, field_name, field_type, field_length=field_length)
            else:
                arcpy.management.AddField(output_fc, field_name, field_type)
            field_names.append(field_name)
            
    except Exception as e:
        messages.addErrorMessage(f"Failed to create output feature class {output_fc}: {e}")
        return
    
    try:
        field_names_with_shape = ["SHAPE@"] + field_names
        features_inserted = 0
        
        # Prepare all rows at once
        all_rows = []
        missing_fields_warned = set()  # Track which fields we've already warned about
        
        for i, geom in enumerate(geometries):
            if geom is None or geom.pointCount == 0:
                continue
            
            row_data = data[i]
            row_values = []
            for name in field_names:
                # Use the exact field name from the data, accounting for any cleanup
                value = row_data.get(name)
                if value is None and name not in row_data:
                    # Only warn once per field across all rows
                    if name not in missing_fields_warned:
                        messages.addWarning(f"Field '{name}' not found in data. Available fields: {list(row_data.keys())}")
                        missing_fields_warned.add(name)
                    value = None  # Use None for missing fields
                
                # Handle special numpy types and NaN values
                if value is not None:
                    # Convert numpy NaN to None for proper NULL handling
                    if isinstance(value, (float, np.floating)) and np.isnan(value):
                        value = None
                    # Convert numpy types to Python types
                    elif isinstance(value, np.integer):
                        value = int(value)
                    elif isinstance(value, np.floating):
                        value = float(value)
                    elif isinstance(value, np.bool_):
                        value = int(value)  # Convert bool to 0/1 for SHORT field
                
                row_values.append(value)
            all_rows.append([geom] + row_values)
        
        # Batch insert with larger chunks
        batch_size = 10000
        with arcpy.da.InsertCursor(output_fc, field_names_with_shape) as cursor:
            for i in range(0, len(all_rows), batch_size):
                batch = all_rows[i:i + batch_size]
                for row in batch:
                    cursor.insertRow(row)
                features_inserted += len(batch)
                
                if features_inserted % 50000 == 0:
                    messages.addMessage(f"  Inserted {features_inserted}/{len(all_rows)} features...")
        
        messages.addMessage(f"Successfully created {os.path.basename(output_fc)} with {features_inserted} features.")
        
    except Exception as e:
        # If the error is about a missing field, show more diagnostic info
        if "Cannot find field" in str(e) or "not found in data" in str(e):
            messages.addErrorMessage(f"Field mapping error for {output_fc}:")
            messages.addErrorMessage(f"  Expected fields: {field_names}")
            if all_rows and len(all_rows) > 0:
                sample_data = data[0] if data else {}
                messages.addErrorMessage(f"  Available fields in data: {list(sample_data.keys())}")
            messages.addErrorMessage(f"  Original error: {e}")
        else:
            messages.addErrorMessage(f"Failed during feature creation for {output_fc}: {e}")


def extract_project_and_plan_info(hdf_path):
    """
    Extract project name and plan number from HDF file path.
    
    Args:
        hdf_path: Path to HDF file (e.g., "BaldEagleDamBrk.p07.hdf")
        
    Returns:
        tuple: (project_name, plan_number, base_name)
        Example: ("BaldEagleDamBrk", "07", "BaldEagleDamBrk.p07")
    """
    filename = os.path.basename(hdf_path)
    base_name = os.path.splitext(filename)[0]
    
    # Try to match pattern: ProjectName.pXX
    match = re.match(r'^(.+?)\.p(\d{2})$', base_name)
    
    if match:
        project_name = match.group(1)
        plan_number = match.group(2)
    else:
        # Fallback: use whole base name as project
        project_name = base_name
        plan_number = "00"
    
    return project_name, plan_number, base_name


def create_geodatabase_from_hdf(hdf_path, messages=None):
    """
    Create a geodatabase path based on HDF file name.
    
    Args:
        hdf_path: Path to HDF file
        messages: Optional messages object for logging
        
    Returns:
        Path to geodatabase
    """
    project_name, plan_number, base_name = extract_project_and_plan_info(hdf_path)
    
    # Create geodatabase name
    gdb_name = f"{base_name}.gdb"
    gdb_folder = os.path.dirname(hdf_path)
    gdb_path = os.path.join(gdb_folder, gdb_name)
    
    # Create if doesn't exist
    if not arcpy.Exists(gdb_path):
        arcpy.CreateFileGDB_management(gdb_folder, gdb_name)
        if messages:
            messages.addMessage(f"Created geodatabase: {gdb_path}")
    
    return gdb_path


def get_feature_dataset_name(hdf_path):
    """
    Get feature dataset name based on project and plan.
    
    Args:
        hdf_path: Path to HDF file
        
    Returns:
        Feature dataset name (e.g., "BaldEagleDamBrk_Plan07")
    """
    project_name, plan_number, _ = extract_project_and_plan_info(hdf_path)
    return f"{project_name}_Plan{plan_number}"


def get_feature_class_name(base_name, project_name, plan_number):
    """
    Get feature class name with project and plan info.
    
    Args:
        base_name: Base feature class name (e.g., "CrossSections")
        project_name: Project name
        plan_number: Plan number
        
    Returns:
        Full feature class name
    """
    return f"{base_name}_{project_name}_Plan{plan_number}"
==================================================

